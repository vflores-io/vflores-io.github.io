<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="noindex, nofollow"><title>Bayesian Logistic Regression with Julia and Turing.jl | Victor Flores, PhD</title>
<meta name=keywords content="Bayesian,Bayesian Regression,Logistic,Regression,Turing,Julia"><meta name=description content="Applying Turing.jl package in Julia for a probabilistic approach to a classification problem on a real-world dataset."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/><link crossorigin=anonymous href=/assets/css/stylesheet.min.cc305c161bc6ee4c6ba7ee115b6e10dd6be37f10696b436f93d754dee01c7e81.css integrity="sha256-zDBcFhvG7kxrp+4RW24Q3WvjfxBpa0Nvk9dU3uAcfoE=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=alternate hreflang=en href=http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/><meta name=twitter:title content="Bayesian Logistic Regression with Julia and Turing.jl | Victor Flores, PhD"><meta name=twitter:description content="Applying Turing.jl package in Julia for a probabilistic approach to a classification problem on a real-world dataset."><meta property="og:title" content="Bayesian Logistic Regression with Julia and Turing.jl | Victor Flores, PhD"><meta property="og:description" content="Applying Turing.jl package in Julia for a probabilistic approach to a classification problem on a real-world dataset."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-09T11:57:07+08:00"><meta property="article:modified_time" content="2024-01-09T11:57:07+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Bayesian Logistic Regression with Julia and Turing.jl","item":"http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bayesian Logistic Regression with Julia and Turing.jl | Victor Flores, PhD","name":"Bayesian Logistic Regression with Julia and Turing.jl","description":"Applying Turing.jl package in Julia for a probabilistic approach to a classification problem on a real-world dataset.","keywords":["Bayesian","Bayesian Regression","Logistic","Regression","Turing","Julia"],"wordCount":"3450","inLanguage":"en","datePublished":"2024-01-09T11:57:07+08:00","dateModified":"2024-01-09T11:57:07+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/"},"publisher":{"@type":"Organization","name":"Victor Flores, PhD","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type=text/x-mathjax-config>
        MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
        }
        });
    </script></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Victor Flores, PhD (Alt + H)">Victor Flores, PhD</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=http://localhost:1313/tags/ title=Tags>Tags</a></li><li><a href=http://localhost:1313/archives/ title=Archive>Archive</a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=http://localhost:1313/about/ title=About>About</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><h1 class=post-title>Bayesian Logistic Regression with Julia and Turing.jl</h1><div class=post-meta><span class=meta-item><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>January 9, 2024</span></span><span class=meta-item>
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=http://localhost:1313/tags/bayesian/>Bayesian</a><a href=http://localhost:1313/tags/bayesian-regression/>Bayesian Regression</a><a href=http://localhost:1313/tags/logistic/>Logistic</a><a href=http://localhost:1313/tags/regression/>Regression</a><a href=http://localhost:1313/tags/turing/>Turing</a><a href=http://localhost:1313/tags/julia/>Julia</a></span></span></div></header><div class=post-content><hr><h2 id=problem-statement>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement>Â¶</a></h2><p>You are interested in studying the factors that influence the likelihood of heart disease among patients.</p><p>You have a dataset of 303 patients, each with 14 variables: age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise induced angina, oldpeak, slope, number of major vessels, thalassemia, and diagnosis of heart disease.</p><p>You want to use Bayesian logistic regression to model the probability of heart disease (the outcome variable) as a function of some or all of the other variables (the predictor variables).</p><p>You also want to compare different models and assess their fit and predictive performance.</p><h2 id=bayesian-workflow>Bayesian Workflow<a hidden class=anchor aria-hidden=true href=#bayesian-workflow>Â¶</a></h2><p>For this project, I will try to follow this workflow:</p><ol><li><p>Data exploration: Explore the data using descriptive statistics and visualizations to get a sense of the distribution, range, and correlation of the variables. Identify any outliers, missing values, or potential errors in the data. Transform or standardize the variables if needed.</p></li><li><p>Model specification: Specify a probabilistic model that relates the outcome variable to the predictor variables using a logistic regression equation. Choose appropriate priors for the model parameters, such as normal, student-t, or Cauchy distributions. You can use the <code>brms</code> package in Julia to define and fit Bayesian models using a formula syntax similar to <code>lme4</code>. However, try to use <code>Turing.jl</code></p></li><li><p>Model fitting: Fit the model using a sampling algorithm such as Hamiltonian Monte Carlo (HMC) or No-U-Turn Sampler (NUTS). You can use the <code>DynamicHMC</code> or <code>Turing.jl</code> package in Julia to implement these algorithms. Check the convergence and mixing of the chains using diagnostics such as trace plots, autocorrelation plots, effective sample size, and potential scale reduction factor. You can use the <code>MCMCDiagnostics</code> or the included diagnostics features in <code>Turing.jl</code> package in Julia to compute these diagnostics.</p></li><li><p>Model checking: Check the fit and validity of the model using posterior predictive checks, residual analysis, and sensitivity analysis. You can use the <code>PPCheck</code> package in Julia to perform posterior predictive checks, which compare the observed data to data simulated from the posterior predictive distribution. You can use the <code>BayesianRidgeRegression</code> package in Julia to perform residual analysis, which plots the residuals against the fitted values and the predictor variables. You can use the <code>Sensitivity</code> package in Julia to perform sensitivity analysis, which measures how the posterior distribution changes with respect to the prior distribution or the likelihood function.</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># import packages</span>
</span></span><span class=line><span class=cl><span class=k>using</span> <span class=n>CSV</span><span class=p>,</span> <span class=n>Turing</span><span class=p>,</span> <span class=n>DataFrames</span><span class=p>,</span> <span class=n>StatsPlots</span><span class=p>,</span> <span class=n>LaTeXStrings</span><span class=p>,</span> <span class=n>Distributions</span>
</span></span><span class=line><span class=cl><span class=k>using</span> <span class=n>Images</span><span class=p>,</span> <span class=n>ImageIO</span>
</span></span><span class=line><span class=cl><span class=k>using</span> <span class=n>Random</span><span class=o>:</span> <span class=n>seed!</span>
</span></span><span class=line><span class=cl><span class=n>seed!</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Random.TaskLocalRNG()
</code></pre><h2 id=data-exploration>Data Exploration<a hidden class=anchor aria-hidden=true href=#data-exploration>Â¶</a></h2><p>After &ldquo;collecting&rdquo; the data, we may import it and arrange it so we can use it further.</p><p>The data set can be found in this <a href=https://www.kaggle.com/datasets/aavigan/cleveland-clinic-heart-disease-dataset>Kaggle link</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>CSV</span><span class=o>.</span><span class=n>read</span><span class=p>(</span><span class=s>&#34;data/processed_cleveland.csv&#34;</span><span class=p>,</span> <span class=n>DataFrame</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>map!</span><span class=p>(</span><span class=n>x</span> <span class=o>-&gt;</span> <span class=n>x</span> <span class=o>!=</span> <span class=mi>0</span> <span class=o>?</span> <span class=mi>1</span> <span class=o>:</span> <span class=mi>0</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>num</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>num</span><span class=p>);</span> <span class=c># make the outcome binary</span>
</span></span><span class=line><span class=cl><span class=n>df</span>
</span></span></code></pre></div><div><div style=float:left><span>303Ã—14 DataFrame</span></div><div style=float:right><span style=font-style:italic>278 rows omitted</span></div><div style=clear:both></div></div><div class=data-frame style=overflow-x:scroll><table class=data-frame style=margin-bottom:6px><thead><tr class=header><th class=rowNumber style=font-weight:700;text-align:right>Row</th><th style=text-align:left>age</th><th style=text-align:left>sex</th><th style=text-align:left>cp</th><th style=text-align:left>trestbps</th><th style=text-align:left>chol</th><th style=text-align:left>fbs</th><th style=text-align:left>restecg</th><th style=text-align:left>thalach</th><th style=text-align:left>exang</th><th style=text-align:left>oldpeak</th><th style=text-align:left>slope</th><th style=text-align:left>ca</th><th style=text-align:left>thal</th><th style=text-align:left>num</th></tr><tr class="subheader headerLastRow"><th class=rowNumber style=font-weight:700;text-align:right></th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Int64 style=text-align:left>Int64</th><th title=Float64 style=text-align:left>Float64</th><th title=Int64 style=text-align:left>Int64</th><th title=String1 style=text-align:left>String1</th><th title=String1 style=text-align:left>String1</th><th title=Int64 style=text-align:left>Int64</th></tr></thead><tbody><tr><td class=rowNumber style=font-weight:700;text-align:right>1</td><td style=text-align:right>63</td><td style=text-align:right>1</td><td style=text-align:right>1</td><td style=text-align:right>145</td><td style=text-align:right>233</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>150</td><td style=text-align:right>0</td><td style=text-align:right>2.3</td><td style=text-align:right>3</td><td style=text-align:left>0</td><td style=text-align:left>6</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>2</td><td style=text-align:right>67</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>160</td><td style=text-align:right>286</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>108</td><td style=text-align:right>1</td><td style=text-align:right>1.5</td><td style=text-align:right>2</td><td style=text-align:left>3</td><td style=text-align:left>3</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>3</td><td style=text-align:right>67</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>120</td><td style=text-align:right>229</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>129</td><td style=text-align:right>1</td><td style=text-align:right>2.6</td><td style=text-align:right>2</td><td style=text-align:left>2</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>4</td><td style=text-align:right>37</td><td style=text-align:right>1</td><td style=text-align:right>3</td><td style=text-align:right>130</td><td style=text-align:right>250</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>187</td><td style=text-align:right>0</td><td style=text-align:right>3.5</td><td style=text-align:right>3</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>5</td><td style=text-align:right>41</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>130</td><td style=text-align:right>204</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>172</td><td style=text-align:right>0</td><td style=text-align:right>1.4</td><td style=text-align:right>1</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>6</td><td style=text-align:right>56</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>120</td><td style=text-align:right>236</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>178</td><td style=text-align:right>0</td><td style=text-align:right>0.8</td><td style=text-align:right>1</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>7</td><td style=text-align:right>62</td><td style=text-align:right>0</td><td style=text-align:right>4</td><td style=text-align:right>140</td><td style=text-align:right>268</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>160</td><td style=text-align:right>0</td><td style=text-align:right>3.6</td><td style=text-align:right>3</td><td style=text-align:left>2</td><td style=text-align:left>3</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>8</td><td style=text-align:right>57</td><td style=text-align:right>0</td><td style=text-align:right>4</td><td style=text-align:right>120</td><td style=text-align:right>354</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>163</td><td style=text-align:right>1</td><td style=text-align:right>0.6</td><td style=text-align:right>1</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>9</td><td style=text-align:right>63</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>130</td><td style=text-align:right>254</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>147</td><td style=text-align:right>0</td><td style=text-align:right>1.4</td><td style=text-align:right>2</td><td style=text-align:left>1</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>10</td><td style=text-align:right>53</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>140</td><td style=text-align:right>203</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>155</td><td style=text-align:right>1</td><td style=text-align:right>3.1</td><td style=text-align:right>3</td><td style=text-align:left>0</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>11</td><td style=text-align:right>57</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>140</td><td style=text-align:right>192</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>148</td><td style=text-align:right>0</td><td style=text-align:right>0.4</td><td style=text-align:right>2</td><td style=text-align:left>0</td><td style=text-align:left>6</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>12</td><td style=text-align:right>56</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>140</td><td style=text-align:right>294</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>153</td><td style=text-align:right>0</td><td style=text-align:right>1.3</td><td style=text-align:right>2</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>13</td><td style=text-align:right>56</td><td style=text-align:right>1</td><td style=text-align:right>3</td><td style=text-align:right>130</td><td style=text-align:right>256</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>142</td><td style=text-align:right>1</td><td style=text-align:right>0.6</td><td style=text-align:right>2</td><td style=text-align:left>1</td><td style=text-align:left>6</td><td style=text-align:right>1</td></tr><tr><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td><td style=text-align:right>&#8942;</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>292</td><td style=text-align:right>55</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>132</td><td style=text-align:right>342</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>166</td><td style=text-align:right>0</td><td style=text-align:right>1.2</td><td style=text-align:right>1</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>293</td><td style=text-align:right>44</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>120</td><td style=text-align:right>169</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>144</td><td style=text-align:right>1</td><td style=text-align:right>2.8</td><td style=text-align:right>3</td><td style=text-align:left>0</td><td style=text-align:left>6</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>294</td><td style=text-align:right>63</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>140</td><td style=text-align:right>187</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>144</td><td style=text-align:right>1</td><td style=text-align:right>4.0</td><td style=text-align:right>1</td><td style=text-align:left>2</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>295</td><td style=text-align:right>63</td><td style=text-align:right>0</td><td style=text-align:right>4</td><td style=text-align:right>124</td><td style=text-align:right>197</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>136</td><td style=text-align:right>1</td><td style=text-align:right>0.0</td><td style=text-align:right>2</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>296</td><td style=text-align:right>41</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>120</td><td style=text-align:right>157</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>182</td><td style=text-align:right>0</td><td style=text-align:right>0.0</td><td style=text-align:right>1</td><td style=text-align:left>0</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>297</td><td style=text-align:right>59</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>164</td><td style=text-align:right>176</td><td style=text-align:right>1</td><td style=text-align:right>2</td><td style=text-align:right>90</td><td style=text-align:right>0</td><td style=text-align:right>1.0</td><td style=text-align:right>2</td><td style=text-align:left>2</td><td style=text-align:left>6</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>298</td><td style=text-align:right>57</td><td style=text-align:right>0</td><td style=text-align:right>4</td><td style=text-align:right>140</td><td style=text-align:right>241</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>123</td><td style=text-align:right>1</td><td style=text-align:right>0.2</td><td style=text-align:right>2</td><td style=text-align:left>0</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>299</td><td style=text-align:right>45</td><td style=text-align:right>1</td><td style=text-align:right>1</td><td style=text-align:right>110</td><td style=text-align:right>264</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>132</td><td style=text-align:right>0</td><td style=text-align:right>1.2</td><td style=text-align:right>2</td><td style=text-align:left>0</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>300</td><td style=text-align:right>68</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>144</td><td style=text-align:right>193</td><td style=text-align:right>1</td><td style=text-align:right>0</td><td style=text-align:right>141</td><td style=text-align:right>0</td><td style=text-align:right>3.4</td><td style=text-align:right>2</td><td style=text-align:left>2</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>301</td><td style=text-align:right>57</td><td style=text-align:right>1</td><td style=text-align:right>4</td><td style=text-align:right>130</td><td style=text-align:right>131</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>115</td><td style=text-align:right>1</td><td style=text-align:right>1.2</td><td style=text-align:right>2</td><td style=text-align:left>1</td><td style=text-align:left>7</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>302</td><td style=text-align:right>57</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>130</td><td style=text-align:right>236</td><td style=text-align:right>0</td><td style=text-align:right>2</td><td style=text-align:right>174</td><td style=text-align:right>0</td><td style=text-align:right>0.0</td><td style=text-align:right>2</td><td style=text-align:left>1</td><td style=text-align:left>3</td><td style=text-align:right>1</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>303</td><td style=text-align:right>38</td><td style=text-align:right>1</td><td style=text-align:right>3</td><td style=text-align:right>138</td><td style=text-align:right>175</td><td style=text-align:right>0</td><td style=text-align:right>0</td><td style=text-align:right>173</td><td style=text-align:right>0</td><td style=text-align:right>0.0</td><td style=text-align:right>1</td><td style=text-align:left>?</td><td style=text-align:left>3</td><td style=text-align:right>0</td></tr></tbody></table></div><p>In the above data frame, the attributes are as follows:</p><table><thead><tr><th style=text-align:center>Variable Name</th><th style=text-align:center>Role</th><th style=text-align:center>Type</th><th style=text-align:center>Demographic</th><th style=text-align:center>Description</th><th style=text-align:center>Units</th><th style=text-align:center>Missing Values</th></tr></thead><tbody><tr><td style=text-align:center>age</td><td style=text-align:center>Feature</td><td style=text-align:center>Integer</td><td style=text-align:center>Age</td><td style=text-align:center></td><td style=text-align:center>years</td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>sex</td><td style=text-align:center>Feature</td><td style=text-align:center>Categorical</td><td style=text-align:center>Sex</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>cp</td><td style=text-align:center>Feature</td><td style=text-align:center>Categorical</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>trestbps</td><td style=text-align:center>Feature</td><td style=text-align:center>Integer</td><td style=text-align:center></td><td style=text-align:center>resting blood pressure (on admission to the hospital)</td><td style=text-align:center>mm Hg</td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>chol</td><td style=text-align:center>Feature</td><td style=text-align:center>Integer</td><td style=text-align:center></td><td style=text-align:center>serum cholestoral</td><td style=text-align:center>mg/dl</td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>fbs</td><td style=text-align:center>Feature</td><td style=text-align:center>Categorical</td><td style=text-align:center></td><td style=text-align:center>fasting blood sugar > 120 mg/dl</td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>restecg</td><td style=text-align:center>Feature</td><td style=text-align:center>Categorical</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>thalach</td><td style=text-align:center>Feature</td><td style=text-align:center>Integer</td><td style=text-align:center></td><td style=text-align:center>maximum heart rate achieved</td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>exang</td><td style=text-align:center>Feature</td><td style=text-align:center>Categorical</td><td style=text-align:center></td><td style=text-align:center>exercise induced angina</td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:center>oldpeak</td><td style=text-align:center>Feature</td><td style=text-align:center>Integer</td><td style=text-align:center></td><td style=text-align:center>ST depression induced by exercise relative to rest</td><td style=text-align:center></td><td style=text-align:center>no</td></tr><tr><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td></tr></tbody></table><p>Complete attribute documentation:</p><pre><code>1. age: age in years
2. sex: sex (1 = male; 0 = female)
3. cp: chest pain type
	- Value 1: typical angina
	- Value 2: atypical angina
	- Value 3: non-anginal pain
	- Value 4: asymptomatic
4. trestbps: resting blood pressure (in mm Hg on admission to the
hospital)
5. chol: serum cholestoral in mg/dl
6.fbs: fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false)
7. restecg: resting electrocardiographic results
	- Value 0: normal
	- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)
	- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
8. thalach: maximum heart rate achieved
9. exang: exercise induced angina (1 = yes; 0 = no)
10. oldpeak: ST depression induced by exercise relative to rest
11. slope: the slope of the peak exercise ST segment
	- Value 1: upsloping
	- Value 2: flat
	- Value 3: downsloping
12. ca: number of major vessels (0-3) colored by flourosopy (for calcification of vessels)
13. thal: results of nuclear stress test (3 = normal; 6 = fixed defect; 7 = reversable defect)
14. num: target variable representing diagnosis of heart disease (angiographic disease status) in any major vessel
	- Value 0: &lt; 50% diameter narrowing
	- Value 1: &gt; 50% diameter narrowing
</code></pre><h2 id=data-interpretation>Data Interpretation<a hidden class=anchor aria-hidden=true href=#data-interpretation>Â¶</a></h2><p>After collecting the data, it has been imported as a Data Frame. Now, to understand what we will do with this exercise, we need to analyze the data by means of Bayesian Logistic Regression.</p><p>With this type of analysis, we can make predictions on (typically) binary outcomes, based on a set of parameters. In this particular case, we are interested in predicting whether a patient will have heart disease based on a set of parameters such as age, chest pain, blood pressure, etc.</p><p>In terms of the data available, we have a set of 303 observations (303 patients) whose symptoms and circumstances have been recorded, and the <strong>outcome</strong> is the heart disease diagnosis. To simplify things, this data set has a binary outcome, i.e. heart disease <em>present/not present</em>.</p><p>Additionally, this study is divided in two parts: first, I will set up the logistic regression model to include only one predictor, i.e., <strong>age</strong>. Afterwards, an analysis will be performed including two or more predictors.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># find the range for the age, to set the plot limits below</span>
</span></span><span class=line><span class=cl><span class=c># min_age = minimum(df.age)</span>
</span></span><span class=line><span class=cl><span class=n>min_age</span> <span class=o>=</span> <span class=mi>15</span> 
</span></span><span class=line><span class=cl><span class=n>max_age</span> <span class=o>=</span> <span class=mi>85</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># visualize data</span>
</span></span><span class=line><span class=cl><span class=n>p_data</span> <span class=o>=</span> <span class=n>scatter</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>age</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>num</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>legend</span> <span class=o>=</span> <span class=nb>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>xlims</span> <span class=o>=</span> <span class=p>(</span><span class=n>min_age</span><span class=p>,</span> <span class=n>max_age</span><span class=p>),</span>
</span></span><span class=line><span class=cl>	<span class=n>color</span> <span class=o>=</span> <span class=ss>:red</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>markersize</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>title</span> <span class=o>=</span> <span class=s>&#34;Probability of Heart Disease&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>xlabel</span> <span class=o>=</span> <span class=s>&#34;Age (years)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>ylabel</span> <span class=o>=</span> <span class=s>&#34;Probability of Heart Disease&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>widen</span> <span class=o>=</span> <span class=nb>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dpi</span> <span class=o>=</span> <span class=mi>150</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20240109_Bayesian_Logistic_Regression/output_7_0.svg type alt=svg></p><h2 id=model-specification>Model Specification<a hidden class=anchor aria-hidden=true href=#model-specification>Â¶</a></h2><p>In this stage of the workflow, we will specify the Bayesian model and then use <code>Turing.jl</code> to program it in Julia.</p><p>The model I will use for this analysis is a Bayesian Logistic Regression model, which relates the probability of heart disease to a <em>linear combination of the predictor variables</em>, using a logistic function. The model can be written as:</p><p>$$\begin{aligned}
y_i &\sim Bernoulli(p_i) \\
p_i &= \frac{1}{1+e^{-\eta_i}} \\
\eta_i &= \alpha + {\beta_1 x_{i,1}} + {\beta_2 x_{i,2}} + \ldots + {\beta_{13} x_{i,13}} \\
\alpha &\sim \mathcal{N}(\mu_\alpha,\sigma_\sigma) \\
\beta_j &\sim \mathcal{N}(\mu_{\beta},\sigma_{\beta}) \\
\end{aligned}$$</p><p>where $y_i$ is the outcome for the <em>i-th</em> patient, $p_i$ is the probability of heart disease for the <em>i-th</em> patient, $\eta_i$ is the linear predictor for the <em>i-th</em> patient, $\alpha$ and $\beta_j$ are the intercept and coefficient for the <em>j-th</em> predictor variable, respectively, and $x_{ij}$ is the value of the <em>j-th</em> predictor variable for the <em>i-th</em> patient.</p><p>The assumptions that I am making are:</p><ol><li>The outcome variable follows a Bernoulli distribution, i.e. $y_i \sim Bernoulli(p_i)$, which is appropriate for binary outcomes</li><li>The predictor variables are linearly related to the log-odds of the outcome variable, i.e. $\log(\frac{p}{1-p})$ which is a common assumption for logistic regression models</li><li>The prior distributions for the model parameters are uniform, which are weakly informative and reflect my prior beliefs about the plausible range of the parameters</li></ol><p>Regarding point (2):</p><p>That statement means that the log-odds of the outcome variable (the log of the odds ratio) can be expressed as a linear function of the predictor variables. Mathematically, this can be written as:</p><p>$$\log(\frac{p}{1-p}) = \alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k$$</p><p>where $p$ is the probability of the outcome variable being 1, $x_1, x_2, \ldots, x_k$ are the predictor variables, and $\alpha, \beta_1, \beta_2, \ldots, \beta_k$ are the coefficients (parameters).</p><p>This assumption implies that the effect of each predictor variable on the log-odds of the outcome variable is contant, regardless of the values of the other predictor variables. It also implies that the relationship between the predictor variables and the probability of the outcome variable is non-linear, as the probability is obtained by applying the inverse of the log-odds function, which is the logistic function:</p><p>$$p = \frac{1}{1+e^{-(\alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k)}}$$</p><p>The logistic function is an S-shaped curve that maps any real number to a value between 0 and 1. It has the property that as the linear predictor increases, the probability approaches 1, and as the linear predictor decreases, the probability approaches 0.</p><h3 id=model-specification-using-turingjl>Model Specification Using <code>Turing.jl</code><a hidden class=anchor aria-hidden=true href=#model-specification-using-turingjl>Â¶</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># define the Bayesian model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@model</span> <span class=k>function</span> <span class=n>logit_model</span><span class=p>(</span><span class=n>predictors</span><span class=p>,</span> <span class=n>disease</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c># priors</span>
</span></span><span class=line><span class=cl>	<span class=n>Î±</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span><span class=mf>10.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>Î²</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span><span class=mf>10.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c># likelihood</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>Î·</span> <span class=o>=</span> <span class=n>Î±</span> <span class=o>.+</span> <span class=n>Î²</span><span class=o>.*</span><span class=n>predictors</span>
</span></span><span class=line><span class=cl>	<span class=n>p</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>./</span> <span class=p>(</span><span class=mi>1</span> <span class=o>.+</span> <span class=n>exp</span><span class=o>.</span><span class=p>(</span><span class=o>-</span><span class=n>Î·</span><span class=p>))</span>     <span class=c># remember to include the &#34;.&#34;!</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=k>in</span> <span class=n>eachindex</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>disease</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>~</span> <span class=n>Bernoulli</span><span class=p>(</span><span class=n>p</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=k>end</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span></code></pre></div><pre><code>logit_model (generic function with 2 methods)
</code></pre><h4 id=crank-up-the-bayes>Crank up the Bayes!<a hidden class=anchor aria-hidden=true href=#crank-up-the-bayes>Â¶</a></h4><p>Run the model using <code>sample(model, sampler, opt_argument, samples, chains)</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># infer posterior probability</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>logit_model</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>age</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>num</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sampler</span> <span class=o>=</span> <span class=n>NUTS</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>samples</span> <span class=o>=</span> <span class=mi>1_000</span>
</span></span><span class=line><span class=cl><span class=n>num_chains</span> <span class=o>=</span> <span class=mi>8</span> 		<span class=c># set the number of chains</span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>sample</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>sampler</span><span class=p>,</span> <span class=n>MCMCThreads</span><span class=p>(),</span> <span class=n>samples</span><span class=p>,</span> <span class=n>num_chains</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.025
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.0125
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.025
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.0125
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.025
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.05
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.025
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.025
[32mSampling (8 threads): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:01[39m





Chains MCMC chain (1000Ã—14Ã—8 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 8
Samples per chain = 1000
Wall duration     = 13.18 seconds
Compute duration  = 100.1 seconds
parameters        = Î±, Î²
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat [0m [1m[0m â‹¯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64 [0m [90m[0m â‹¯

           Î±   -3.0326    0.7453    0.0210   1242.6057   1246.3034    1.0043   â‹¯
           Î²    0.0524    0.0134    0.0004   1224.4182   1259.7727    1.0037   â‹¯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           Î±   -4.4814   -3.5432   -3.0127   -2.5183   -1.5973
           Î²    0.0264    0.0432    0.0521    0.0617    0.0789
</code></pre><p><strong>NOTE</strong>: The above routine employs the <code>MCMCThreads()</code> method to sample multiple chains. However, to implement this, one needs to change the environment variables for the number of threads Julia can use. These two discussions might shed some light as to how to achieve this:</p><ol><li><a href=https://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading>https://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading</a></li><li><a href=https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794>https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794</a></li></ol><p>Of course, if you don&rsquo;t want to bother, then just change the last two functional lines in the cell above so that they read:</p><pre><code># set number of chains - comment this out:
# num_chains = 8

# crank up the Bayes! - delete MCMCThreads() and num_chains
chain = sample(model, sampler, samples)
</code></pre><h4 id=plot-the-mcmc-diagnostics>Plot the MCMC Diagnostics<a hidden class=anchor aria-hidden=true href=#plot-the-mcmc-diagnostics>Â¶</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>plot</span><span class=p>(</span><span class=n>chain</span><span class=p>,</span> <span class=n>dpi</span> <span class=o>=</span> <span class=mi>150</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20240109_Bayesian_Logistic_Regression/output_15_0.svg type alt=png></p><h4 id=get-the-summary-statistics>Get the Summary Statistics<a hidden class=anchor aria-hidden=true href=#get-the-summary-statistics>Â¶</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>summarystats</span><span class=p>(</span><span class=n>chain</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat [0m [1m[0m â‹¯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64 [0m [90m[0m â‹¯

           Î±   -3.0326    0.7453    0.0210   1242.6057   1246.3034    1.0043   â‹¯
           Î²    0.0524    0.0134    0.0004   1224.4182   1259.7727    1.0037   â‹¯
[36m                                                                1 column omitted[0m
</code></pre><h3 id=plot-and-interpret-the-results>Plot and Interpret the Results<a hidden class=anchor aria-hidden=true href=#plot-and-interpret-the-results>Â¶</a></h3><p>Ok, how do we interpret the results from a Bayesian approach? Let&rsquo;s start by plotting the results. This will help us understand not only the results, but really grasp the power of a Bayesian model in action.</p><p>From a frequentist or a machine learning approach, we would expect to find a function that models the data the best possible way, i.e. fit a model. If we were to visualize it, we would see one single sigmoid curve trying its best to explain the data.</p><p>How about this chart here, though? This chart is a collection of possible outcomes given that the <em>parameters</em> $\alpha$ and $\beta$ in this case, are modeled as random variables with some probability distribution. Therefore, there is an uncertainty associated with them. This uncertainty is naturally <em>propagated</em> onto the sigmoid function. Therefore, there is also an uncertainty associated with that sigmoid curve that we are trying to model.</p><p>Again, below we can see a collection of possible outcomes given the parameter sample space. There is a darker region where most sigmoid functions turned out, and these tend to be the most probable sigmoid functions, or, in other words, these sigmoid functions are the most probable functions that could fit the data, considering the distributions of the parameters too!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=kt>Int</span><span class=p>(</span><span class=n>samples</span><span class=o>/</span><span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>100
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>x_line</span> <span class=o>=</span> <span class=mi>15</span><span class=o>:</span><span class=mi>1</span><span class=o>:</span><span class=n>max_age</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=k>in</span> <span class=mi>1</span><span class=o>:</span><span class=n>samples</span>
</span></span><span class=line><span class=cl>    <span class=n>b</span> <span class=o>=</span> <span class=n>chain</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>m</span> <span class=o>=</span> <span class=n>chain</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>line</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>=</span> <span class=n>m</span><span class=o>*</span><span class=n>x</span> <span class=o>+</span><span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>p</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>line</span><span class=p>(</span><span class=n>x</span><span class=p>))</span> <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>plot!</span><span class=p>(</span><span class=n>p_data</span><span class=p>,</span> <span class=n>x_line</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    	<span class=n>legend</span> <span class=o>=</span> <span class=nb>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>linewidth</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span> <span class=ss>:blue</span><span class=p>,</span> <span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.02</span><span class=p>,</span> <span class=n>dpi</span> <span class=o>=</span> <span class=mi>150</span>
</span></span><span class=line><span class=cl>	<span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_data</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20240109_Bayesian_Logistic_Regression/output_20_0.svg type alt=png></p><h3 id=making-predictions>Making Predictions<a hidden class=anchor aria-hidden=true href=#making-predictions>Â¶</a></h3><p>So why go through all this trouble, you might be asking. Well, one of the reasons we want to use probabilistic models is, first, to make predictions. But I would go further than that: these models are useful when making informed decisions. Let&rsquo;s try this out.</p><p>Let&rsquo;s make predictions for different arbitrary ages (50, 60, 70, 80, 20):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>new_Age</span> <span class=o>=</span> <span class=p>[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>60</span><span class=p>,</span> <span class=mi>70</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>20</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>p_disease</span> <span class=o>=</span> <span class=n>fill</span><span class=p>(</span><span class=nb>missing</span><span class=p>,</span> <span class=n>length</span><span class=p>(</span><span class=n>new_Age</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>logit_model</span><span class=p>(</span><span class=n>new_Age</span><span class=p>,</span> <span class=n>p_disease</span><span class=p>),</span> <span class=n>chain</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>summarystats</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m [0m â‹¯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m [0m â‹¯

  disease[1]    0.3855    0.4867    0.0055   7711.3762        NaN    0.9998    â‹¯
  disease[2]    0.5258    0.4994    0.0055   8284.1301        NaN    0.9998    â‹¯
  disease[3]    0.6432    0.4791    0.0056   7441.4457        NaN    1.0002    â‹¯
  disease[4]    0.7555    0.4298    0.0050   7352.4368        NaN    0.9998    â‹¯
  disease[5]    0.1224    0.3277    0.0039   7016.6404        NaN    1.0004    â‹¯
[36m                                                                1 column omitted[0m
</code></pre><h4 id=interpreting-the-predictions>Interpreting the predictions<a hidden class=anchor aria-hidden=true href=#interpreting-the-predictions>Â¶</a></h4><p>The last operations make predictions of heart diseased based <em>on age only</em>. What the predictions mean is that, given the data, the probability distribution of an individual of age 50 to have heart disease has a mean of 0.379, and a standard deviation of 0.485 (this is highly uncertain, by the way).</p><p>Similarly, a 20-year-old individual has a probability with a mean of 0.13 and standard deviation of 0.336 of having heart disease.</p><p>These statistics are extremely powerful when you are trying to make decisions, such as when diagnosing Heart Disease. It stands to reason that, if you were a physician, you want to know what your model says might be wrong (or not) with your patient, but you also want to know how much you can trust that prediction.</p><p>If your model classifies Patient X as having heart disease, you would probably want to know how sure you are of this. And this certainty comes partially from&mldr; you guessed it: your priors <em>and</em> the data.</p><p>In the plot below, we can see the where the predictions lie. Note that these probabilities are on a continuum given by the sigmoid function. But we want our final decision to be a yes or a no. To do that, we need to set a decision threshold.</p><p>We will do that at the end of the next section.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=k>in</span> <span class=mi>1</span><span class=o>:</span><span class=n>length</span><span class=p>(</span><span class=n>new_Age</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>pred_mean</span> <span class=o>=</span> <span class=n>mean</span><span class=p>(</span><span class=n>predictions</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>pred_plot</span> <span class=o>=</span> <span class=n>scatter!</span><span class=p>(</span><span class=n>p_data</span><span class=p>,</span> <span class=p>(</span><span class=n>new_Age</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>pred_mean</span><span class=p>),</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>p_data</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20240109_Bayesian_Logistic_Regression/output_24_0.svg type alt=png></p><h2 id=model-specification-using-multiple-predictors>Model Specification Using Multiple Predictors<a hidden class=anchor aria-hidden=true href=#model-specification-using-multiple-predictors>Â¶</a></h2><h3 id=some-data-cleaning>Some Data Cleaning<a hidden class=anchor aria-hidden=true href=#some-data-cleaning>Â¶</a></h3><p>In this part, I am using the <code>Turing.jl</code> documentation tutorial found in <a href=https://turinglang.org/dev/tutorials/02-logistic-regression/>https://turinglang.org/dev/tutorials/02-logistic-regression/</a>.</p><p>In the tutorial, they quite rightly incorporate a train/test split, and data normalization, which is the recommended practice. I didn&rsquo;t do it in the first part of this tutorial to keep things simple!</p><p>Here is how they handle the split and the data normalization using <code>MLUtils</code>.</p><pre><code>function split_data(df, target; at=0.70)
    shuffled = shuffleobs(df)
    return trainset, testset = stratifiedobs(row -&gt; row[target], shuffled; p=at)
end

features = [:StudentNum, :Balance, :Income]
numerics = [:Balance, :Income]
target = :DefaultNum

trainset, testset = split_data(data, target; at=0.05)
for feature in numerics
    Î¼, Ïƒ = rescale!(trainset[!, feature]; obsdim=1)
    rescale!(testset[!, feature], Î¼, Ïƒ; obsdim=1)
end

# Turing requires data in matrix form, not dataframe
train = Matrix(trainset[:, features])
test = Matrix(testset[:, features])
train_label = trainset[:, target]
test_label = testset[:, target];
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=k>using</span> <span class=n>MLDataUtils</span><span class=o>:</span> <span class=n>shuffleobs</span><span class=p>,</span> <span class=n>stratifiedobs</span><span class=p>,</span> <span class=n>rescale!</span>
</span></span><span class=line><span class=cl><span class=k>using</span> <span class=n>StatsFuns</span> <span class=c># we introduce this package so we can later call the </span>
</span></span><span class=line><span class=cl>                <span class=c># logistic function directly instead of defining it manually as before</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=k>function</span> <span class=n>split_data</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>target</span><span class=p>;</span> <span class=n>at</span><span class=o>=</span><span class=mf>0.70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>shuffled</span> <span class=o>=</span> <span class=n>shuffleobs</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>trainset</span><span class=p>,</span> <span class=n>testset</span> <span class=o>=</span> <span class=n>stratifiedobs</span><span class=p>(</span><span class=n>row</span> <span class=o>-&gt;</span> <span class=n>row</span><span class=p>[</span><span class=n>target</span><span class=p>],</span> <span class=n>shuffled</span><span class=p>;</span> <span class=n>p</span><span class=o>=</span><span class=n>at</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=n>features</span> <span class=o>=</span> <span class=p>[</span><span class=ss>:age</span><span class=p>,</span> <span class=ss>:cp</span><span class=p>,</span> <span class=ss>:chol</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>target</span> <span class=o>=</span> <span class=ss>:num</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=n>trainset</span><span class=p>,</span> <span class=n>testset</span> <span class=o>=</span> <span class=n>split_data</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>target</span><span class=p>;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># convert the feature columns to float64 to ensure compatibility with rescale!</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>feature</span> <span class=k>in</span> <span class=n>features</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>feature</span><span class=p>]</span> <span class=o>=</span> <span class=n>float</span><span class=o>.</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>feature</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>feature</span> <span class=k>in</span> <span class=n>features</span>
</span></span><span class=line><span class=cl>    <span class=n>Î¼</span><span class=p>,</span> <span class=n>Ïƒ</span> <span class=o>=</span> <span class=n>rescale!</span><span class=p>(</span><span class=n>trainset</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>feature</span><span class=p>];</span> <span class=n>obsdim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rescale!</span><span class=p>(</span><span class=n>testset</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>feature</span><span class=p>],</span> <span class=n>Î¼</span><span class=p>,</span> <span class=n>Ïƒ</span><span class=p>;</span> <span class=n>obsdim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=c># Turing requires data in matrix form, not dataframe</span>
</span></span><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=kt>Matrix</span><span class=p>(</span><span class=n>trainset</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>features</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>test</span> <span class=o>=</span> <span class=kt>Matrix</span><span class=p>(</span><span class=n>testset</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>features</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>train_label</span> <span class=o>=</span> <span class=n>trainset</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>target</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>test_label</span> <span class=o>=</span> <span class=n>testset</span><span class=p>[</span><span class=o>!</span><span class=p>,</span> <span class=n>target</span><span class=p>];</span>
</span></span></code></pre></div><h3 id=inference>Inference<a hidden class=anchor aria-hidden=true href=#inference>Â¶</a></h3><p>Now that our data is formatted, we can perform our Bayesian logistic regression with multiple predictors: using chest pain (cp), age (age), resting bloodpressure (tresttbps) and cholesterol (chol) levels.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=nd>@model</span> <span class=k>function</span> <span class=n>logreg_multi</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c># priors</span>
</span></span><span class=line><span class=cl>	<span class=n>intercept</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>age</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>cp</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>chol</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>n</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>size</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=k>in</span> <span class=mi>1</span><span class=o>:</span><span class=n>n</span>
</span></span><span class=line><span class=cl>		<span class=c># call the logistic function directly, instead of manually</span>
</span></span><span class=line><span class=cl>		<span class=n>v</span> <span class=o>=</span> <span class=n>logistic</span><span class=p>(</span><span class=n>intercept</span> <span class=o>+</span> <span class=n>age</span><span class=o>*</span><span class=n>X</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>cp</span><span class=o>*</span><span class=n>X</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>chol</span><span class=o>*</span><span class=n>X</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>~</span> <span class=n>Bernoulli</span><span class=p>(</span><span class=n>v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span></code></pre></div><pre><code>logreg_multi (generic function with 2 methods)
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>train</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>train_label</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=p>(</span><span class=n>size</span><span class=p>(</span><span class=n>train</span><span class=p>),</span> <span class=n>size</span><span class=p>(</span><span class=n>test</span><span class=p>))</span>
</span></span></code></pre></div><pre><code>(212, 3)(91, 3)
</code></pre><p>Now we build the model and create the chain:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>model_multi</span> <span class=o>=</span> <span class=n>logreg_multi</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>chain_multi</span> <span class=o>=</span> <span class=n>sample</span><span class=p>(</span><span class=n>model_multi</span><span class=p>,</span> <span class=n>NUTS</span><span class=p>(),</span> <span class=n>MCMCThreads</span><span class=p>(),</span> <span class=mi>2_000</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span> <span class=c># select 2000 samples directly</span>
</span></span></code></pre></div><pre><code>[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 1.6
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.8
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.8
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.8
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.8
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 1.6
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 0.8
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 1.6





Chains MCMC chain (2000Ã—16Ã—8 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 8
Samples per chain = 2000
Wall duration     = 11.32 seconds
Compute duration  = 87.27 seconds
parameters        = intercept, age, cp, chol
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m   ess_bulk [0m [1m   ess_tail [0m [1m    rhat[0m â‹¯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m    Float64 [0m [90m    Float64 [0m [90m Float64[0m â‹¯

   intercept   -0.2821    0.1647    0.0012   20113.9419   13042.8456    1.0003 â‹¯
         age    0.6003    0.1760    0.0013   18327.5449   12926.7418    1.0001 â‹¯
          cp    1.0699    0.1922    0.0014   19583.1899   13534.2405    0.9999 â‹¯
        chol   -0.0073    0.1641    0.0012   18280.4944   12242.8280    1.0004 â‹¯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

   intercept   -0.6118   -0.3923   -0.2817   -0.1711    0.0388
         age    0.2645    0.4792    0.5964    0.7178    0.9575
          cp    0.7106    0.9372    1.0636    1.1963    1.4603
        chol   -0.3283   -0.1177   -0.0080    0.1025    0.3151
</code></pre><h3 id=plot-the-mcmc-diagnostics-1>Plot the MCMC Diagnostics<a hidden class=anchor aria-hidden=true href=#plot-the-mcmc-diagnostics-1>Â¶</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>plot</span><span class=p>(</span><span class=n>chain_multi</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20240109_Bayesian_Logistic_Regression/output_34_0.svg type alt=png></p><h3 id=summary-statistics>Summary Statistics<a hidden class=anchor aria-hidden=true href=#summary-statistics>Â¶</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>summarystats</span><span class=p>(</span><span class=n>chain_multi</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m   ess_bulk [0m [1m   ess_tail [0m [1m    rhat[0m â‹¯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m    Float64 [0m [90m    Float64 [0m [90m Float64[0m â‹¯

   intercept   -0.2821    0.1647    0.0012   20113.9419   13042.8456    1.0003 â‹¯
         age    0.6003    0.1760    0.0013   18327.5449   12926.7418    1.0001 â‹¯
          cp    1.0699    0.1922    0.0014   19583.1899   13534.2405    0.9999 â‹¯
        chol   -0.0073    0.1641    0.0012   18280.4944   12242.8280    1.0004 â‹¯
[36m                                                                1 column omitted[0m
</code></pre><h2 id=thank-you>Thank you!<a hidden class=anchor aria-hidden=true href=#thank-you>Â¶</a></h2><p>And that concludes this little tutorial showcasing the power of a Bayesian model and the fun of using Julia. Thank you for stopping by!</p><p>Victor Flores</p></div><footer class=post-footer><nav class=paginav><a class=prev href=http://localhost:1313/posts/20240217_bayesian_poisson_regression/20240217_bayesian_poisson_regression/><span class=title><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>Bayesian Poisson Regression with Julia and Turing.jl</span>
</a><a class=next href=http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/><span class=title>Next Page&nbsp;<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>Bayesian Linear Regression with Julia and Turing.jl</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Victor Flores, PhD</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>(function(){const a=""=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>