<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="noindex, nofollow"><title>Bayesian Linear Regression with Julia and Turing.jl | Victor Flores</title>
<meta name=keywords content="Bayesian,Bayesian Linear Regression,Regression,Turing,Julia"><meta name=description content="Learn the basics of Bayesian linear regression using Julia and Turing.jl. This tutorial covers model formulation, implementation, and interpretation through a practical example."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/><link crossorigin=anonymous href=/assets/css/stylesheet.min.cc305c161bc6ee4c6ba7ee115b6e10dd6be37f10696b436f93d754dee01c7e81.css integrity="sha256-zDBcFhvG7kxrp+4RW24Q3WvjfxBpa0Nvk9dU3uAcfoE=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=alternate hreflang=en href=http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/><meta name=twitter:title content="Bayesian Linear Regression with Julia and Turing.jl | Victor Flores"><meta name=twitter:description content="Learn the basics of Bayesian linear regression using Julia and Turing.jl. This tutorial covers model formulation, implementation, and interpretation through a practical example."><meta property="og:title" content="Bayesian Linear Regression with Julia and Turing.jl | Victor Flores"><meta property="og:description" content="Learn the basics of Bayesian linear regression using Julia and Turing.jl. This tutorial covers model formulation, implementation, and interpretation through a practical example."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-10T14:53:29+08:00"><meta property="article:modified_time" content="2023-11-10T14:53:29+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Bayesian Linear Regression with Julia and Turing.jl","item":"http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bayesian Linear Regression with Julia and Turing.jl | Victor Flores","name":"Bayesian Linear Regression with Julia and Turing.jl","description":"Learn the basics of Bayesian linear regression using Julia and Turing.jl. This tutorial covers model formulation, implementation, and interpretation through a practical example.","keywords":["Bayesian","Bayesian Linear Regression","Regression","Turing","Julia"],"wordCount":"1351","inLanguage":"en","datePublished":"2023-11-10T14:53:29+08:00","dateModified":"2023-11-10T14:53:29+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/"},"publisher":{"@type":"Organization","name":"Victor Flores","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type=text/x-mathjax-config>
        MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
        }
        });
    </script></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Victor Flores (Alt + H)">Victor Flores</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=http://localhost:1313/tags/ title=Tags>Tags</a></li><li><a href=http://localhost:1313/archives/ title=Archive>Archive</a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=http://localhost:1313/about/ title=About>About</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><h1 class=post-title>Bayesian Linear Regression with Julia and Turing.jl</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>November 10, 2023</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=http://localhost:1313/tags/bayesian/>Bayesian</a><a href=http://localhost:1313/tags/bayesian-linear-regression/>Bayesian Linear Regression</a><a href=http://localhost:1313/tags/regression/>Regression</a><a href=http://localhost:1313/tags/turing/>Turing</a><a href=http://localhost:1313/tags/julia/>Julia</a></span></span></div></header><div class=post-content><hr><h2 id=finding-a-linear-relationship-between-height-and-weight-using-bayesian-methods>Finding a Linear Relationship Between Height and Weight Using Bayesian Methods<a hidden class=anchor aria-hidden=true href=#finding-a-linear-relationship-between-height-and-weight-using-bayesian-methods>¶</a></h2><h3 id=problem-statement>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement>¶</a></h3><p>You have some data on the relationship between the height and weight of some people, and you want to fit a linear model of the form:</p><p>$$y = \alpha + \beta x + \varepsilon$$</p><p>where $y$ is the weight, $x$ is the height, $\alpha$ is the intercept, $\beta$ is the slope, and $\varepsilon$ is the error term. You want to use Bayesian inference to estimate the posterior distributions of $\alpha$ and $\beta$ given the data and some prior assumptions. You also want to use probabilistic programming to implement the Bayesian model and perform inference using a package like <code>Turing.jl</code>.</p><p>Your task is to write the code in Julia that can generate some synthetic data (or use an existing data set), define the Bayesian linear regression model, and sample from the posterior distributions using Hamiltonian Monte Carlo (HMC).</p><h6 id=credit>Credit<a hidden class=anchor aria-hidden=true href=#credit>¶</a></h6><p>This exercise is heavily inspired, and mostly taken from, the doggo&rsquo;s tutorial. Please visit his <a href=https://www.youtube.com/@doggodotjl>Youtube channel here</a>, it&rsquo;s an amazing starting point for Julia programming!</p><h3 id=import-the-necessary-packages>Import the Necessary Packages<a hidden class=anchor aria-hidden=true href=#import-the-necessary-packages>¶</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=k>using</span> <span class=n>LinearAlgebra</span><span class=p>,</span> <span class=n>Turing</span><span class=p>,</span> <span class=n>CSV</span><span class=p>,</span> <span class=n>DataFrames</span><span class=p>,</span> <span class=n>Plots</span><span class=p>,</span> <span class=n>StatsPlots</span><span class=p>,</span> <span class=n>LaTeXStrings</span>
</span></span></code></pre></div><h3 id=bayesian-workflow>Bayesian Workflow<a hidden class=anchor aria-hidden=true href=#bayesian-workflow>¶</a></h3><p>For this exercise, I will implement the following workflow:</p><ul><li>Collect data: this will be implemented by downloading the relevant data</li><li>Build a Bayesian model: will use <code>Turing.jl</code> to build the model</li><li>Infer the posterior distributions of the parameters $\alpha$ and $\beta$</li><li>Evaluate the fit of the model</li></ul><h4 id=collecting-the-data>Collecting the data<a hidden class=anchor aria-hidden=true href=#collecting-the-data>¶</a></h4><p>The data to be analyzed will be the height vs. weight data from:
<a href=https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset>https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset</a>.</p><p>Since the dataset is too large, we will select only the first 1000 entries.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># collect data</span>
</span></span><span class=line><span class=cl><span class=c># this data set was downloaded from kaggle:</span>
</span></span><span class=line><span class=cl><span class=c># https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>CSV</span><span class=o>.</span><span class=n>read</span><span class=p>(</span><span class=n>joinpath</span><span class=p>(</span><span class=s>&#34;data&#34;</span><span class=p>,</span> <span class=s>&#34;SOCR-HeightWeight.csv&#34;</span><span class=p>),</span> <span class=n>DataFrame</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># select only 100 entries</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=mi>1</span><span class=o>:</span><span class=mi>1000</span><span class=p>,</span> <span class=o>:</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>first</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span></code></pre></div><div><div style=float:left><span>5×3 DataFrame</span></div><div style=clear:both></div></div><div class=data-frame style=overflow-x:scroll><table class=data-frame style=margin-bottom:6px><thead><tr class=header><th class=rowNumber style=font-weight:700;text-align:right>Row</th><th style=text-align:left>Index</th><th style=text-align:left>Height(Inches)</th><th style=text-align:left>Weight(Pounds)</th></tr><tr class="subheader headerLastRow"><th class=rowNumber style=font-weight:700;text-align:right></th><th title=Int64 style=text-align:left>Int64</th><th title=Float64 style=text-align:left>Float64</th><th title=Float64 style=text-align:left>Float64</th></tr></thead><tbody><tr><td class=rowNumber style=font-weight:700;text-align:right>1</td><td style=text-align:right>1</td><td style=text-align:right>65.7833</td><td style=text-align:right>112.993</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>2</td><td style=text-align:right>2</td><td style=text-align:right>71.5152</td><td style=text-align:right>136.487</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>3</td><td style=text-align:right>3</td><td style=text-align:right>69.3987</td><td style=text-align:right>153.027</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>4</td><td style=text-align:right>4</td><td style=text-align:right>68.2166</td><td style=text-align:right>142.335</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>5</td><td style=text-align:right>5</td><td style=text-align:right>67.7878</td><td style=text-align:right>144.297</td></tr></tbody></table></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># change the column headers for easier access</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>colnames</span> <span class=o>=</span> <span class=p>[</span><span class=s>&#34;index&#34;</span><span class=p>,</span><span class=s>&#34;height&#34;</span><span class=p>,</span><span class=s>&#34;weight&#34;</span><span class=p>];</span> <span class=n>rename!</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=kt>Symbol</span><span class=o>.</span><span class=p>(</span><span class=n>colnames</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>first</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span></code></pre></div><div><div style=float:left><span>5×3 DataFrame</span></div><div style=clear:both></div></div><div class=data-frame style=overflow-x:scroll><table class=data-frame style=margin-bottom:6px><thead><tr class=header><th class=rowNumber style=font-weight:700;text-align:right>Row</th><th style=text-align:left>index</th><th style=text-align:left>height</th><th style=text-align:left>weight</th></tr><tr class="subheader headerLastRow"><th class=rowNumber style=font-weight:700;text-align:right></th><th title=Int64 style=text-align:left>Int64</th><th title=Float64 style=text-align:left>Float64</th><th title=Float64 style=text-align:left>Float64</th></tr></thead><tbody><tr><td class=rowNumber style=font-weight:700;text-align:right>1</td><td style=text-align:right>1</td><td style=text-align:right>65.7833</td><td style=text-align:right>112.993</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>2</td><td style=text-align:right>2</td><td style=text-align:right>71.5152</td><td style=text-align:right>136.487</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>3</td><td style=text-align:right>3</td><td style=text-align:right>69.3987</td><td style=text-align:right>153.027</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>4</td><td style=text-align:right>4</td><td style=text-align:right>68.2166</td><td style=text-align:right>142.335</td></tr><tr><td class=rowNumber style=font-weight:700;text-align:right>5</td><td style=text-align:right>5</td><td style=text-align:right>67.7878</td><td style=text-align:right>144.297</td></tr></tbody></table></div><h4 id=visualizing-the-data>Visualizing the Data<a hidden class=anchor aria-hidden=true href=#visualizing-the-data>¶</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>plot_data</span> <span class=o>=</span> <span class=n>scatter</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>height</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>legend</span> <span class=o>=</span> <span class=nb>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>title</span> <span class=o>=</span> <span class=s>&#34;Height vs. Weight&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>xlabel</span> <span class=o>=</span> <span class=s>&#34;Height (in)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>ylabel</span> <span class=o>=</span> <span class=s>&#34;Weight (lb)&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20231110_Bayesian_Linear_Regression_Julia/output_9_0.svg type alt=svg></p><h4 id=building-a-bayesian-model-with-turingjl>Building a Bayesian model with <code>Turing.jl</code>.<a hidden class=anchor aria-hidden=true href=#building-a-bayesian-model-with-turingjl>¶</a></h4><p>First, we assume that the weight is a variable dependent on the height. Thus, we can express the Bayesian model as:</p><p>$$y\sim N(\alpha + \beta^{T}\mathbf{X}, \sigma^2)$$</p><p>The above means that we assume that the data follows a normal distribution (in this case, a multivariate normal distribution), whose standard deviation is σ and its mean is the linear relationship $\alpha + \beta^{T}\mathbf{X}$.</p><p>Next, we need to assign priors to the variables $\alpha$, $\beta$ and $\sigma^2$. The latter is a measure of the uncertainty in <em>the model</em>.</p><p>So, the priors will be assigned as follows:</p><p>$$\alpha \sim N(0,10)$$
$$\beta \sim U(0,50)$$
$$\sigma^{2} \sim TN(0,100;0,\infty)$$</p><p>The last distribution is a <em>truncated normal distribution</em> bounded from 0 to $\infty$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=nd>@model</span> <span class=k>function</span> <span class=n>blr</span><span class=p>(</span><span class=n>height</span><span class=p>,</span> <span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c># priors:</span>
</span></span><span class=line><span class=cl>	<span class=n>α</span> <span class=o>~</span> <span class=n>Normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=mi>10</span><span class=p>)</span> <span class=c># intercept</span>
</span></span><span class=line><span class=cl>	<span class=n>β</span> <span class=o>~</span> <span class=n>Uniform</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>σ</span> <span class=o>~</span> <span class=n>truncated</span><span class=p>(</span><span class=n>Normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>100</span><span class=p>);</span> <span class=n>lower</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>  <span class=c># variance standard distribution</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c># likelihood</span>
</span></span><span class=line><span class=cl>	<span class=c># the likelihood in this case means that I assume that the data follows a</span>
</span></span><span class=line><span class=cl>	<span class=c># multivariate normal distribution, whose uncertainty is σ, and its mean is the linear relationship:</span>
</span></span><span class=line><span class=cl>	<span class=n>avg_weight</span> <span class=o>=</span> <span class=n>α</span> <span class=o>.+</span> <span class=p>(</span><span class=n>β</span><span class=o>.*</span><span class=n>height</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c># build the model</span>
</span></span><span class=line><span class=cl>	<span class=n>weight</span> <span class=o>~</span> <span class=n>MvNormal</span><span class=p>(</span><span class=n>avg_weight</span><span class=p>,</span> <span class=n>σ</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span></code></pre></div><pre><code>blr (generic function with 2 methods)
</code></pre><p>The next step is to perform Bayesian inference. <em>Crank up the Bayes!</em></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># crank up the bayes!</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>blr</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>height</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>samples</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>sample</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>NUTS</span><span class=p>(),</span> <span class=n>samples</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 9.765625e-5
[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:11[39m9m





Chains MCMC chain (1000×15×1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 31.4 seconds
Compute duration  = 31.4 seconds
parameters        = α, β, σ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m [0m ⋯
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m [0m ⋯

           α   -34.8414    7.6414    0.4117   344.5155   365.1189    1.0038    ⋯
           β     2.3859    0.1124    0.0060   345.5269   345.0618    1.0039    ⋯
           σ    10.3030    0.2239    0.0100   509.4680   389.9078    1.0016    ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m     2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

           α   -49.8948   -39.7950   -34.9188   -29.8116   -19.8403
           β     2.1673     2.3108     2.3872     2.4580     2.6100
           σ     9.8649    10.1550    10.3018    10.4554    10.7449
</code></pre><h4 id=visualizing-the-mcmc-diagnostics-and-summarizing-the-results>Visualizing the MCMC Diagnostics and Summarizing the Results<a hidden class=anchor aria-hidden=true href=#visualizing-the-mcmc-diagnostics-and-summarizing-the-results>¶</a></h4><p>Now that we have performed Bayesian inference using the <code>NUTS()</code> algorithm, we can visualize the results. Addisionally, call for a summary of the statistics of the inferred posterior distributions of $\theta$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>summarize</span><span class=p>(</span><span class=n>chain</span><span class=p>)</span>
</span></span></code></pre></div><pre><code> [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m [0m ⋯
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m [0m ⋯

           α   -34.8414    7.6414    0.4117   344.5155   365.1189    1.0038    ⋯
           β     2.3859    0.1124    0.0060   345.5269   345.0618    1.0039    ⋯
           σ    10.3030    0.2239    0.0100   509.4680   389.9078    1.0016    ⋯
[36m                                                                1 column omitted[0m
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>plot</span><span class=p>(</span><span class=n>chain</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20231110_Bayesian_Linear_Regression_Julia/output_16_0.svg type alt=svg></p><h5 id=visualizing-the-results>Visualizing the results<a hidden class=anchor aria-hidden=true href=#visualizing-the-results>¶</a></h5><p>It is worth noting that the results from a Bayesian Linear Regression is not one single regression line, but many. From PyMC&rsquo;s <a href=https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html>Generalized Linear Regression tutorial</a>:</p><blockquote><p>In GLMs, we do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. We can manually generate these regression lines using the posterior samples directly.</p></blockquote><p>What this means is that if we want to visualize all the lines that are generated by the parameter posterior distribution sample pool, we need to generate one line per sample set, and then we can plot them all. This procedure is executed next.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=c># plot all the sample regressions</span>
</span></span><span class=line><span class=cl><span class=c># this method was taken from: https://www.youtube.com/watch?v=EgrrtZEVOv0&amp;t=1113s</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=k>in</span> <span class=mi>1</span><span class=o>:</span><span class=n>samples</span>
</span></span><span class=line><span class=cl>	<span class=n>α</span> <span class=o>=</span> <span class=n>chain</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span>    <span class=c>#chain[row, column, chain_ID]</span>
</span></span><span class=line><span class=cl>	<span class=n>β</span> <span class=o>=</span> <span class=n>chain</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>σ²</span> <span class=o>=</span> <span class=n>chain</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>plot!</span><span class=p>(</span><span class=n>plot_data</span><span class=p>,</span> <span class=n>x</span> <span class=o>-&gt;</span> <span class=n>α</span> <span class=o>+</span> <span class=n>β</span><span class=o>*</span><span class=n>x</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>legend</span> <span class=o>=</span> <span class=nb>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=c># samples</span>
</span></span><span class=line><span class=cl>		<span class=n>linewidth</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span> <span class=n>color</span> <span class=o>=</span> <span class=ss>:orange</span><span class=p>,</span> <span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.02</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=c># error</span>
</span></span><span class=line><span class=cl>        <span class=n>ribbon</span> <span class=o>=</span> <span class=n>σ²</span><span class=p>,</span> <span class=n>fillalpha</span> <span class=o>=</span> <span class=mf>0.002</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>	
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plot_data</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20231110_Bayesian_Linear_Regression_Julia/output_18_0.svg type alt=svg></p><h3 id=using-the-regression-model-to-make-predictions>Using the Regression Model to Make Predictions<a hidden class=anchor aria-hidden=true href=#using-the-regression-model-to-make-predictions>¶</a></h3><p>Select the heights for which we want to predict the weights and then run the prediction command from <code>Turing</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>pred_height</span> <span class=o>=</span> <span class=p>[</span><span class=mi>62</span><span class=p>,</span> <span class=mi>84</span><span class=p>,</span> <span class=mi>75</span><span class=p>,</span> <span class=mi>70</span><span class=p>,</span> <span class=mi>71</span><span class=p>,</span> <span class=mi>67</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>blr</span><span class=p>(</span><span class=n>pred_height</span><span class=p>,</span> <span class=nb>missing</span><span class=p>),</span> <span class=n>chain</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Chains MCMC chain (1000×6×1 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 1
Samples per chain = 1000
parameters        = weight[1], weight[2], weight[3], weight[4], weight[5], weight[6]
internals         = 

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m[0m ⋯
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m[0m ⋯

   weight[1]   113.6815   10.3344    0.3270    997.5393   947.2109    0.9993   ⋯
   weight[2]   165.3164   10.8352    0.3744    832.5405   818.6640    1.0008   ⋯
   weight[3]   143.8911   10.5355    0.3461    929.5467   874.2977    0.9993   ⋯
   weight[4]   132.3417   10.4836    0.3448    921.6347   943.0320    1.0007   ⋯
   weight[5]   134.7606   10.7046    0.3350   1023.8876   977.6814    1.0025   ⋯
   weight[6]   124.9423   10.2245    0.3247    993.9282   867.7391    0.9991   ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m     2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

   weight[1]    93.9378   106.3972   113.6943   120.8093   134.9264
   weight[2]   142.4871   158.4933   165.5406   172.7313   184.7437
   weight[3]   122.8292   137.0108   144.0339   151.1920   164.2645
   weight[4]   111.8872   125.3733   132.1726   139.2690   153.7222
   weight[5]   113.9147   127.4356   135.0149   142.1375   154.5537
   weight[6]   105.3221   118.0098   125.1640   131.6011   145.2976
</code></pre><h4 id=visualize-the-distributions-of-the-predicted-weights>Visualize the Distributions of the Predicted Weights<a hidden class=anchor aria-hidden=true href=#visualize-the-distributions-of-the-predicted-weights>¶</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>plot</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/images/20231110_Bayesian_Linear_Regression_Julia/output_22_0.svg type alt=svg></p><p>Finally, to obtain a point estimate, compute the mean weight prediction for each height.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-julia data-lang=julia><span class=line><span class=cl><span class=n>mean_predictions</span> <span class=o>=</span> <span class=n>mean</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Mean
 [1m parameters [0m [1m     mean [0m
 [90m     Symbol [0m [90m  Float64 [0m

   weight[1]   113.6815
   weight[2]   165.3164
   weight[3]   143.8911
   weight[4]   132.3417
   weight[5]   134.7606
   weight[6]   124.9423
</code></pre></div><footer class=post-footer><nav class=paginav><a class=prev href=http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>Bayesian Logistic Regression with Julia and Turing.jl</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Victor Flores</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>(function(){const a=""=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>