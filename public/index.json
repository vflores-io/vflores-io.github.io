[{"content":"State and Input Estimation with RxInfer In this example, we estimate system states and unknown input forces for a simple structural dynamical system using the Augmented Kalman Filter (AKF) smoother in RxInfer.\nThe system can be described using a state-space model: $$ x[k+1] \\sim \\mathcal{N}(A x[k] + B p[k], Q), $$ $$ y[k] \\sim \\mathcal{N}(G x[k] + J p[k], R), $$ where $ x[k] $ are the states, $ p[k] $ are the unknown inputs, and $ y[k] $ are noisy measurements.\nWe simulate the system, generate synthetic observations, and estimate the states and inputs using RxInfer.jl.\nData Environment We start by constructing a StructuralModelData data structure that stores all relevant information for the structural dynamics model and its simulation environment. This data structure organizes the system\u0026rsquo;s properties, simulation parameters, and results for easy access and manipulation later on by RxInfer. It will later become clear what each field means!\nusing LinearAlgebra, Statistics, Random, Plots # define a data structure for the structural model environment struct StructuralModelData t::Union{Nothing, Any} # expects StepRangeLen ndof::Union{Nothing, Int64} nf::Union{Nothing, Int64} N_data::Union{Nothing, Int64} y_meas::Union{Nothing, Vector{Vector{Float64}}} A_aug::Union{Nothing, Matrix{Float64}} G_aug::Union{Nothing, Matrix{Float64}} G_aug_fullfield::Union{Nothing, Matrix{Float64}} Q_akf::Union{Nothing, Matrix{Float64}} R::Union{Nothing, LinearAlgebra.Diagonal{Float64, Vector{Float64}}} x_real::Union{Nothing, Matrix{Float64}} y_real::Union{Nothing, Matrix{Float64}} p_real::Union{Nothing, Matrix{Float64}} end The dynamics of a structural system are governed by its mass ($ M $), stiffness ($ K $), and damping ($ C $) matrices, leading to the equation of motion:\n$$ M \\ddot{x}(t) + C \\dot{x}(t) + K x(t) = p(t), $$\nwhere $ x(t) $ represents the displacements at each degree of freedom, and $ (t) $ is the external force applied to the system.\nFor this example, we consider a simplified 4-floor shear building model with 4 degrees of freedom (DOF). This toy model captures the essential dynamics of a multi-story structure while remaining computationally manageable. The system matrices are defined as follows:\n$ M $ is the diagonal mass matrix representing the lumped masses at each floor, $ K $ is the tridiagonal stiffness matrix representing inter-floor lateral stiffness, and $ C $ is the proportional damping matrix reflecting energy dissipation. The structural system is depicted below:\n# define the structural system matrices struct StructuralMatrices M::Union{Nothing, Matrix{Float64}} K::Union{Nothing, Matrix{Float64}} C::Union{Nothing, Matrix{Float64}} end M = I(4) K = [ 2 -1 0 0; -1 2 -1 0; 0 -1 2 -1; 0 0 -1 1 ] * 1e3 C = [ 2 -1 0 0; -1 2 -1 0; 0 -1 2 -1; 0 0 -1 1 ] StructuralModel = StructuralMatrices(M, K, C); Constructing the State-Space Model We convert the structural system into its discrete-time state-space form for numerical simulation. Starting from the equation of motion:\n$$ M \\ddot{x}(t) + C \\dot{x}(t) + K x(t) = F(t), $$\nwe introduce the state variable: $$ z(t) = \\begin{bmatrix} x(t) \\ \\dot{x}(t) \\end{bmatrix}, $$ which allows us to express the system as: $$ \\dot{z}(t) = A_{\\text{c}} z(t) + B_{\\text{c}} p(t), $$ where:\n$ A_{\\text{c}} = \\begin{bmatrix} 0 \u0026amp; I \\ -(M^{-1} K) \u0026amp; -(M^{-1} C) \\end{bmatrix} $ $ B_{\\text{c}} = \\begin{bmatrix} 0 \\ M^{-1} S_p \\end{bmatrix} $ $ S_p $ is the input selection matrix that determines where the external forces $ p(t) $ are applied. To perform simulations, the system is discretized using a time step $ \\Delta t $ as: $$ z[k+1] = A z[k] + B p[k], $$ where:\n$ A = e^{A_{\\text{c}} \\Delta t} $ is the state transition matrix. $ B = (A - I) A_{\\text{c}}^{-1} B_{\\text{c}} $ is the input matrix, obtained by integrating the continuous-time system. This state-space representation forms the basis for propagating the system states during simulation.\n# function to construct the state space model function construct_ssm(StructuralModel,dt, ndof, nf) # unpack the structural model M = StructuralModel.M K = StructuralModel.K C = StructuralModel.C Sp = zeros(ndof, nf) Sp[4, 1] = 1 Z = zeros(ndof, ndof) Id = I(ndof) A_continuous = [Z Id; -(M \\ K) -(M \\ C)] B_continuous = [Z; Id \\ M] * Sp A = exp(dt * A_continuous) B = (A - I(2*ndof)) * A_continuous \\ B_continuous return A, B, Sp end construct_ssm (generic function with 1 method) Generating Input Forces External forces $ p[k] $ acting on the system are modeled as Gaussian white noise:\n$$ p[k] \\sim \\mathcal{N}(\\mu, \\sigma^2), $$ where $ \\mu $ is the mean and $ \\sigma $ controls the intensity of the force.\nIn this example, the inputs are generated independently at each time step $ k $ and across input channels to simulate random excitations, such as wind or seismic forces.\n# function to generate random input noise function generate_input(N_data::Int, nf::Int; input_mu::Float64, input_std::Float64) Random.seed!(42) p_real = input_mu .+ randn(N_data, nf) .* input_std return p_real end generate_input (generic function with 1 method) Observation Model System responses, such as accelerations, are often measured at specific locations using sensors. The measurements are simulated using the equation:\n$$ y[k] = G x[k] + J p[k] + v[k], $$ where:\n$ G $ maps the system states $ x[k] $ to measured outputs. $ J $ maps the input forces $ p[k] $ to the measurements. $ v[k] \\sim \\mathcal{N}(0, \\sigma_y^2 I) $ is Gaussian noise representing sensor inaccuracies. The noise variance $ \\sigma_y^2 $ is chosen as a fraction of the true system response variance for realism.\nIn this example, accelerations are measured at selected degrees of freedom (e.g., nodes 1 and 4).\n# function to generate the measurements and noise function generate_measurements(ndof, na, nv, nd, N_data, x_real, y_real, p_real, StructuralModel, Sp) # unpack the structural model M = StructuralModel.M K = StructuralModel.K C = StructuralModel.C Sa = zeros(na, ndof) # selection matrix Sa[1, 1] = 1 # acceleration at node 1 Sa[2, 4] = 1 # acceleration at node 4 G = Sa * [-(M \\ K) -(M \\ C)] J = Sa * (I \\ M) * Sp ry = Statistics.var(y_real[2*ndof+1, :], ) * (0.1^2) # simulate noise as 1% RMS of the noise-free acceleration response nm = na + nv + nd R = I(nm) .* ry y_meas = zeros(nm, N_data) y_noise = sqrt(ry) .* randn(nm, N_data) # reconstruct the measurements y_meas = Vector{Vector{Float64}}(undef, N_data) for i in 1:N_data y_meas[i] = G * x_real[:, i] + J * p_real[i, :] + y_noise[:, i] end return y_meas, G, J, R end generate_measurements (generic function with 1 method) Simulating the Structural Response The structural response under applied forces is governed by the state-space equations:\n$$ \\begin{aligned} x[k+1] \u0026amp; = A x[k] + B p[k], \\ y[k] \u0026amp; = G_{\\text{full}} x[k] + J_{\\text{full}} p[k], \\end{aligned} $$ where $ x[k] $ are the system states, $ p[k] $ are the input forces, and $ y[k] $ are the full-field responses, i.e., the response at every degree of freedom in our structure.\nThe function below returns:\nTrue States: $ x_{\\text{real}} $, propagated using $ A $ and $ B $. Full-Field Responses: $ y_{\\text{real}} $, incorporating both states and inputs. Input Forces: $ p_{\\text{real}} $, generated as stochastic excitations. Response Matrices: $ G_{\\text{full}} $ (state-to-response) and $ J_{\\text{full}} $ (input-to-response). These outputs simulate the physical behavior of the system and serve as the basis for inference. We keep the matrices because they will be used later when analyzing our results.\n# function to simulate the structural response function simulate_response(A, B, StructuralModel, Sp, nf, ndof, N_data) # unpack the structural model M = StructuralModel.M K = StructuralModel.K C = StructuralModel.C p_real = generate_input(N_data, nf, input_mu = 0.0, input_std = 0.05) Z = zeros(ndof, ndof) Id = I(ndof) G_full = [ Id Z; Z Id; -(M \\ K) -(M \\ C) ] J_full = [ Z; Z; Id \\ M ] * Sp # preallocate matrices x_real = zeros(2 * ndof, N_data) y_real = zeros(3 * ndof, N_data) for i in 2:N_data x_real[:, i] = A * x_real[:, i-1] + B * p_real[i-1, :] y_real[:, i] = G_full * x_real[:, i-1] + J_full * p_real[i-1, :] end return x_real, y_real, p_real, G_full, J_full end simulate_response (generic function with 1 method) Augmented State-Space Model In structural health monitoring, external input forces $ p[k] $ acting on a structure, such as environmental loads or unknown excitations, are often not directly measurable. To estimate both the system states $ x[k] $ and these unknown input forces, we augment the state vector as follows:\n$$ \\tilde{x}[k] = \\begin{bmatrix} x[k] \\ p[k] \\end{bmatrix}. $$\nThis approach allows us to simultaneously infer the internal system states (e.g., displacements and velocities) and the unknown inputs using available measurements.\nThe augmented system dynamics are then expressed as:\n$$ \\begin{aligned} \\tilde{x}[k+1] \u0026amp; = A_{\\text{aug}} \\tilde{x}[k] + w[k], \\ y[k] \u0026amp; = G_{\\text{aug}} \\tilde{x}[k] + v[k], \\end{aligned} $$\nwhere:\n$ A_{\\text{aug}} $: Augmented state transition matrix. $ G_{\\text{aug}} $: Augmented measurement matrix. $ Q_{\\text{akf}} $: Augmented process noise covariance, capturing uncertainties in both states and inputs. $ w[k] $, $ v[k] $: Process and measurement noise. Full-Field vs. Measurement Space To avoid confusion, we define two augmented measurement matrices:\n$ G_{\\text{aug}} $: Projects the augmented state vector $ \\tilde{x}[k] $ to the observed sensor measurements (e.g., accelerations at specific nodes). $ G^* $: The augmented full-field measurement matrix, which projects the augmented state vector to the full-field system response. This includes all degrees of freedom (displacements, velocities, and accelerations). The distinction is critical:\n$ G_{\\text{aug}} $ is used directly in the smoother to estimate states and inputs from limited measurements. $ G^* $ is used later to reconstruct the full response field for visualization and validation. For clarity, we will refer to the augmented full-field matrix as $ G^* $ throughout the rest of this example, whereas, in the code, this will be the G_aug_fullfield object.\nNoise Covariances In this step, the process and measurement noise covariances are assumed to be known or pre-calibrated. For example:\nThe input force uncertainty $ Q_p $ is set to a high value to reflect significant variability. State noise covariance $ Q_x $ is chosen to reflect minimal uncertainty in the model. The augmented noise covariance matrix $ Q_{\\text{akf}} $ combines these quantities:\n\\[ Q_{\\text{akf}} = \\begin{bmatrix} Q_x \u0026amp; 0 \\\\ 0 \u0026amp; Q_p \\end{bmatrix}. \\]\n# function to construct the augmented model function construct_augmented_model(A, B, G, J, G_full, J_full, nf, ndof) Z_aug = zeros(nf, 2*ndof) A_aug = [ A B; Z_aug I(nf) ] G_aug = [G J] G_aug_fullfield = [G_full J_full] # full-field augmented matrix Qp_aug = I(nf) * 1e-2 # assumed known or pre-callibrated Qx_aug = zeros(2*ndof, 2*ndof) Qx_aug[(ndof+1):end, (ndof+1):end] = I(ndof) * 1e-1 # assumed known or pre-callibrated Q_akf = [ Qx_aug Z_aug\u0026#39;; Z_aug Qp_aug ] return A_aug, G_aug, Q_akf, G_aug_fullfield end construct_augmented_model (generic function with 1 method) Finally, we combine all the key steps into a single workflow to generate the system dynamics, responses, measurements, and the augmented state-space model.\nThe results are stored in a StructuralModelData object for convenient access:\nfunction get_structural_model(StructuralModel, simulation_time, dt) # intialize ndof = size(StructuralModel.M)[1] # number of degrees of freedom nf = 1 # number of inputs na, nv, nd = 2, 0, 0 # number of oberved accelerations, velocities, and displacements N_data = Int(simulation_time / dt) + 1 t = range(0, stop=simulation_time, length=N_data) # construct state-space model from structural matrices A, B, Sp = construct_ssm(StructuralModel, dt, ndof, nf) # Generate input and simulate response x_real, y_real, p_real, G_full, J_full = simulate_response(A, B, StructuralModel, Sp, nf, ndof, N_data) # Generate measurements y_meas, G, J, R = generate_measurements(ndof, na, nv, nd, N_data, x_real, y_real, p_real, StructuralModel, Sp) # Construct augmented model A_aug, G_aug, Q_akf, G_aug_fullfield = construct_augmented_model(A, B, G, J, G_full, J_full, nf, ndof) return StructuralModelData(t, ndof, nf, N_data, y_meas, A_aug, G_aug, G_aug_fullfield, Q_akf, R, x_real, y_real, p_real) end get_structural_model (generic function with 1 method) We define the simulation time and time step, then run the workflow to generate the structural model:\nsimulation_time = 5.0 dt = 0.001 model_data = get_structural_model(StructuralModel, simulation_time, dt); State and Input Estimation with RxInfer In this section, we use RxInfer to estimate the system states and unknown input forces from the simulated noisy measurements using the Augmented State Space Model discussed.\nusing RxInfer Defining the AKF Smoother Model Here, we define our Augmented Kalman Filter (AKF) smoother using RxInfer. This probabilistic model estimates the system states and unknown input forces based on the measurements.\nState Prior: We start with a prior belief about the initial state, x0. State Transition: At each time step, the system state evolves based on the transition matrix $ A $ and process noise covariance $ Q $: $$ x[k] \\sim \\mathcal{N}(A x[k-1], Q). $$ Measurements: The observations (sensor data) are modeled as noisy measurements of the states: $$ y[k] \\sim \\mathcal{N}(G x[k], R), $$ where $ G $ maps the states to the measurements, and $ R $ is the measurement noise covariance. @model function smoother_model(y, x0, A, G, Q, R) x_prior ~ x0 x_prev = x_prior # initialize previous state with x_prior for i in 1:length(y) x[i] ~ MvNormalMeanCovariance(A * x_prev, Q) y[i] ~ MvNormalMeanCovariance(G * x[i], R) x_prev = x[i] end end Running the AKF Smoother Now that we have our system set up, it\u0026rsquo;s time to estimate the system states and unknown input forces using RxInfer. We\u0026rsquo;ll run the Augmented Kalman Filter (AKF) smoother to make sense of the noisy measurements.\nHere’s the game plan:\nUnpack the Data:\nWe grab everything we need from the model_data object – time, matrices, measurements, and noise covariances.\nSet the Initial State:\nWe start with a prior belief about the first state, assuming it\u0026rsquo;s zero with some process noise:\n$$ x_0 \\sim \\mathcal{N}(0, Q_{\\text{akf}}). $$\nRun the Smoother:\nWe define a helper function to keep things tidy. This function calls RxInfer’s infer method, which does the heavy lifting for us.\nExtract and Reconstruct:\nRxInfer gives us state marginals, which are the posterior estimates of the states. Using a helper function, we reconstruct the full-field responses (displacements, velocities, and accelerations). We also extract the estimated input forces, which are part of the augmented state. That’s it! With just a few lines of code, RxInfer takes care of the math behind the scenes and delivers smooth, reliable estimates of what’s happening inside the system.\n# let\u0026#39;s wrap the results in a struct struct InferenceResults state_marginals y_full_means y_full_stds p_means p_stds end function run_smoother(model_data) # unpack the model data t = model_data.t; N_data = model_data.N_data A_aug = model_data.A_aug; G_aug = model_data.G_aug; G_aug_fullfield = model_data.G_aug_fullfield; Q_akf = model_data.Q_akf; R = model_data.R; y_meas = model_data.y_meas; # initialize the state - required when doing smoothing x0 = MvNormalMeanCovariance(zeros(size(A_aug, 1)), Q_akf); # define the smoother engine function smoother_engine(y_meas, A, G, Q, R) # run the akf smoother result_smoother = infer( model = smoother_model(x0 = x0, A = A, G = G, Q = Q, R = R), data = (y = y_meas,), options = (limit_stack_depth = 500, ) ) # return posteriors as this inference task returns the results as posteriors # because inference is done over the full graph return result_smoother.posteriors[:x] end # get the marginals of x state_marginals = smoother_engine(y_meas, A_aug, G_aug, Q_akf, R) # reconstructing the full-field response: # use helper function to reconstruct the full-field response y_full_means, y_full_stds = reconstruct_full_field(state_marginals, G_aug_fullfield, N_data) # extract the estimated input (input modeled as an augmentation state) p_results_means = getindex.(mean.(state_marginals), length(state_marginals[1])) p_results_stds = getindex.(std.(state_marginals), length(state_marginals[1])) return InferenceResults(state_marginals, y_full_means, y_full_stds, p_results_means, p_results_stds) end run_smoother (generic function with 1 method) Mapping States to Full-Field Responses In the run_smoother function, we used a helper function to map the state estimates from the AKF smoother back to the full-field responses (e.g., displacements, velocities, and accelerations).\nWhy is this important?\nWhile the smoother estimates the system states, we often care about physical quantities like accelerations or displacements across the entire structure.\nUsing the augmented full-field matrix $ G^* $, we compute:\nResponse means from state means:\n$$ \\mu_y[i] = G^* \\mu_x[i]. $$ Response uncertainties from state covariances:\n$$ \\sigma_y[i] = \\sqrt{\\text{diag}(G^* \\Sigma_x[i] {G^*}^\\top)}. $$ This gives us both the expected responses and their uncertainties at each time step.\nIn other words, this function connects the smoother’s internal state estimates to meaningful, physical quantities, making it easy to visualize the system’s behavior.\n# helper function to reconstruct the full field response from the state posteriors function reconstruct_full_field( x_marginals, G_aug_fullfield, N_data::Int ) # preallocate the full field response y_means = Vector{Vector{Float64}}(undef, N_data) # vector of vectors y_stds = Vector{Vector{Float64}}(undef, N_data) # reconstruct the full-field response using G_aug_fullfield for i in 1:N_data # extract the mean and covariance of the state posterior state_mean = mean(x_marginals[i]) # each index is a vector state_cov = cov(x_marginals[i]) # project mean and covariance onto the full-field response space y_means[i] = G_aug_fullfield * state_mean y_stds[i] = sqrt.(diag(G_aug_fullfield * state_cov * G_aug_fullfield\u0026#39;)) end return y_means, y_stds end reconstruct_full_field (generic function with 1 method) We now run the AKF smoother using the structural model data to estimate the system states, reconstruct the full-field responses, and extract the input forces along with their uncertainties.\nLet’s fire up that RxInfer!\n# run the smoother smoother_results = run_smoother(model_data); To better understand the results, we use two helper functions to create interactive PlotlyBase using PlotlyBase:\nWe first write a helper function and then plot the true states, full-field response, input, their estimates, and the associated uncertainty:\n# helper function function plot_with_uncertainty( t, true_values, estimated_means, estimated_uncertainties, ylabel_text, title_text, label_suffix=\u0026#34;\u0026#34;; plot_size = (700,300), ) # plot true values plt = plot( t, true_values, label=\u0026#34;true ($label_suffix)\u0026#34;, lw=2, color=:blue, size=plot_size, left_margin = 5Plots.mm, top_margin = 5Plots.mm, bottom_margin = 5Plots.mm ) # plot estimated values with uncertainty ribbon plot!( plt, t, estimated_means, ribbon=estimated_uncertainties, fillalpha=0.3, label=\u0026#34;estimated ($label_suffix)\u0026#34;, lw=2, color=:orange, linestyle=:dash ) # add labels and title xlabel!(\u0026#34;time (s)\u0026#34;) ylabel!(ylabel_text) title!(title_text) return plt end plot_with_uncertainty (generic function with 2 methods) # select some DOFs to plot ndof = size(StructuralModel.M)[1] display_state_dof = 4 # dof 1:4 displacements, dof 5:8 velocities display_response_dof = 2*ndof + 1 # dof 1:4 displacements, dof 5:8 velocities, dof 9:12 accelerations display_input_dof = 1 # the only one really # plot the states state_plot = plot_with_uncertainty( model_data.t, model_data.x_real[display_state_dof, :], getindex.(mean.(smoother_results.state_marginals), display_state_dof), getindex.(std.(smoother_results.state_marginals), display_state_dof), \u0026#34;state value\u0026#34;, \u0026#34;state estimate (dof $(display_state_dof))\u0026#34;, \u0026#34;state dof $(display_state_dof)\u0026#34; ); # plot the responses response_plot = plot_with_uncertainty( model_data.t, model_data.y_real[display_response_dof, :], getindex.(smoother_results.y_full_means, display_response_dof), getindex.(smoother_results.y_full_stds, display_response_dof), \u0026#34;response value\u0026#34;, \u0026#34;reconstructed response (dof $(display_response_dof))\u0026#34;, \u0026#34;response dof $(display_response_dof)\u0026#34; ); # plot the inputs input_plot = plot_with_uncertainty( model_data.t, model_data.p_real[:, display_input_dof], smoother_results.p_means, smoother_results.p_stds, \u0026#34;force value\u0026#34;, \u0026#34;input estimate (applied at dof $(display_input_dof))\u0026#34;, \u0026#34;input force $(display_input_dof)\u0026#34; ); display(state_plot) display(response_plot) display(input_plot) \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e Let\u0026rsquo;s quickly go over these results now:\nState estimation: The true state and the estimated state show excellent agreement, demonstrating the accuracy of the smoother model implemented via RxInfer. The uncertainty bounds around the estimated states are noticeable, especially early in the domain. This reflects the natural uncertainty in state estimation since only accelerations are observed, whereas displacements and velocities are inferred through integration. Reconstructed response: the real response and the reconstructed response align well across the domain, confirming that the filter captures the dynamics quite nicely. The uncertainty bounds here are narrower, showing that the confidence improves as the filter incorporates observations of these quantities of interest (i.e. accelerations). Input force reconstruction: The input force and its reconstructed counterpart show significant high frequency variations with very narrow uncertainty bounds. This is expected because accelerations, being the directly observed quantities, are estimated with higher confidence. Plus, we gave ourselves a small advantage by using a well-calibrated prior on this quantity of interest ($Q_p$). The results demonstrate how well the smoother model, implemented with RxInfer, performs in capturing the system dynamics and reconstructing hidden states and inputs. Notably, setting up the probabilistic model was straightforward and intuitive—much easier than dealing with the rest of the structural modeling! This highlights the power of RxInfer for quickly building and solving complex inference problems while keeping the implementation clean and efficient.\nWith just a few lines of code, we were able to estimate states, reconstruct responses, and confidently quantify uncertainties—a win for both accuracy and usability. 🚀\n","permalink":"http://localhost:1313/posts/20241219_rxinfer_structuraldynamics/structural_dynamics_example_4dof_no_filter_portfolio/","summary":"Implementing an AKF in Julia and RxInfer for Structural Dyanmics analysis.","title":"Kalman Filter and RxInfer for Structural Dynamics"},{"content":"\n1. Introduction Cirrhosis is a progressive liver disease characterized by the replacement of healthy liver tissue with scar tissue, leading to impaired liver function. Early prediction of patient survival can significantly impact treatment decisions and improve outcomes. In this project, we employ Bayesian statistical methods to analyze and predict the survival of patients with cirrhosis using a publicly available dataset from the UCI ML Dataset Repository.\nBayesian Classification (Logistic Regression): We develop a Bayesian logistic regression model to predict the survival status of patients based on various clinical features. This probabilistic approach allows us to incorporate prior knowledge and quantify uncertainty in our predictions.\nBayesian Survival Analysis: We perform a comprehensive survival analysis using Bayesian methods. We start with a basic Weibull model without covariates to understand the baseline survival function. We then introduce covariates to the Weibull model, and despite encountering challenges with this approach, we proceed to implement a log-normal model with covariates, which demonstrates improved performance. Finally, we refine the Weibull model by including selected covariates and accounting for censored data to enhance the model\u0026rsquo;s applicability to real-world scenarios.\nThroughout this project, we emphasize the iterative nature of model development in Bayesian statistics and showcase how to handle practical issues that may arise during analysis.\n1.1 Importing Packages To begin our analysis, we first import the necessary Python libraries for data manipulation, visualization, and Bayesian modeling.\nimport sys if \u0026#39;google.colab\u0026#39; in str(get_ipython()): if \u0026#39;numpyro\u0026#39; not in sys.modules: from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) print(\u0026#34;Running on Google Colab. NumPyro will be installed in this environment.\u0026#34;) !pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro arviz else: print(\u0026#34;NumPyro is already installed. Skipping installation.\u0026#34;) else: print(\u0026#34;Running locally. Make sure NumPyro and dependencies are installed in the environment.\u0026#34;) Running locally. Make sure NumPyro and dependencies are installed in the environment. import numpyro numpyro.set_host_device_count(4) import numpyro.distributions as dist from numpyro.infer import MCMC, NUTS, Predictive from jax import random import jax.numpy as jnp import numpy as np import seaborn as sns import arviz as az import pandas as pd import matplotlib.pyplot as plt from matplotlib import cm from sklearn.preprocessing import LabelEncoder, scale 1.2 Loading the Dataset We load the cirrhosis dataset, adjusting the file path depending on whether we\u0026rsquo;re running the code on Google Colab or locally. After reading the data into a pandas DataFrame, we display the first few rows to preview the dataset.\nif \u0026#39;google.colab\u0026#39; in str(get_ipython()): data_path = \u0026#39;https://archive.ics.uci.edu/static/public/878/data.csv\u0026#39; else: data_path = \u0026#39;data/cirrhosis.csv\u0026#39; data = pd.read_csv(data_path) data.head() ID N_Days Status Drug Age Sex Ascites Hepatomegaly Spiders Edema Bilirubin Cholesterol Albumin Copper Alk_Phos SGOT Tryglicerides Platelets Prothrombin Stage 0 1 400 D D-penicillamine 21464 F Y Y Y Y 14.5 261.0 2.60 156.0 1718.0 137.95 172.0 190.0 12.2 4.0 1 2 4500 C D-penicillamine 20617 F N Y Y N 1.1 302.0 4.14 54.0 7394.8 113.52 88.0 221.0 10.6 3.0 2 3 1012 D D-penicillamine 25594 M N N N S 1.4 176.0 3.48 210.0 516.0 96.10 55.0 151.0 12.0 4.0 3 4 1925 D D-penicillamine 19994 F N Y Y S 1.8 244.0 2.54 64.0 6121.8 60.63 92.0 183.0 10.3 4.0 4 5 1504 CL Placebo 13918 F N Y Y N 3.4 279.0 3.53 143.0 671.0 113.15 72.0 136.0 10.9 3.0 2. Preparing and Cleaning the Data We examine the dataset\u0026rsquo;s structure and data types using the data.info() method. This provides an overview of the dataset, including the number of entries, columns, non-null counts, data types, and memory usage. This step is crucial for identifying missing values and planning how to handle them in our analysis.\ndata.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 418 non-null int64 1 N_Days 418 non-null int64 2 Status 418 non-null object 3 Drug 312 non-null object 4 Age 418 non-null int64 5 Sex 418 non-null object 6 Ascites 312 non-null object 7 Hepatomegaly 312 non-null object 8 Spiders 312 non-null object 9 Edema 418 non-null object 10 Bilirubin 418 non-null float64 11 Cholesterol 284 non-null float64 12 Albumin 418 non-null float64 13 Copper 310 non-null float64 14 Alk_Phos 312 non-null float64 15 SGOT 312 non-null float64 16 Tryglicerides 282 non-null float64 17 Platelets 407 non-null float64 18 Prothrombin 416 non-null float64 19 Stage 412 non-null float64 dtypes: float64(10), int64(3), object(7) memory usage: 65.4+ KB To assess the extent of missing data in our dataset, we calculate the total number of NaN (missing) values.\n# check for nan nan_values = data.isna().sum() print(\u0026#39;Number of NaN values:\u0026#39;, nan_values.sum()) Number of NaN values: 1033 This reveals that there are 1,033 missing values across various columns. Understanding the amount and distribution of missing data is crucial for deciding how to handle it in our analysis, whether through imputation, removal, or other methods.\nTo pinpoint which columns contain missing values, we identify and list all columns with NaN entries. Recognizing these columns is essential for data cleaning and preprocessing steps. To handle the missing data, we choose to fill all NaN values with zeros using the fillna(0) method. This approach ensures that our dataset is complete and ready for analysis, without excluding any records due to missing values. After performing this operation, we display the updated dataset to confirm that all missing values have been addressed.\n# find columns with NaN values nan_columns = data.columns[data.isna().any()].tolist() print(\u0026#34;Columns with NaN values:\\n\u0026#34;, nan_columns) # fill NaN values with 0 data = data.fillna(0) data Columns with NaN values: ['Drug', 'Ascites', 'Hepatomegaly', 'Spiders', 'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage'] ID N_Days Status Drug Age Sex Ascites Hepatomegaly Spiders Edema Bilirubin Cholesterol Albumin Copper Alk_Phos SGOT Tryglicerides Platelets Prothrombin Stage 0 1 400 D D-penicillamine 21464 F Y Y Y Y 14.5 261.0 2.60 156.0 1718.0 137.95 172.0 190.0 12.2 4.0 1 2 4500 C D-penicillamine 20617 F N Y Y N 1.1 302.0 4.14 54.0 7394.8 113.52 88.0 221.0 10.6 3.0 2 3 1012 D D-penicillamine 25594 M N N N S 1.4 176.0 3.48 210.0 516.0 96.10 55.0 151.0 12.0 4.0 3 4 1925 D D-penicillamine 19994 F N Y Y S 1.8 244.0 2.54 64.0 6121.8 60.63 92.0 183.0 10.3 4.0 4 5 1504 CL Placebo 13918 F N Y Y N 3.4 279.0 3.53 143.0 671.0 113.15 72.0 136.0 10.9 3.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 413 414 681 D 0 24472 F 0 0 0 N 1.2 0.0 2.96 0.0 0.0 0.00 0.0 174.0 10.9 3.0 414 415 1103 C 0 14245 F 0 0 0 N 0.9 0.0 3.83 0.0 0.0 0.00 0.0 180.0 11.2 4.0 415 416 1055 C 0 20819 F 0 0 0 N 1.6 0.0 3.42 0.0 0.0 0.00 0.0 143.0 9.9 3.0 416 417 691 C 0 21185 F 0 0 0 N 0.8 0.0 3.75 0.0 0.0 0.00 0.0 269.0 10.4 3.0 417 418 976 C 0 19358 F 0 0 0 N 0.7 0.0 3.29 0.0 0.0 0.00 0.0 350.0 10.6 4.0 418 rows × 20 columns\n2.1 Data Preprocessing To prepare the dataset for modeling, we perform several preprocessing steps, including handling categorical variables, scaling numerical features, and encoding target variables.\n1. Extracting and Processing Categorical Data:\nSelecting Categorical Columns: We extract all columns with data type 'object', which represent categorical variables in the dataset.\nIncluding 'Stage' as Categorical: Although the 'Stage' column is numeric, it represents categorical stages of cirrhosis, so we include it in the categorical data.\nMapping 'Status' to Binary Values: The 'Status' column indicates the patient\u0026rsquo;s survival status with values 'C' (Censored), 'CL' (Censored Liver), and 'D' (Deceased). We map these to binary values for modeling, where 'C' and 'CL' are mapped to 0 (survived), and 'D' is mapped to 1 (did not survive).\n2. Extracting and Scaling Numerical Data:\nSelecting Numerical Features: We select all numerical columns from the dataset, excluding 'ID', 'N_Days', and 'Stage'.\n'ID': A unique identifier for each patient, which does not contribute to the model and can be excluded. 'N_Days': Represents the number of days of follow-up and will be used as the target variable in survival analysis. 'Stage': Already included as a categorical variable. Scaling Numerical Features: We scale the numerical features using standard scaling (mean = 0, variance = 1) to normalize the data, which can improve the performance of many machine learning models.\n3. Encoding Categorical Variables:\nLabel Encoding: We encode the categorical variables into numerical format using label encoding, which assigns unique integer values to each category in a column. 4. Combining Processed Data:\nConcatenating DataFrames: We concatenate the encoded categorical data, the scaled numerical data, and the 'ID', 'N_Days', and 'Stage' columns into a single DataFrame. This consolidated dataset is now ready for modeling. Finally, we display the first few rows of the processed dataset to verify that the preprocessing steps have been applied correctly.\n# separate categorical and numerical features categorical_data = data.select_dtypes(include = \u0026#39;object\u0026#39;).astype(str) # include \u0026#39;Stage\u0026#39; in categorical data categorical_data[\u0026#39;Stage\u0026#39;] = data[\u0026#39;Stage\u0026#39;] # map status to binary vlaues status_mapping = {\u0026#39;C\u0026#39;: 0, \u0026#39;CL\u0026#39;: 0, \u0026#39;D\u0026#39;: 1} categorical_data[\u0026#39;Status\u0026#39;] = categorical_data[\u0026#39;Status\u0026#39;].map(status_mapping) # select numerical features, excluding \u0026#39;Stage\u0026#39;, \u0026#39;ID\u0026#39; and \u0026#39;N_Days\u0026#39; # \u0026#39;Stage\u0026#39; is actually a categorical feature # \u0026#39;ID\u0026#39; has no relevance # \u0026#39;N_Days\u0026#39; is the target value numerical_data = data.select_dtypes(include = \u0026#39;number\u0026#39;).drop([\u0026#39;ID\u0026#39;, \u0026#39;N_Days\u0026#39;, \u0026#39;Stage\u0026#39;], axis = 1) # scale numerical features numerical_scaled = scale(numerical_data) numerical_scaled_df = pd.DataFrame(numerical_scaled, columns = numerical_data.columns) # label encoder label_encoder = LabelEncoder() categorical_data = categorical_data.apply(lambda x: label_encoder.fit_transform(x)) # concatenate encoded categorical and scaled numerical features data = pd.concat([categorical_data, data[[\u0026#39;ID\u0026#39;, \u0026#39;N_Days\u0026#39;, \u0026#39;Stage\u0026#39;]], numerical_scaled_df], axis = 1) data.head() Status Drug Sex Ascites Hepatomegaly Spiders Edema Stage ID N_Days ... Age Bilirubin Cholesterol Albumin Copper Alk_Phos SGOT Tryglicerides Platelets Prothrombin 0 1 1 0 2 2 2 2 4 1 400 ... 0.768941 2.562152 0.038663 -2.114296 0.981918 0.116853 0.642305 1.110012 -0.572406 1.20688 1 0 1 0 1 2 2 0 3 2 4500 ... 0.546706 -0.481759 0.198060 1.513818 -0.216383 2.902613 0.304654 0.048897 -0.277943 -0.06384 2 1 1 1 1 1 1 1 4 3 1012 ... 1.852567 -0.413611 -0.291793 -0.041088 1.616312 -0.473001 0.063889 -0.367969 -0.942860 1.04804 3 1 1 0 1 2 2 1 4 4 1925 ... 0.383244 -0.322748 -0.027428 -2.255651 -0.098903 2.277917 -0.426348 0.099427 -0.638898 -0.30210 4 0 2 0 1 2 2 0 3 5 1504 ... -1.210972 0.040704 0.108642 0.076708 0.829193 -0.396938 0.299540 -0.153220 -1.085342 0.17442 5 rows × 21 columns\n2.2 Exploratory Data Analysis (EDA) With the data preprocessed, we now perform Exploratory Data Analysis to gain insights into the dataset. This involves visualizing the distributions of key variables and exploring patterns that might inform our modeling approach.\nVisualizing Survival Status We begin by examining the distribution of the \u0026lsquo;Status\u0026rsquo; variable, which indicates whether a patient survived (0) or did not survive (1). Understanding the class balance is crucial for classification tasks and can impact the performance of our predictive models.\n# count plot _, ax = plt.subplots() sns.countplot(x = \u0026#39;Status\u0026#39;, data = data, ax = ax, color = \u0026#39;lightblue\u0026#39;) ax.set_title(\u0026#39;Status\u0026#39;); The bar plot above shows the distribution of patient survival status in the dataset. The 'Status' variable is binary, with 0 representing patients who survived and 1 representing those who did not survive. The plot reveals that there are more patients who survived (labeled as 0) compared to those who did not survive (labeled as 1), indicating a slight class imbalance in the dataset.\nCorrelation Heatmap Next, we examine the relationships between the numerical features by visualizing their pairwise correlations using a heatmap. A correlation heatmap helps us understand how different variables are related to one another, which is particularly useful for identifying multicollinearity or discovering variables that might be strong predictors of survival.\nIn this plot, the correlation coefficients range from -1 to 1:\nPositive correlation (values closer to 1) indicates that as one variable increases, the other also increases. Negative correlation (values closer to -1) indicates that as one variable increases, the other decreases. Values near 0 suggest little to no linear relationship between the variables. # correlation heatmap _, ax = plt.subplots(figsize = (10, 8)) sns.heatmap(numerical_data.corr(), ax = ax, cmap = \u0026#39;coolwarm\u0026#39;, annot = True) ax.set_title(\u0026#39;Correlation Heatmap\u0026#39;); Interpretation of the Correlation Heatmap\nThe heatmap reveals several interesting relationships between the features. For example:\nCholesterol and SGOT have a relatively high positive correlation (0.61), indicating that as cholesterol levels increase, SGOT tends to increase as well. Cholesterol and Triglycerides also show a strong positive correlation (0.63), which suggests that these two features might capture similar information. Bilirubin and Copper exhibit a moderate positive correlation (0.36), which could indicate some physiological link between these features. By identifying correlated features, we can make informed decisions on feature selection for modeling to avoid multicollinearity or redundancy in the predictors.\nCorrelation of Features with the Target To further investigate the predictive power of each feature, we calculate the absolute correlation between each feature and the target variable, Status, which represents patient survival. Correlation with the target helps us identify which features are most strongly associated with survival and might be valuable predictors in our classification and survival models.\n# calculate correlations with the target variable correlations = data.drop([\u0026#39;ID\u0026#39;, \u0026#39;N_Days\u0026#39;, \u0026#39;Stage\u0026#39;, \u0026#39;Status\u0026#39;], axis=1).corrwith(data[\u0026#39;Status\u0026#39;]).abs().sort_values(ascending=False) # generate colors n_features = len(correlations) colors = cm.rainbow(np.linspace(0, 1, n_features)) # plot horizontal bar plot for feature correlations with the target correlations.plot.barh(color=colors) plt.title(\u0026#39;Feature Correlations with the Target\u0026#39;) plt.xlabel(\u0026#39;Correlation\u0026#39;) plt.ylabel(\u0026#39;Feature\u0026#39;); Plotting Feature Correlations\nIn the horizontal bar plot above, the features are sorted by their correlation with the target variable. Features with higher correlations are likely to be more informative in predicting the patient\u0026rsquo;s survival status.\nInterpretation of the Plot\nBilirubin, Edema, and Copper are among the features with the strongest correlation to the target, suggesting they may be highly influential in determining survival outcomes. Sex and Drug show relatively weak correlations, indicating they might have less impact on survival predictions. This plot will help guide the selection of features in our model, focusing on those with higher correlations for better predictive performance. Pair Plot of Selected Features To explore the relationships between the most significant features and the target variable, we create a pair plot. This type of plot is particularly useful for visualizing the pairwise relationships between features, along with their distributions, while also distinguishing between different classes of the target variable.\nSelecting Top Features\nWe focus on the top features identified from the previous correlation analysis to avoid overloading the plot with too many variables. The selected features have the strongest correlation with the target variable, Status, and are likely to be the most informative for our analysis.\n# pairplot # first, take care of the warning: # FutureWarning: use_inf_as_na option is deprecated and will be removed # in a future version. Convert inf values to NaN before operating instead. data.replace([np.inf, -np.inf], np.nan) # there are too many features, so we only plot some of them # we have previously assigned the correlations and sorted them # so now we select the top features top_features = correlations.index[:7] selected_features = data[top_features.to_list() + [\u0026#39;Status\u0026#39;]] print(top_features.to_list()) sns.pairplot(selected_features, hue = \u0026#39;Status\u0026#39;); ['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos'] Interpretation of the Pair Plot\nIn the pair plot above, we observe scatter plots for each pair of selected features, with data points colored by the patient\u0026rsquo;s survival status (Status):\nBlue represents patients who survived (Status = 0). Orange represents patients who did not survive (Status = 1). Diagonal plots show the distribution of individual features, while off-diagonal plots reveal the relationships between different features. This helps us identify potential clusters, outliers, and separations between survival classes based on feature combinations. For example, we can observe how Bilirubin and Edema may differ between the two groups, providing insights into their predictive power.\n3. Bayesian Logistic Regression for Survival Classification In this section, we implement a Bayesian logistic regression model to predict patient survival based on clinical features. The logistic regression model is a widely used method for binary classification tasks. In a Bayesian framework, the model allows us to incorporate prior knowledge and quantify uncertainty in the predictions, making it especially suitable for medical applications where uncertainty plays a crucial role.\n3.1 Defining the Bayesian Logistic Regression Model The logistic regression model predicts the probability of an event (in this case, patient survival) occurring. The model\u0026rsquo;s structure can be described as:\n\\begin{aligned} \\text{logit}(P(y = 1 \\mid X)) = \\alpha + X\\beta \\end{aligned}\nWhere:\n$ y $ is the binary outcome (survival: 0 or 1). $ X $ is the matrix of features (predictors). $ \\alpha $ is the intercept term (a scalar). $ \\beta $ is the vector of coefficients corresponding to the features in $ X $. $ \\text{logit}(p) $ is the log-odds transformation: $ \\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) $. The posterior distribution for the model parameters $ \\alpha $ and $ \\beta $ is obtained using Bayesian inference, which combines prior distributions with the likelihood of the data.\nCode Explanation In the code below, we define the Bayesian logistic regression model using NumPyro:\nPriors: We assume normal priors for both the intercept term $ \\alpha $ and the coefficients $ \\beta $. These priors reflect our belief about the parameters before observing the data:\n$ \\alpha \\sim \\mathcal{N}(0, 1) $ $ \\beta_j \\sim \\mathcal{N}(0, 1) $ for each feature $ j $. Likelihood: The likelihood function specifies how the data (observed outcomes) are generated given the parameters. In this case, the likelihood follows a Bernoulli distribution, where the probability of success (survival) is given by the logistic function applied to the linear combination of the features.\n\\begin{aligned} P(y_i = 1 \\mid X_i, \\alpha, \\beta) = \\frac{1}{1 + \\exp^{-(\\alpha + X_i\\beta)}} \\end{aligned}\nThe code block defines the model structure, including priors and likelihood, and will be used in the next steps for inference using MCMC (Markov Chain Monte Carlo).\n# define the bayesian model import jax.numpy as jnp def cirrhosis_classification_model(X, y = None): # define priors alpha = numpyro.sample(\u0026#39;alpha\u0026#39;, dist.Normal(0, 1)) beta = numpyro.sample(\u0026#39;beta\u0026#39;, dist.Normal( jnp.zeros([X.shape[1]]), jnp.ones([X.shape[1]])) ) logits = alpha + jnp.matmul(X, beta) # define likelihood with numpyro.plate(\u0026#39;data\u0026#39;, X.shape[0]): numpyro.sample(\u0026#39;y\u0026#39;, dist.Bernoulli(logits = logits), obs = y) Preparing the Data for Modeling Before fitting the Bayesian logistic regression model, we need to prepare the input data. We use the top features identified from the correlation analysis as our predictors, and the Status column as the binary outcome (survival).\nX: The matrix of predictor variables (features), consisting of the top correlated features selected during EDA. y: The target variable, representing the patient\u0026rsquo;s survival status (0 for survived, 1 for not survived). We then check the shape of X to confirm that it has the correct dimensions for modeling (rows corresponding to patients and columns corresponding to features).\n# prepare the data X = data[top_features.to_list()].values y = data[\u0026#39;Status\u0026#39;].values X.shape (418, 7) Visualizing the Model Structure To better understand the structure of our Bayesian logistic regression model, we generate a graphical representation using numpyro.render_model(). This function provides a clear visualization of the model\u0026rsquo;s components, including the priors, likelihood, and data dependencies.\nIn the rendered graph, we can see:\nPriors: The intercept alpha and coefficients beta are sampled from normal distributions. Likelihood: The binary target variable y follows a Bernoulli distribution with probabilities defined by the logistic transformation of the linear combination of features (logits). This graphical model representation helps ensure that the model is correctly specified and provides insights into how the data interacts with the parameters.\nnumpyro.render_model(cirrhosis_classification_model, model_args = (X, y)) 3.2 Running MCMC Inference We use Markov Chain Monte Carlo (MCMC) with the No-U-Turn Sampler (NUTS) to perform Bayesian inference on our logistic regression model. MCMC is a powerful method for sampling from the posterior distribution of our model\u0026rsquo;s parameters, especially in cases where analytical solutions are intractable.\nNUTS: A variant of Hamiltonian Monte Carlo (HMC) that automatically tunes the step size and trajectory length, improving the efficiency of the sampling process. num_warmup: The number of warm-up steps (burn-in period) where the sampler adapts to the posterior distribution. num_samples: The number of samples to draw from the posterior after the warm-up phase. num_chains: The number of independent MCMC chains to run in parallel, allowing us to evaluate the convergence of the model. We then execute the sampling procedure using the run() method, which generates samples from the posterior distribution of the model parameters. After sampling, we print the summary of the results, which includes posterior means, standard deviations, and diagnostics such as effective sample size (ESS) and R-hat (a measure of convergence).\nmcmc = MCMC(NUTS(cirrhosis_classification_model), num_warmup = 1000, num_samples = 1000, num_chains = 4) mcmc.run(random.PRNGKey(0), X, y) mcmc.print_summary() 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] mean std median 5.0% 95.0% n_eff r_hat alpha -0.54 0.14 -0.54 -0.76 -0.32 6398.50 1.00 beta[0] 1.21 0.21 1.21 0.85 1.55 6336.86 1.00 beta[1] 0.65 0.30 0.65 0.11 1.09 4819.01 1.00 beta[2] 0.32 0.15 0.32 0.06 0.55 6200.94 1.00 beta[3] 0.44 0.15 0.44 0.21 0.69 6358.73 1.00 beta[4] -0.14 0.13 -0.15 -0.36 0.07 6798.94 1.00 beta[5] 0.64 0.14 0.64 0.41 0.86 7289.60 1.00 beta[6] 0.41 0.13 0.40 0.19 0.62 6308.57 1.00 Number of divergences: 0 MCMC Inference Summary The output of the MCMC inference provides a summary of the posterior distributions for each parameter in the model, including the intercept (alpha) and the coefficients for the selected features (beta values). Each row represents a parameter, and the summary includes the following key statistics:\nmean: The mean of the posterior distribution, which gives us a point estimate of the parameter. std: The standard deviation of the posterior, indicating the uncertainty in the parameter estimate. median: The median of the posterior distribution, often used as a robust point estimate. 5.0% and 95.0% percentiles: The lower and upper bounds of the 90% credible interval, giving us a range within which the true value of the parameter is likely to lie with 90% probability. n_eff: The effective sample size, which quantifies the amount of independent information in the MCMC samples. A higher value suggests that the chain is well-mixed and the samples are less correlated. r_hat: The R-hat statistic, which measures the convergence of the MCMC chains. Values close to 1 indicate good convergence, meaning the chains have converged to the same distribution. Interpretation alpha: The intercept has a posterior mean of -0.54 with a standard deviation of 0.14, indicating moderate uncertainty. The 90% credible interval ranges from -0.76 to -0.32.\nbeta[0]: This coefficient has the highest posterior mean (1.21) and the smallest credible interval, suggesting that it has a strong positive effect on the probability of survival.\nbeta[1] to beta[6]: These coefficients represent the effects of the top features on survival. The values range between -0.14 and 0.65, indicating varying levels of influence. For example, beta[5] (0.64) and beta[6] (0.41) show relatively strong positive associations with the target variable.\nEffective Sample Size (n_eff): All parameters have high effective sample sizes (in the thousands), indicating that the MCMC sampler performed efficiently and produced a large number of independent samples.\nR-hat: The R-hat values are all 1.00, confirming that the MCMC chains have converged successfully for each parameter.\nThe inference results show that the model has converged well, and we have meaningful posterior estimates for the model parameters. The posterior means and credible intervals provide insights into how the features are associated with the probability of survival, with several coefficients (e.g., beta[0], beta[5]) standing out as strong predictors.\n3.3 Trace and Pair Plots for MCMC Diagnostics To assess the performance and convergence of the MCMC sampler, we generate trace plots and pair plots of the sampled posterior distributions. These plots allow us to visually inspect how well the sampler explored the parameter space and whether there were any issues, such as divergences.\nTrace Plot: The trace plot shows the sampled values of each parameter across the MCMC iterations. Ideally, we expect to see \u0026ldquo;well-mixed\u0026rdquo; chains, where the samples quickly explore the parameter space without getting stuck, indicating good convergence.\nPair Plot: The pair plot visualizes the relationships between the parameters by plotting their joint posterior distributions. We also include information about divergences, which can indicate potential problems with the sampling process. Divergences suggest that the sampler struggled to explore certain regions of the posterior, often due to ill-specified priors or likelihoods.\nThese plots provide valuable diagnostics for ensuring that the MCMC sampling was successful and that the posterior estimates are reliable.\ntrace = az.from_numpyro(mcmc) az.plot_trace(trace, compact = False) plt.tight_layout() plt.show() Interpreting the Trace Plot The trace plot shows the sampled values of each parameter (both alpha and beta coefficients) across the iterations of the MCMC chains. The left-hand side of each panel displays the posterior distribution (density plot), while the right-hand side shows the trace of the samples.\nPosterior Distributions: Each parameter\u0026rsquo;s posterior distribution appears well-behaved and roughly Gaussian, with all MCMC chains overlapping significantly. This indicates good convergence across the different chains.\nTrace of Samples: The trace plots show that the MCMC samples fluctuate around a stable mean, with no obvious trends or drifts. This behavior suggests that the chains have mixed well and are sampling from the stationary distribution, which is essential for reliable posterior estimates.\nOverall, the trace plot confirms that the model parameters have converged and the posterior distributions are stable.\naz.plot_pair(trace, divergences = True) plt.show() Interpreting the Pair Plot The pair plot shows pairwise relationships between the parameters (alpha and beta coefficients), with each dot representing a sample from the posterior distribution. The diagonal plots display the marginal distributions for each parameter, while the off-diagonal scatter plots show the joint distributions between pairs of parameters.\nJoint Distributions: The scatter plots between parameter pairs show no strong patterns, such as high correlations or dependencies. This suggests that the parameters are relatively independent of each other, which is a positive sign for the model\u0026rsquo;s stability and interpretability.\nDivergences: No major divergences or problematic regions are visible in the joint distributions, indicating that the sampler did not struggle with any particular combination of parameters.\nThe pair plot helps us visually confirm that the posterior samples form well-behaved distributions, with no obvious issues like strong correlations or divergences. Together, the trace and pair plots provide strong evidence that the MCMC sampling has performed effectively and that the posterior estimates are trustworthy.\n3.4 Prior Predictive Checks Before analyzing the posterior predictive distribution, it is important to first check the prior predictive distribution to ensure that our prior assumptions make sense. A prior predictive check involves generating data from the model using only the priors, without any influence from the observed data. This allows us to see if the priors we have chosen are reasonable and if they align with plausible outcomes.\nPredictive: We use the Predictive class to sample from the prior distribution of our Bayesian model. y_prior: This contains samples generated from the prior distribution. These are drawn without considering the observed data, giving us a sense of the expected outcomes under the prior assumptions. prior_data: The prior samples are converted into an InferenceData object, which allows us to use ArviZ\u0026rsquo;s plotting utilities. Plotting Prior Predictive Samples: The prior predictive plot shows the range of outcomes generated purely from the prior distributions. This check ensures that our prior assumptions are not unrealistic and that the prior distributions provide a reasonable range of expected outcomes.\nprior = Predictive(cirrhosis_classification_model, num_samples=1000) # get the prior samples y_prior = prior(random.PRNGKey(42), X) # convert to InferenceData prior_data = az.from_numpyro(mcmc, prior = y_prior) # plot the prior samples az.plot_ppc(prior_data, group = \u0026#39;prior\u0026#39;, observed = True); Interpretation of the Prior Predictive Check The prior predictive check plot shows the range of possible outcomes (in blue) generated from the model based purely on the prior distributions. The dashed orange line represents the mean of the prior predictive distribution, while the solid black line indicates the observed data.\nPrior Predictive Range: The blue lines show a wide spread of potential outcomes, indicating that the priors we defined for the model parameters allow for a broad range of values for the binary target variable (y), which corresponds to patient survival. This ensures that the priors are not overly restrictive.\nComparison to Observed Data: The observed data (black lines) mostly align within the range of the prior predictive distribution, suggesting that the priors are reasonable and that the model is well-calibrated to allow for plausible outcomes before even fitting to the actual data.\nOverall, this plot suggests that the prior distributions are neither too narrow nor too unrealistic, providing a good foundation for the next step: posterior inference based on observed data.\n3.5 Posterior Predictive Check After fitting the model to the observed data, we perform a posterior predictive check. This step involves generating new data from the posterior distribution of the model, which incorporates both the prior information and the observed data. By comparing the posterior predictive distribution to the actual observed outcomes, we can assess how well the model fits the data.\nposterior_samples: These are the samples drawn from the posterior distribution after fitting the model to the data using MCMC. Predictive: We use the Predictive class to generate predictions based on the posterior samples. Posterior Predictive Distribution: The plot visualizes the range of predicted values from the posterior distribution, along with the observed data for comparison. # get samples from the posterior posterior_samples = mcmc.get_samples() # define a Predictive class predictive = Predictive(cirrhosis_classification_model, posterior_samples = posterior_samples) # get the samples y_pred = predictive(random.PRNGKey(42), X) # convert to inference data posterior_data = az.from_numpyro(mcmc, posterior_predictive = y_pred) az.plot_ppc(data = posterior_data, group = \u0026#39;posterior\u0026#39;); Interpretation of the Posterior Predictive Plot In the resulting plot:\nThe blue lines represent the range of possible outcomes generated from the posterior predictive distribution. The solid black line shows the observed data. The dashed orange line indicates the posterior predictive mean. In contrast to the prior predictive plot, the posterior predictive distribution is now informed by the data:\nThe posterior predictive outcomes are much more closely aligned with the observed data, indicating that the model has successfully learned from the data. The posterior predictive mean closely matches the observed values, showing that the model is well-calibrated and provides accurate predictions for the binary survival outcomes. This posterior predictive check confirms that the model is capable of producing predictions that are consistent with the observed data, providing confidence in its predictive power.\n3.6 Visualizing the Logistic Regression Sigmoid Curves To better understand the relationship between Bilirubin (one of the top predictive features) and the survival status of patients, we plot the sigmoid function for each of the posterior samples. The sigmoid curve represents the probability of survival as a function of the Bilirubin levels.\nSigmoid Function: The logistic regression model uses the sigmoid function to transform the linear combination of features into a probability:\n\\begin{aligned} \\text{sigmoid}(x) = \\frac{1}{1 + \\exp{-(\\alpha + x \\cdot \\beta)}} \\end{aligned}\nwhere $ \\alpha $ is the intercept and $ \\beta $ is the coefficient for Bilirubin in this case.\nPosterior Samples: We draw several samples from the posterior distribution of the model parameters (alpha and beta), and for each sample, we plot the corresponding sigmoid curve. This gives us a range of possible outcomes, representing the uncertainty in the model.\n# define the sigmoid function def sigmoid(x, alpha, beta): return 1 / (1 + jnp.exp(-(alpha + jnp.dot(x, beta)))) # generate a range of X values for plotting the sigmoid functions x_range = jnp.linspace(-2, X[:,0].max(), 100) # plot the data plt.figure(figsize = (10, 6)) plt.scatter(X[:, 0], y, color = \u0026#39;red\u0026#39;, label = \u0026#39;Data\u0026#39;, edgecolors = \u0026#39;black\u0026#39;) # bilirubin vs status # plot sigmoid functions from the posterior samples for i in range(len(x_range)): alpha_sample = posterior_samples[\u0026#39;alpha\u0026#39;][i] beta_sample = posterior_samples[\u0026#39;beta\u0026#39;][i, 0] y_sample = sigmoid(x_range, alpha_sample, beta_sample) plt.plot(x_range, y_sample, color = \u0026#39;blue\u0026#39;, alpha = 0.1, ) plt.xlabel(top_features[0]) plt.ylabel(\u0026#39;Status\u0026#39;) plt.title(\u0026#39;Cirrhosis Classification Model\u0026#39;) plt.legend(); Interpretation of the Plot Red Points: These represent the actual data, with the x-axis showing Bilirubin levels and the y-axis showing the observed survival status (0 or 1).\nBlue Sigmoid Curves: The faint blue lines represent the range of sigmoid functions sampled from the posterior distribution. These curves show how the model\u0026rsquo;s predictions vary depending on different samples from the posterior. The spread of the curves reflects the uncertainty in the model\u0026rsquo;s predictions.\nGeneral Trend: As Bilirubin levels increase, the probability of survival decreases, as shown by the steep rise of the sigmoid curves from 0 to 1. This suggests that higher Bilirubin levels are associated with a higher likelihood of not surviving.\nThis plot provides a clear visualization of how the model is using Bilirubin to predict patient survival and the uncertainty in those predictions.\n3.7 Model Evaluation: Confusion Matrix After generating predictions using the posterior samples, we evaluate the model\u0026rsquo;s performance by comparing the predicted classes to the actual survival outcomes. This is done using a confusion matrix, which summarizes the classification results in terms of:\nTrue Positives (TP): Correct predictions of non-survival. True Negatives (TN): Correct predictions of survival. False Positives (FP): Incorrect predictions of non-survival. False Negatives (FN): Incorrect predictions of survival. In the code:\nWe use the sigmoid function to convert the posterior samples into probabilities. We then average the probabilities across all posterior samples and apply a threshold of 0.5 to classify each patient as either survived (0) or not survived (1). Finally, we generate a confusion matrix to visually assess the performance of the classifier. from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score # generate predictions # use sigmoid function defined previously # generate predictions using the posterior samples n_samples = posterior_samples[\u0026#39;alpha\u0026#39;].shape[0] # calculate the mean probability across posterior samples predicted_probabilities = np.mean( [sigmoid(X, posterior_samples[\u0026#39;alpha\u0026#39;][i], posterior_samples[\u0026#39;beta\u0026#39;][i] ) for i in range(n_samples)], axis = 0) # convert probabilities to binary predictions threshold = 0.5 predicted_classes = (predicted_probabilities \u0026gt;= threshold).astype(int) # compare predicted classes with true classes # actual labels y_true = y # confusion matrix conf_matrix = confusion_matrix(y_true, predicted_classes) plt.figure(figsize=(10, 6)) sns.heatmap(conf_matrix, annot = True, fmt = \u0026#39;d\u0026#39;); Confusion Matrix Interpretation In the plot:\n233 true negatives: The model correctly predicted 233 patients as survivors. 98 true positives: The model correctly predicted 98 patients as non-survivors. 24 false positives: The model incorrectly predicted 24 patients as non-survivors when they actually survived. 63 false negatives: The model incorrectly predicted 63 patients as survivors when they did not survive. Overall Performance The confusion matrix shows that the model performs well on survival predictions, with more correct predictions than incorrect ones. The balance between true positives and true negatives suggests that the model handles both classes reasonably well, but there are more false negatives than false positives, indicating that the model might underpredict non-survival cases slightly.\n4. Bayesian Survival Analysis In this section, we shift our focus from classification to Bayesian survival analysis. Survival analysis is a statistical method used to estimate the time until an event of interest occurs, often referred to as \u0026ldquo;time-to-event\u0026rdquo; data. In our case, we are interested in modeling the time until either a patient succumbs to cirrhosis or is censored (i.e., the event does not occur during the observation period). This form of analysis allows us to incorporate censored data and provides insights into the distribution of survival times.\n4.1 Basics of Survival Analysis In classical survival analysis, we model the survival function $S(t)$, which represents the probability of survival beyond time $t$:\n\\begin{aligned} S(t) = P(T \u0026gt; t) \\end{aligned}\nWhere:\n$T$ is the random variable representing the time to the event (e.g., death or censoring). $t$ is the specific time of interest. The complementary function, called the hazard function, $h(t)$, represents the instantaneous rate at which events occur at time $t$, given that the individual has survived up to that time. The hazard function is defined as:\n\\begin{aligned} h(t) = \\frac{f(t)}{S(t)} \\end{aligned}\nWhere $f(t)$ is the probability density function of survival times.\nBayesian Survival Analysis In a Bayesian framework, we treat the parameters governing the survival function (or hazard function) as random variables, and we estimate their posterior distributions using observed data. By incorporating prior beliefs about these parameters, Bayesian methods offer a natural way to handle uncertainty, particularly in the presence of censored data.\nThe general approach involves:\nDefining a Parametric Survival Model: A common choice for the survival model is the Weibull distribution, which provides flexibility in modeling both increasing and decreasing hazard rates. Priors: Assign prior distributions to the model parameters (e.g., shape and scale parameters of the Weibull distribution). Likelihood: The likelihood is defined based on the observed survival times and whether the event was censored. Posterior Inference: Use Markov Chain Monte Carlo (MCMC) or similar methods to draw samples from the posterior distribution of the parameters. For example, the Weibull distribution is a common parametric model for survival times. Its survival function is given by:\n\\begin{aligned} S(t) = \\exp \\left( - \\left( \\frac{t}{\\lambda} \\right)^\\kappa \\right) \\end{aligned}\nWhere:\n$\\lambda$ is the scale parameter. $\\kappa$ is the shape parameter. In the Bayesian context, we assign priors to $\\lambda$ and $\\kappa$, then use the observed data to update these priors to form the posterior distributions.\nCensored Data In survival analysis, it\u0026rsquo;s common to have censored data, where we know that the event of interest did not occur before a certain time, but we do not know the exact time of the event. Bayesian methods handle censored data naturally within the likelihood function by accounting for both observed events and censored observations.\nThe likelihood for a censored observation is:\n\\begin{aligned} P(T \u0026gt; t \\mid \\text{censored}) = S(t) \\end{aligned}\nThus, in a Bayesian survival model, the posterior distribution is based on both observed survival times and the fact that certain observations were censored.\nThis framework allows for a flexible and powerful analysis of time-to-event data while handling uncertainties in parameter estimates.\n4.2 Weibull Survival Model Without Covariates We begin by defining a Weibull survival model for time-to-event data, with no covariates. The Weibull distribution is a flexible parametric model commonly used in survival analysis due to its ability to model both increasing and decreasing hazard rates over time, depending on the value of its shape parameter $k$.\nThe model assumes that survival times follow a Weibull distribution parameterized by:\n$k$ (shape parameter): This determines the form of the hazard function. A value of $k \u0026gt; 1$ suggests an increasing hazard rate, while $k \u0026lt; 1$ implies a decreasing hazard rate. $\\lambda$ (scale parameter): This controls the scale of the distribution and shifts the time-to-event distribution horizontally. Code Explanation Priors:\nWe place Exponential(1.0) priors on both the shape parameter $k$ and the scale parameter $\\lambda$. The exponential distribution is a standard choice when we do not have strong prior information and expect non-negative values for these parameters.\n$k \\sim \\text{Exponential}(1.0)$\n$\\lambda \\sim \\text{Exponential}(1.0)$\nLikelihood:\nThe likelihood assumes that the observed survival times follow a Weibull distribution. We model the observed data (obs) as being drawn from a Weibull distribution, parameterized by the sampled values of $k$ and $\\lambda$.\nThe survival times in data are modeled using this likelihood, and if no data is provided, a placeholder dimension (1000) is used to define the plate.\nThe Weibull model is ideal for analyzing baseline survival without introducing any covariates to the model.\ndef survival_weibull_model(data = None): # define priors k = numpyro.sample(\u0026#39;k\u0026#39;, dist.Exponential(1.0)) lam = numpyro.sample(\u0026#39;lam\u0026#39;, dist.Exponential(1.0)) # likelihood with numpyro.plate(\u0026#39;data\u0026#39;, data.shape[0] if data is not None else 1000): numpyro.sample(\u0026#39;obs\u0026#39;, dist.Weibull(k, lam), obs = data) Preparing the Data for Survival Analysis Before fitting the survival model, we need to preprocess the data by selecting only the uncensored survival times. In survival analysis, censored data refers to cases where the event of interest (e.g., death) did not occur during the observation period, and we do not know the exact time of the event. For this initial analysis, we focus on observed events.\nsurvival_times_observed: We extract the survival times for patients where the event (death) was observed, indicated by Status == 1. This ensures that only complete (uncensored) data is included in the model. # prepare the data # select only the uncensored data survival_times_observed = data[data[\u0026#39;Status\u0026#39;] == 1][\u0026#39;N_Days\u0026#39;].values Fitting the Weibull Survival Model We now proceed to fit the Weibull survival model to the uncensored survival times using Markov Chain Monte Carlo (MCMC) with the No-U-Turn Sampler (NUTS). This step involves estimating the posterior distributions of the model parameters ($k$ and $\\lambda$) based on the observed data.\nNUTS: The No-U-Turn Sampler is a variant of Hamiltonian Monte Carlo (HMC), which efficiently explores the posterior distribution by dynamically tuning the step size and trajectory length. num_warmup: The number of warm-up (burn-in) iterations, where the sampler adapts to the posterior distribution before drawing final samples. num_samples: The number of posterior samples to draw after the warm-up phase. num_chains: The number of independent MCMC chains to run in parallel, which helps assess convergence and reduce bias. After running the MCMC sampler, we print a summary of the estimated posterior distributions for the model parameters.\n# fit the model mcmc_survival_weibull = MCMC(NUTS(survival_weibull_model), num_warmup = 1000, num_samples = 1000, num_chains = 4) mcmc_survival_weibull.run(random.PRNGKey(42), data = survival_times_observed) mcmc_survival_weibull.print_summary() 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] mean std median 5.0% 95.0% n_eff r_hat k 48.25 7.99 47.86 35.15 61.08 1442.43 1.00 lam 0.25 0.02 0.25 0.22 0.28 1419.78 1.00 Number of divergences: 0 Interpretation of Weibull Model Posterior Estimates The output from the MCMC run provides the posterior distributions for the shape parameter $k$ and the scale parameter $\\lambda$ of the Weibull survival model. Here’s a brief interpretation of the results:\n$k$ (shape parameter): The posterior mean of $k$ is 48.25, with a standard deviation of 7.99. The 90% credible interval (from 5% to 95%) ranges from 35.15 to 61.08. A high value of $k$ suggests that the hazard rate increases rapidly over time, indicating that the risk of the event (death) increases as time progresses.\n$\\lambda$ (scale parameter): The posterior mean of $\\lambda$ is 0.25, with a standard deviation of 0.02. The 90% credible interval ranges from 0.22 to 0.28. The scale parameter affects the distribution\u0026rsquo;s spread; smaller values of $\\lambda$ indicate that the survival times are more concentrated around lower values, meaning shorter survival times are more likely.\nEffective Sample Size (n_eff): Both $k$ and $\\lambda$ have high effective sample sizes, indicating that the MCMC chains were well-mixed and independent, producing a sufficient number of effective samples.\n$\\hat{R}$ values: The $\\hat{R}$ values for both parameters are exactly 1.00, indicating that the MCMC chains have converged well, and the posterior estimates are reliable.\nNumber of Divergences: There were 0 divergences, meaning that the NUTS sampler encountered no issues while exploring the posterior distribution.\nThe posterior estimates suggest that the risk of the event (death) increases as time passes, with the hazard rate rising rapidly due to the large shape parameter $k$. The small scale parameter $\\lambda$ indicates that shorter survival times are common in the dataset.\nPrior Predictive Check for the Weibull Survival Model Before we assess the model\u0026rsquo;s posterior predictions, we perform a prior predictive check to ensure that the prior distributions we\u0026rsquo;ve chosen are reasonable. This step involves generating data based purely on the priors, without any observed data influencing the predictions. By comparing the prior predictive distribution to the observed data, we can determine if our prior assumptions lead to plausible outcomes.\nCode Explanation Predictive: We use the Predictive class to generate survival times from the prior distribution by drawing 1,000 samples. Prior Predictive Plot: The plot visualizes the prior predictive distribution, represented by the dashed orange line (the mean of the prior predictions), and the solid blue line shows the individual prior predictive samples. # perform prior predictive check prior_predictive_survival_weibull = Predictive(survival_weibull_model, num_samples = 1000) prior_samples_survival_weibull = prior_predictive_survival_weibull(random.PRNGKey(42), data = survival_times_observed) # convert to InferenceData prior_predictive_data_survival_weibull = az.from_numpyro(mcmc_survival_weibull, prior = prior_samples_survival_weibull) # prior_samples_survival_weibull prior_predictive_data_survival_weibull # plot the prior samples az.plot_ppc(prior_predictive_data_survival_weibull, group = \u0026#39;prior\u0026#39;); Interpretation of the Plot The solid blue line represents the individual prior predictive samples, while the dashed orange line represents the mean of these prior predictions. The plot shows how the model\u0026rsquo;s prior assumptions generate survival times, and we can compare this to the range of observed data to evaluate the plausibility of the priors. In this case, the prior predictive distribution seems to cover a reasonable range of potential survival times, suggesting that the prior assumptions about the shape and scale of the Weibull distribution are not overly restrictive or implausible. This prior predictive check gives us confidence that the model\u0026rsquo;s priors are sensible, providing a good foundation for further inference when we include the observed data.\nPosterior Trace Plot for Weibull Survival Model After fitting the Weibull survival model, we generate a trace plot to assess the MCMC sampling process and the posterior distributions of the model parameters. The trace plot shows both the posterior distributions and the sampling traces for the shape parameter $k$ and the scale parameter $\\lambda$.\nCode Explanation survival_posterior_samples_weibull: We extract the posterior samples for the model parameters. az.plot_trace: This function generates the trace plot, showing the posterior distribution and the corresponding MCMC trace for each parameter. survival_posterior_samples_weibull = mcmc_survival_weibull.get_samples() az_survival_weibull = az.from_numpyro(mcmc_survival_weibull) az.plot_trace(az_survival_weibull, compact = False); Interpretation of the Trace Plot Posterior Distributions (Left Panels):\nThe left panels show the posterior distributions for $k$ and $\\lambda$. Both parameters exhibit well-defined, unimodal distributions, which indicate stable and meaningful posterior estimates. For $k$, the distribution has a peak around 48, while for $\\lambda$, the peak is around 0.25. Sampling Trace (Right Panels):\nThe right panels show the sampling trace for each parameter across the MCMC iterations. Both $k$ and $\\lambda$ exhibit well-mixed traces, with no visible trends or drifts over time. This indicates that the sampler explored the parameter space effectively, without getting stuck in any particular region. The trace plots confirm that the MCMC chains have converged well, as there is good overlap between the chains and no sign of poor mixing. Overall, the trace plot shows that the MCMC sampling worked efficiently, with well-behaved posterior distributions and convergence for both $k$ and $\\lambda$. This gives us confidence in the reliability of the posterior estimates for the survival model.\nPosterior Predictive Check for Weibull Survival Model We now perform a posterior predictive check to evaluate how well the Weibull survival model, after being fitted to the data, predicts survival times. Posterior predictive checks allow us to compare the model\u0026rsquo;s predicted values to the observed data, providing insight into how well the model captures the true underlying patterns.\nCode Explanation Predictive: We use the Predictive class to generate survival times based on the posterior samples of the model parameters ($k$ and $\\lambda$). Posterior Predictive Data: We create an InferenceData object using the posterior predictive samples, which includes both the predicted survival times and the observed data. Posterior Predictive Plot: The plot visualizes the posterior predictive distribution (blue lines), the observed data (solid black line), and the posterior predictive mean (dashed orange line). survival_posterior_predictive_weibull = Predictive(survival_weibull_model, posterior_samples = survival_posterior_samples_weibull) survival_times_predicted_weibull = survival_posterior_predictive_weibull(random.PRNGKey(42), survival_times_observed) survival_posterior_data_weibull = az.from_numpyro(mcmc_survival_weibull, posterior_predictive = survival_times_predicted_weibull) az.plot_ppc(survival_posterior_data_weibull, kind = \u0026#39;kde\u0026#39;, group = \u0026#39;posterior\u0026#39;, observed = True); Plot Interpretation for Weibull Posterior Predictive vs. Observed Data (Uncensored) This plot shows a comparison between the Weibull posterior predictive distribution and the observed uncensored survival data, without any covariates involved. The purpose of this plot is to evaluate how well the Weibull model, fit to the uncensored data, can replicate the distribution of survival times.\nExplanation of the Plot: X-axis (Survival Times):\nThe x-axis represents the survival times (in days), ranging from 0 to about 4500 days. Y-axis (Density):\nThe y-axis shows the density of survival times, or the likelihood of individuals surviving for certain durations, based on both the observed data and the model\u0026rsquo;s posterior predictions. Lines in the Plot: Blue Line (Weibull Posterior Predictive):\nThis line represents the predicted survival times from the Weibull model. It shows the expected distribution of survival times based on the model\u0026rsquo;s posterior distribution. Green Line (Weibull Observed Data):\nThis line represents the true observed distribution of uncensored survival times. It shows the actual survival time density for individuals who experienced the event (e.g., death). Interpretation: Fit Between Lines:\nThe green line (observed data) and the blue line (posterior predictive) show a relatively good alignment in terms of overall trend, particularly in the early time periods (0 to 2000 days). The posterior predictive distribution follows the general trend of the observed data, which suggests that the Weibull model does an acceptable job at capturing the distribution of survival times in this range. Discrepancies:\nSimilar to the lognormal model, we observe that the posterior predictive line fluctuates more in the earlier survival times, indicating higher uncertainty or variability in the predictions. The fit becomes more diverging in the later periods (beyond 2500 days), where the posterior predictive distribution starts to exhibit higher peaks and troughs compared to the observed data, suggesting that the Weibull model may struggle to accurately capture extreme or late survival times. Conclusion:\nThe Weibull model generally captures the trend of survival times, but as seen, there are some inconsistencies in the later survival times, where the model tends to overestimate or underestimate specific time periods. These fluctuations indicate that while the model is useful for capturing general trends, it might not fully account for more extreme cases of survival. az.plot_kde(survival_posterior_data_weibull[\u0026#39;posterior_predictive\u0026#39;][\u0026#39;obs\u0026#39;] .to_numpy() .flatten(), plot_kwargs = {\u0026#39;color\u0026#39; : \u0026#39;C0\u0026#39;, \u0026#39;label\u0026#39; : \u0026#39;Weibull Posterior Predictive\u0026#39;}) az.plot_kde(survival_posterior_data_weibull[\u0026#39;observed_data\u0026#39;][\u0026#39;obs\u0026#39;] .to_numpy() .flatten(), plot_kwargs = {\u0026#39;color\u0026#39; : \u0026#39;C2\u0026#39;, \u0026#39;label\u0026#39; : \u0026#39;Weibull Observed Data\u0026#39;}) \u0026lt;Axes: \u0026gt; Interpretation of the Posterior Predictive Check Plot Blue Lines (Posterior Predictive Samples): These represent the survival times generated from the posterior distribution. The spread of the lines shows the model\u0026rsquo;s uncertainty in predicting survival times based on the posterior samples.\nSolid Black Line (Observed Data): This represents the actual observed survival times.\nDashed Orange Line (Posterior Predictive Mean): This shows the mean of the posterior predictive samples.\nIn this case, the posterior predictive samples align well with the observed survival data, suggesting that the Weibull survival model provides a good fit to the data. The predicted survival times fall within a reasonable range around the observed data, and the posterior predictive mean closely follows the observed survival trend. This check confirms that the model has successfully learned from the data and is capable of generating realistic survival time predictions.\nPosterior Summary for Weibull Survival Model The summary table provides key statistics for the posterior distributions of the shape parameter ($k$) and scale parameter ($\\lambda$) of the Weibull survival model. These values give us insights into the uncertainty and characteristics of the estimated parameters after fitting the model to the data.\nKey Metrics: Mean: The posterior mean of each parameter, representing the expected value based on the posterior distribution.\n$k$ (shape): 48.25 — This value indicates that the hazard rate increases rapidly over time. $\\lambda$ (scale): 0.25 — A small scale parameter, which suggests that the model predicts shorter survival times on average. Standard Deviation (sd): The standard deviation of the posterior distribution, reflecting the uncertainty in the parameter estimates.\n$k$: 7.99 $\\lambda$: 0.017 HDI 3% - 97%: The highest density interval (HDI) represents the 94% credible interval for the parameters, showing the range of likely values.\n$k$: 34.08 to 63.65 $\\lambda$: 0.219 to 0.283 ESS (Effective Sample Size): Indicates the number of effectively independent samples in the posterior. High values of ESS mean that the MCMC chains mixed well, and the posterior is reliable.\n$k$: 1451.0 $\\lambda$: 1423.0 $\\hat{R}$ (r_hat): A diagnostic measure for MCMC convergence. Values close to 1.0 indicate good convergence, meaning the chains have reached the same distribution.\nBoth $k$ and $\\lambda$ have $\\hat{R} = 1.0$, confirming that the model has converged. The summary shows that the model has successfully converged, with high effective sample sizes and credible intervals that provide reasonable bounds on the parameter estimates. The increasing hazard rate (high $k$ value) suggests that the risk of death increases over time, while the small $\\lambda$ value indicates relatively short survival times. This analysis gives us a good understanding of the survival patterns in the data.\naz.summary(survival_posterior_data_weibull) mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat k 48.249 7.987 34.077 63.653 0.209 0.148 1451.0 1795.0 1.0 lam 0.251 0.017 0.219 0.283 0.000 0.000 1423.0 1762.0 1.0 4.3 Weibull Survival Model With Covariates In this section, we will extend the Weibull survival model to account for individual-specific covariates. by introducing covariates, we allow the model to make \u0026ldquo;personalized\u0026rdquo; predictions for survival times, based on characteristics like age, bilirubin levels and albumin.\nThe scale parameter $\\lambda_i$, which influences the expected survival time, will now depend on a combination of covariates (characteristics) for each individual. This makes the model more flexible and capable of capturing how different factors impact survival.\nThe scale parameter $\\lambda_i$ for individual $i$ is modeled as:\n$$\\lambda_i = \\exp \\left( \\beta_0 + \\beta_1 \\cdot x_{1i} + \\beta_2 \\cdot x_{2i} + \\dots + \\beta_k \\cdot x_{ki} \\right)$$\nWhere:\n$ x_{1i}, x_{2i}, \\dots, x_{ki} $ are the covariates for individual i , $ \\beta_0, \\beta_1, \\dots, \\beta_k $ are the regression coefficients for each covariate. The likelihood function for the observed survival times $t_i$ is given by the Weibull distribution:\n$$ f(t_i | k, \\lambda_i) = \\frac{k}{\\lambda_i} \\left( \\frac{t_i}{\\lambda_i} \\right)^{k-1} \\exp \\left(- \\left(\\frac{t_i}{\\lambda_i}\\right)^k\\right) $$\nWhere:\n$k$ is the shape parameter, $\\lambda_i$ is the scale parameter that varies with covariates. By estimating the coefficients $\\beta$, we can assess how each covariate influences survival time. This approach personalizes the model to each individual\u0026rsquo;s characteristics, leading to more tailored predictions.\nWe will implement this in the following steps:\nDefine the model where $\\lambda_i$ is a function of covariates. Prepare the data (covariates and survival times) Perform MCMC sampling to fit the model Analyze the results. Define the Weibull Survival Model with Covariates We will now define the model with priors for both the shape parameter $k$ and the regression coefficients $\\beta$. The scale parameter $\\lambda_i$ will be modeled as the exponential of a linear combination of the covariates.\ndef survival_model_weibull_with_covariates(covariates, data): # define priors k = numpyro.sample(\u0026#39;k\u0026#39;, dist.Exponential(1.0)) beta = numpyro.sample(\u0026#39;beta\u0026#39;, dist.Normal(0, 1).expand([covariates.shape[1]])) # coefficients (beta) # linear model for log(lambda) log_lambda = jnp.dot(covariates, beta) # linear combination of covariates lambda_i = jnp.exp(log_lambda) # ensure scale parameter lambda is positive # likelihood with numpyro.plate(\u0026#39;data\u0026#39;, data.shape[0] if data is not None else 1000): numpyro.sample(\u0026#39;obs\u0026#39;, dist.Weibull(k, lambda_i), obs = data) Selecting Covariates for the Weibull Model Before fitting the Weibull model with covariates, we need to ensure that the relevant covariates are selected from the dataset. These covariates will be used to personalize the scale parameter $\\lambda_i$ for each individual, allowing the model to capture how various features influence survival times.\nIn this step, we simply print the list of top features, which were previously selected based on their correlation with the survival outcome. These features will serve as the covariates in the model.\nprint(top_features.to_list()) ['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos'] To extend the Weibull model with covariates, we need to prepare two key components:\nCovariates: The selected covariates (features) that will be used to predict survival times for each individual. Survival Times: The observed survival times for the uncensored data (i.e., where the event of interest has occurred). Code Explanation covariate_selection: We explicitly select a subset of covariates that are expected to have an influence on survival, including Bilirubin, Edema, Prothrombin, Albumin, and Age. covariates_survival: We filter the dataset to include only individuals with observed survival times (where Status == 1) and extract the covariate values. survival_times: We extract the corresponding survival times for these individuals. Finally, we print the top features and the selected covariates to verify that the correct data is being used for the model.\ncovariate_selection = [\u0026#39;Bilirubin\u0026#39;, \u0026#39;Edema\u0026#39;, \u0026#39;Prothrombin\u0026#39;, \u0026#39;Albumin\u0026#39;, \u0026#39;Age\u0026#39;] covariates_survival = data[data[\u0026#39;Status\u0026#39;] == 1][covariate_selection].values survival_times = data[data[\u0026#39;Status\u0026#39;] == 1][\u0026#39;N_Days\u0026#39;].values print(top_features.to_list()) print(covariate_selection) ['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos'] ['Bilirubin', 'Edema', 'Prothrombin', 'Albumin', 'Age'] Fitting the Weibull Survival Model with Covariates We now fit the Weibull survival model with covariates using Markov Chain Monte Carlo (MCMC) with the No-U-Turn Sampler (NUTS). This model accounts for the effects of the selected covariates on the survival times, making the predictions more personalized for each individual.\nCode Explanation NUTS: We use the NUTS kernel to efficiently explore the posterior distribution of the parameters. MCMC Setup: The model is run with 1,000 warm-up iterations, followed by 2,000 sampling iterations, and using 4 chains to assess convergence. Covariates: The scale parameter $\\lambda_i$ is modeled as a function of the selected covariates, allowing the model to account for individual characteristics when predicting survival times. nuts_kernel_survival_weibull_with_covariates = NUTS(survival_model_weibull_with_covariates) mcmc_survival_weibull_with_covariates = MCMC(nuts_kernel_survival_weibull_with_covariates, num_warmup = 1000, num_samples = 2000, num_chains = 4) mcmc_survival_weibull_with_covariates.run(random.PRNGKey(42), data = survival_times_observed, covariates = covariates_survival) mcmc_survival_weibull_with_covariates.print_summary() 0%| | 0/3000 [00:00\u0026lt;?, ?it/s] 0%| | 0/3000 [00:00\u0026lt;?, ?it/s] 0%| | 0/3000 [00:00\u0026lt;?, ?it/s] 0%| | 0/3000 [00:00\u0026lt;?, ?it/s] mean std median 5.0% 95.0% n_eff r_hat beta[0] 0.07 0.04 0.07 0.03 0.12 2.96 1.70 beta[1] 0.03 0.54 -0.24 -0.36 0.96 2.02 8.85 beta[2] -0.05 0.05 -0.06 -0.11 0.04 2.42 2.29 beta[3] 0.12 0.39 -0.10 -0.14 0.80 2.00 20.11 beta[4] -0.19 0.30 -0.03 -0.72 0.01 2.01 14.35 k 284.18 163.08 372.41 2.42 392.76 nan 15.33 Number of divergences: 2000 Results Interpretation: The output provides the posterior estimates for the regression coefficients ($\\beta$) and the shape parameter ($k$), along with diagnostics.\n$\\beta$ coefficients:\nbeta[0] (Bilirubin): Mean of 0.07, indicating a small positive effect on survival time, though there is substantial uncertainty as indicated by the standard deviation (std). beta[1] to beta[4] (Other Covariates): The estimates for the remaining covariates (Edema, Prothrombin, Albumin, Age) show wide credible intervals and large $\\hat{R}$ values, suggesting poor convergence and high uncertainty in the estimates. Shape Parameter ($k$): The mean estimate for $k$ is 284.18, with a high standard deviation, indicating extreme variability in the estimates. Additionally, the model encountered 2,000 divergences, signaling serious issues with the MCMC sampling process.\nConvergence Issues:\nThe $\\hat{R}$ values for most of the parameters are far from 1.0, indicating that the chains did not converge. Effective Sample Size (n_eff): Extremely low values for the effective sample size show that the sampler struggled to explore the posterior efficiently, producing highly correlated samples. Divergences: The model reported 2,000 divergences, suggesting that the NUTS sampler encountered significant problems in exploring the posterior, which likely stems from poor model specification or highly informative priors. The results indicate that the model failed to converge, likely due to the high number of divergences and poor effective sample sizes. These issues point to potential problems with the model\u0026rsquo;s specification, such as overly complex priors or insufficient data to support the model\u0026rsquo;s structure. In subsequent steps, it may be necessary to simplify the model, adjust the priors, or address potential multicollinearity in the covariates.\n4.4 Introducing the Log-Normal Survival Model After encountering issues with the Weibull model with covariates, we modify the model structure and introduce a log-normal survival model with covariates. This change in model specification can sometimes lead to better performance or convergence when the original model struggles with the data.\nThe log-normal distribution assumes that the logarithm of the survival times follows a normal distribution, which is another common choice for modeling time-to-event data. Unlike the Weibull distribution, the log-normal distribution may provide better flexibility when the hazard rate does not follow the typical monotonic increasing or decreasing pattern.\nCode Explanation Priors:\nWe place Normal(0, 1) priors on the regression coefficients $\\beta$, which influence how each covariate impacts the survival time. An Exponential(1.0) prior is assigned to $\\sigma$, the standard deviation, to ensure positivity. Log-Mu (Mean of Log-Survival Times):\nThe linear combination of covariates is modeled as $ \\log(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{1i} + \\beta_2 \\cdot x_{2i} + \\dots + \\beta_k \\cdot x_{ki} $, where $x_{1i}, x_{2i}, \\dots, x_{ki}$ are the covariates for individual $i$. This represents the log of the expected survival time for each individual. Likelihood:\nThe observed survival times are modeled as being drawn from a LogNormal distribution, where the mean is $\\log(\\mu)$ and the standard deviation is $\\sigma$. This distribution models the uncertainty in the survival times, allowing the log-transformed times to follow a normal distribution, while ensuring that the predicted survival times themselves are positive. This adjustment in the model may lead to improved performance, especially if the underlying survival times fit better with a log-normal assumption rather than a Weibull distribution.\ndef lognormal_survival_model_with_covariates(covariates, data): # define priors beta = numpyro.sample(\u0026#39;beta\u0026#39;, dist.Normal(0, 1).expand([covariates.shape[1]])) # coefficients (beta) sigma = numpyro.sample(\u0026#39;sigma\u0026#39;, dist.Exponential(1.0)) # linear model for log(mu) log_mu = jnp.dot(covariates, beta) # linear combination of covariates # likelihood with numpyro.plate(\u0026#39;data\u0026#39;, len(data)): numpyro.sample(\u0026#39;obs\u0026#39;, dist.LogNormal(log_mu, sigma), obs = data) Fitting the Log-Normal Survival Model with Covariates We now fit the log-normal survival model with covariates using the No-U-Turn Sampler (NUTS). This model assumes that the logarithm of the survival times follows a normal distribution, and the scale parameter (or mean of the log-survival times) depends on a linear combination of the selected covariates.\nCode Explanation Feature Selection: We choose a new set of covariates based on their relevance to survival: Bilirubin, Edema, Copper, Prothrombin, Albumin, and Age. These features are expected to influence survival times based on prior domain knowledge or their relationship with survival. MCMC with NUTS: The log-normal model is fitted using MCMC with 1,000 warm-up iterations and 1,000 sampling iterations. The NUTS sampler explores the posterior distribution of the model parameters, accounting for the covariates in predicting survival times. By running this code, we estimate the posterior distributions of the regression coefficients ($\\beta$) and the standard deviation ($\\sigma$) for the log-normal survival model.\nselection_features_lognormal = [\u0026#39;Bilirubin\u0026#39;, \u0026#39;Edema\u0026#39;, \u0026#39;Copper\u0026#39;, \u0026#39;Prothrombin\u0026#39;, \u0026#39;Albumin\u0026#39;, \u0026#39;Age\u0026#39;] covariates_survival_lognormal = data[data[\u0026#39;Status\u0026#39;] == 1][selection_features_lognormal].values survival_times = data[data[\u0026#39;Status\u0026#39;] == 1][\u0026#39;N_Days\u0026#39;].values mcmc_survival_with_covariates_lognormal = MCMC(NUTS(lognormal_survival_model_with_covariates), num_warmup = 1000, num_samples = 1000, num_chains = 4) mcmc_survival_with_covariates_lognormal.run(random.PRNGKey(42), data = survival_times_observed, covariates = covariates_survival_lognormal) mcmc_survival_with_covariates_lognormal.print_summary() 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] mean std median 5.0% 95.0% n_eff r_hat beta[0] 0.79 0.34 0.80 0.21 1.34 5897.04 1.00 beta[1] 1.57 0.60 1.56 0.64 2.56 5230.58 1.00 beta[2] 0.56 0.38 0.56 -0.07 1.18 6122.26 1.00 beta[3] 0.60 0.41 0.60 -0.04 1.30 6428.30 1.00 beta[4] 0.03 0.42 0.03 -0.70 0.68 5850.73 1.00 beta[5] 1.23 0.46 1.23 0.45 1.95 5866.68 1.00 sigma 5.96 0.33 5.94 5.39 6.47 6314.24 1.00 Number of divergences: 0 Results Interpretation for the Log-Normal Survival Model with Covariates The output provides the posterior estimates for the regression coefficients ($\\beta$) and the standard deviation ($\\sigma$) in the log-normal survival model. The model successfully converged with no divergences, and the effective sample sizes ($n_{\\text{eff}}$) and $\\hat{R}$ values indicate good convergence.\n$\\beta$ coefficients:\nbeta[0] (Bilirubin): Mean of 0.79, suggesting that higher Bilirubin levels are associated with a higher log-survival time, meaning longer survival on average. beta[1] (Edema): Mean of 1.57, showing a relatively strong positive effect on survival. This indicates that patients with Edema have longer predicted survival times. beta[2] (Copper): Mean of 0.56, suggesting that higher copper levels are also associated with longer survival times, though the effect size is smaller than for Bilirubin and Edema. beta[3] (Prothrombin): Mean of 0.60, indicating a positive relationship between Prothrombin and survival time. beta[4] (Albumin): Mean of 0.03, suggesting that the effect of Albumin on survival time is weak and uncertain, as evidenced by the wide credible interval. beta[5] (Age): Mean of 1.23, suggesting that older age is associated with longer survival times, which might be counterintuitive but could reflect a complex interaction with other covariates in this dataset. Standard Deviation ($\\sigma$):\nThe mean estimate for $\\sigma$ is 5.96, indicating that there is considerable variability in the log-survival times that the model is capturing. The narrow credible interval (5.39 to 6.47) suggests that the uncertainty around this parameter is low. Effective Sample Size ($n_{\\text{eff}}$) and $\\hat{R}$ values:\nAll parameters have high effective sample sizes, indicating good exploration of the posterior distribution during sampling. The $\\hat{R}$ values are all 1.00, indicating that the MCMC chains have converged well. The results indicate that the log-normal model has fit the data well, with all parameters showing good convergence and meaningful posterior estimates. The positive $\\beta$ coefficients for most covariates suggest that these factors are positively correlated with survival times. However, further analysis might be needed to interpret some of the results, such as the positive association between age and survival.\nUsing a log-normal model may be more suitable than the Weibull model in this case because the log-normal distribution can capture more flexible hazard patterns, including non-monotonic hazard rates, which the Weibull model may struggle with. If the survival data exhibits more complex or non-linear relationships, the log-normal model can provide a better fit by allowing the log of survival times to follow a normal distribution, capturing a wider range of survival behaviors.\nPosterior Trace and Pair Plots for the Log-Normal Survival Model After fitting the log-normal survival model with covariates, we use trace and pair plots to assess the quality of the MCMC sampling and the relationships between the parameters. These visualizations help verify that the posterior distributions are well-behaved and that the sampler has explored the parameter space efficiently.\ntrace_survival_with_covariates_lognormal = az.from_numpyro(mcmc_survival_with_covariates_lognormal) az.plot_trace(trace_survival_with_covariates_lognormal, compact = False) plt.tight_layout() plt.show() az.plot_pair(trace_survival_with_covariates_lognormal, divergences = True) plt.show() Trace Plot Interpretation Posterior Distributions (Left Panels):\nThe left panels show the posterior distributions for the regression coefficients ($\\beta$) and the standard deviation ($\\sigma$). All the distributions appear unimodal and well-defined, with little to no skewness, indicating that the model has produced meaningful estimates for each parameter. Each coefficient corresponds to one of the covariates selected for the model, and $\\sigma$ represents the standard deviation of the log-normal distribution. MCMC Trace (Right Panels):\nThe right panels show the MCMC sampling traces. All traces exhibit good mixing and stability, meaning that the chains are not stuck and have thoroughly explored the parameter space. The lack of any visible trends suggests that the MCMC chains have converged well. Pair Plot Interpretation The pair plots show the joint distributions and scatter plots of the posterior samples for each pair of parameters. The plots indicate minimal correlation between the parameters, as the scatter plots are roughly circular, suggesting that each parameter is being estimated independently of the others. No significant divergences are visible in the scatter plots, confirming that the NUTS sampler did not encounter serious issues during sampling. The trace and pair plots demonstrate that the MCMC sampling worked efficiently, with good mixing and convergence. The posterior distributions are well-defined, and the parameter estimates are reliable. Additionally, the pair plots show that there are no strong correlations between the covariates, supporting the validity of the model structure and the sampling process.\nPosterior Predictive Check for Log-Normal Survival Model with Covariates We now perform a posterior predictive check to evaluate how well the log-normal survival model with covariates predicts the observed survival times. This helps assess the quality of the model’s fit by comparing the predicted and observed data distributions.\nCode Explanation Posterior Predictive Samples: We generate posterior predictive survival times using the posterior samples of the regression coefficients ($\\beta$) and standard deviation ($\\sigma$). This allows us to simulate the model\u0026rsquo;s predictions based on the observed data and the learned parameters. Posterior Predictive Plot: The plot shows the posterior predictive distribution (blue lines), the observed data (black line), and the posterior predictive mean (dashed orange line). Posterior Predictive Check for Log-Normal Survival Model with Covariates We now perform a posterior predictive check to evaluate how well the log-normal survival model with covariates predicts the observed survival times. This helps assess the quality of the model’s fit by comparing the predicted and observed data distributions.\nCode Explanation Posterior Predictive Samples: We generate posterior predictive survival times using the posterior samples of the regression coefficients ($\\beta$) and standard deviation ($\\sigma$). This allows us to simulate the model\u0026rsquo;s predictions based on the observed data and the learned parameters. Posterior Predictive Plot: The plot shows the posterior predictive distribution (blue lines), the observed data (black line), and the posterior predictive mean (dashed orange line). survival_posterior_samples_lognormal = mcmc_survival_with_covariates_lognormal.get_samples() survival_posterior_predictive_lognormal_with_covariates = Predictive(lognormal_survival_model_with_covariates, posterior_samples = survival_posterior_samples_lognormal) survival_times_predicted_lognormal_with_covariates = survival_posterior_predictive_lognormal_with_covariates(random.PRNGKey(42), data = survival_times_observed, covariates = covariates_survival_lognormal) survival_posterior_data_lognormal_with_covariates = az.from_numpyro(mcmc_survival_with_covariates_lognormal, posterior_predictive = survival_times_predicted_lognormal_with_covariates) az.plot_ppc(survival_posterior_data_lognormal_with_covariates, kind = \u0026#39;kde\u0026#39;, group = \u0026#39;posterior\u0026#39;, observed = True); Interpretation of the Posterior Predictive Plot Blue Lines (Posterior Predictive Samples): These represent the model’s predictions for survival times based on the posterior distribution. The spread of the lines illustrates the uncertainty in the predictions.\nSolid Black Line (Observed Data): This represents the actual observed survival times.\nDashed Orange Line (Posterior Predictive Mean): The dashed line shows the mean of the posterior predictive samples.\nThe posterior predictive samples align well with the observed data, suggesting that the log-normal survival model with covariates captures the underlying survival patterns in the data. The predicted survival times fall within a reasonable range around the observed data, and the posterior predictive mean closely follows the trend of the observed survival times. This result indicates that the model has fit the data well and is capable of generating realistic survival time predictions.\nPlot Interpretation for Lognormal Posterior Predictive vs. Observed Data In this step, we visualize the posterior predictive check for the lognormal model with covariates using kernel density estimation (KDE) plots. These plots help us understand how well the model captures the distribution of the survival data.\naz.plot_kde(survival_posterior_data_lognormal_with_covariates[\u0026#39;posterior_predictive\u0026#39;][\u0026#39;obs\u0026#39;].to_numpy().flatten(), plot_kwargs = {\u0026#39;color\u0026#39; : \u0026#39;C0\u0026#39;, \u0026#39;label\u0026#39; : \u0026#39;Lognormal Posterior Predictive\u0026#39;}) az.plot_kde(survival_posterior_data_lognormal_with_covariates[\u0026#39;observed_data\u0026#39;][\u0026#39;obs\u0026#39;].to_numpy().flatten(), plot_kwargs = {\u0026#39;color\u0026#39;: \u0026#39;C2\u0026#39;, \u0026#39;label\u0026#39; : \u0026#39;Observed Data\u0026#39;}) plt.show() Explanation of the Plot: X-axis (Survival Times):\nThe x-axis represents the survival times (in days), ranging from 0 to about 4500 days. Y-axis (Density):\nThe y-axis represents the density of survival times, or how frequently individuals survive for certain durations based on the observed data and model predictions. Lines in the Plot: Blue Line (Lognormal Posterior Predictive Uncensored):\nThis line represents the model\u0026rsquo;s prediction of uncensored survival times using the lognormal distribution. It shows how the model expects individuals to survive over time, based on the posterior samples. Green Line (Observed Data):\nThis line represents the actual observed data of uncensored survival times. It shows the true distribution of survival times for individuals where the event (e.g., death) has occurred. Interpretation: Alignment of the Lines:\nThe blue and green lines show some overlap, especially in the range from 500 to 2000 days. This indicates that the model has captured the trend of survival times relatively well in this region. Discrepancies at the Extremes:\nThe blue line fluctuates more at the start of the timeline (before 1000 days) and at the far right (after 3000 days). This suggests that the model has higher variability in predicting shorter survival times and potentially struggles to match the observed data precisely in those ranges. Overall Fit:\nDespite some variability, the general trend of the posterior predictive distribution (blue) aligns with the observed data (green). This indicates that the lognormal model with covariates is reasonably accurate in predicting uncensored survival times but could be improved in certain time ranges, especially for early and late survival times. By including this plot, we visually validate that the lognormal model captures key aspects of the survival data, though some refinement may be needed for more extreme cases.\n4.5 Log-Normal Survival Model with Covariates and Censored Data In this final part, we extend the log-normal survival model to account for censored data. In survival analysis, censoring occurs when we do not observe the exact event time for some individuals. For example, if a patient is still alive at the end of the study or drops out before experiencing the event (death), their survival time is right-censored—we only know that the event did not occur before a certain time.\nModeling Censored Data To account for censored data, we modify the likelihood function to handle both observed and censored survival times. Specifically, for censored data, the likelihood is based on the survival function rather than the probability density function (PDF).\nLikelihood for Censored and Observed Data For observed survival times $t_i$ (i.e., the event occurred): $$ f(t_i | \\mu_i, \\sigma) = \\frac{1}{t_i \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log t_i - \\mu_i)^2}{2\\sigma^2}\\right) $$\nFor censored survival times $t_i$ (i.e., the event did not occur by time $t_i$): $$ S(t_i | \\mu_i, \\sigma) = 1 - F(t_i | \\mu_i, \\sigma) $$\nWhere:\n$f(t_i | \\mu_i, \\sigma)$ is the log-normal probability density function (PDF) for observed data, $S(t_i | \\mu_i, \\sigma)$ is the survival function, representing the probability of survival beyond time $t_i$ for censored data, $F(t_i | \\mu_i, \\sigma)$ is the cumulative distribution function (CDF) of the log-normal distribution, $\\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_k x_{ki}$ is the mean of the log-survival times for individual $i$, based on their covariates, $\\sigma$ is the standard deviation of the log-survival times. Handling Censoring In the likelihood, we condition on whether the data is censored or not:\nFor observed data, we use the PDF of the log-normal distribution. For censored data, we use the survival function (1 minus the CDF). By incorporating both observed and censored survival times, this model can better capture the full picture of the dataset, including individuals who did not experience the event during the study period.\nSteps for the Model: Define the log-normal survival model with covariates, including censored data. Prepare the dataset, distinguishing between censored and uncensored observations. Fit the model using MCMC and analyze the results. Defining the Log-Normal Survival Model with Censored Data In this model, we incorporate censored data into the log-normal survival model with covariates. Censoring allows us to handle cases where the exact survival time is unknown because the event (e.g., death) has not yet occurred by the end of the observation period.\nCode Explanation Priors:\n$\\sigma$ (standard deviation parameter): We place an Exponential(1.0) prior on $\\sigma$. This parameter controls the spread of the log-normal distribution for survival times, representing the variability in the log-transformed survival times. $\\beta$ (covariate coefficients): If covariates are provided, we define a normal prior on $\\beta$, which adjusts the mean of the log-survival times $\\mu_i$ for each individual based on their characteristics. Handling Censored Data:\nWe use a mask to differentiate between uncensored (event occurred) and censored (event did not occur) data. For uncensored data, the likelihood is based on the log-normal distribution: $f(t_i | \\mu_i, \\sigma)$, where $\\mu_i$ is the mean log-survival time (a linear combination of the covariates). For censored data, the likelihood is based on the survival function of the log-normal distribution: $S(t_i | \\mu_i, \\sigma) = 1 - F(t_i | \\mu_i, \\sigma)$, where $F(t_i | \\mu_i, \\sigma)$ is the cumulative distribution function (CDF) of the log-normal distribution. Likelihood:\nWe define the likelihood separately for uncensored and censored data: For uncensored data, we use the log-normal distribution to model the observed survival times. For censored data, we use a Bernoulli likelihood to model the survival probabilities, representing the probability that the event has not yet occurred by the observed time. This is based on the survival function of the log-normal distribution. By integrating both the observed events and censored data, this model more accurately reflects the reality of survival analysis, where not all individuals experience the event within the study period. The log-normal model offers flexibility, particularly for data where the hazard rate may change over time, making it a more suitable choice for complex survival data.\ndef survival_model_lognormal_with_censored(covariates, survival_times, event_occurred): # define priors sigma = numpyro.sample(\u0026#39;sigma\u0026#39;, dist.Exponential(1.0)) # standard deviation for the log-normal distribution if covariates is not None: beta = numpyro.sample(\u0026#39;beta\u0026#39;, dist.Normal(0, 1).expand([covariates.shape[1]])) # regression coefficients log_mu = jnp.dot(covariates, beta) # linear model for log(mu) else: log_mu = 0 # if no covariates, set log(mu) to 0 (default) # likelihood - handle based on censoring status with numpyro.plate(\u0026#39;data\u0026#39;, survival_times.shape[0]): # uncensored data (event occurred, i.e. event_occurred == 1) uncensored_mask = (event_occurred == 1) numpyro.sample(\u0026#39;obs_uncensored\u0026#39;, dist.LogNormal(log_mu, sigma).mask(uncensored_mask), obs=survival_times) # censored data (event did not occur, i.e. event_occurred == 0) censored_mask = (event_occurred == 0) survival_prob = 1 - dist.LogNormal(log_mu, sigma).cdf(survival_times) numpyro.sample(\u0026#39;obs_censored\u0026#39;, dist.Bernoulli(survival_prob).mask(censored_mask), obs=event_occurred) Preparing the Data for the Log-Normal Survival Model with Censored Data To fit the log-normal survival model with censored data, we need to prepare the following components:\nSurvival Times (survival_times): This represents the number of days each patient survived. These are the time-to-event data, which include both censored and uncensored observations.\nEvent Occurred (event_occurred): This variable indicates whether the event (death) occurred or not. We create a binary indicator where 1 represents that the event occurred (i.e., death), and 0 represents that the data is censored (i.e., the patient was alive at the end of the study).\nCovariates (covariates_survival): We select a set of covariates that will be used to model the individual-specific survival times. In this case, we choose 'Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', and 'Age' as the covariates, which are expected to influence the survival times, as we had already observed in EDA.\nCode Explanation: survival_times: Extracts the survival times (in days) from the dataset. event_occurred: Converts the status variable into a binary format where 1 means the event occurred, and 0 means the data is censored. covariates_survival: Selects the covariates of interest, which are used to predict survival times. This step ensures that we have the necessary data formatted correctly to fit the survival model, accounting for both censored and uncensored data.\n# prepare the data survival_times = data[\u0026#39;N_Days\u0026#39;].values # the survival times (days) event_occurred = (data[\u0026#39;Status\u0026#39;] == 1).astype(int).values # 1 if event (death), 0 if censored # select your covariates covariates_survival = data[[\u0026#39;Bilirubin\u0026#39;, \u0026#39;Edema\u0026#39;, \u0026#39;Copper\u0026#39;, \u0026#39;Prothrombin\u0026#39;, \u0026#39;Albumin\u0026#39;, \u0026#39;Age\u0026#39;]].values Fitting the Weibull Survival Model with Censored Data We now fit the Weibull survival model with covariates and censored data using Markov Chain Monte Carlo (MCMC) with the No-U-Turn Sampler (NUTS). The model accounts for both censored and uncensored survival times, making it suitable for real-world survival analysis where not all events occur within the observation window.\nCode Explanation NUTS Sampler: We use the NUTS kernel to efficiently explore the posterior distribution of the parameters. MCMC Setup: The model is run with 1,000 warm-up iterations and 1,000 sampling iterations, with 4 independent chains to ensure proper convergence and mixing. Input Data: Covariates: The selected covariates are passed to the model. Survival Times: The observed or censored survival times are provided to the model. Event Occurred: This binary indicator differentiates between censored (0) and uncensored (1) data. # fit the Weibull model with censored data mcmc_lognormal_censored = MCMC(NUTS(survival_model_lognormal_with_censored), num_warmup=1000, num_samples=1000, num_chains=4) mcmc_lognormal_censored.run(random.PRNGKey(42), covariates=covariates_survival, survival_times=survival_times, event_occurred=event_occurred) mcmc_lognormal_censored.print_summary() 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] 0%| | 0/2000 [00:00\u0026lt;?, ?it/s] mean std median 5.0% 95.0% n_eff r_hat beta[0] 0.91 0.30 0.91 0.44 1.42 5068.22 1.00 beta[1] 1.32 0.52 1.31 0.50 2.20 5117.46 1.00 beta[2] 0.61 0.32 0.61 0.11 1.14 6312.73 1.00 beta[3] 0.68 0.34 0.68 0.09 1.20 6077.50 1.00 beta[4] -0.02 0.35 -0.02 -0.62 0.52 6067.08 1.00 beta[5] 1.16 0.35 1.16 0.59 1.74 6004.50 1.00 sigma 5.25 0.24 5.24 4.84 5.62 6263.06 1.00 Number of divergences: 0 Results Interpretation for the Log-Normal Survival Model with Censored Data The output provides the posterior estimates for the regression coefficients ($\\beta$) and the standard deviation ($\\sigma$) for the log-normal survival model with censored data. The model has converged well, as indicated by the $\\hat{R}$ values and effective sample sizes ($n_{\\text{eff}}$), with no divergences reported during sampling.\n$\\beta$ Coefficients (Covariates):\nbeta[0] (Bilirubin): Mean of 0.91, indicating a positive association between Bilirubin levels and survival time. Higher Bilirubin levels are associated with increased survival times, and the 90% credible interval does not cross zero, suggesting a strong effect. beta[1] (Albumin): Mean of 1.32, showing a significant positive effect of Albumin on survival time. Higher Albumin levels appear to correlate with longer survival times, and the effect is pronounced. beta[2] (Stage): Mean of 0.61, suggesting a positive effect of Stage on survival time. The 90% credible interval remains above zero, implying that individuals at higher stages have slightly increased survival times, though the effect size is moderate. beta[3] and beta[4]: These coefficients represent additional covariates in the model. Both show some positive effects, with beta[3] having a mean of 0.68 and beta[4] showing a very slight negative effect with a mean of -0.02. However, the credible interval for beta[4] crosses zero, indicating less certainty about this effect. beta[5]: A mean of 1.16, suggesting another strong positive association with survival time, with a credible interval that does not cross zero. Standard Deviation ($\\sigma$):\nThe parameter $\\sigma$ has a mean of 5.25, indicating the variability in the log-transformed survival times. This suggests that there is a moderate amount of variance in the log-survival times across individuals, likely due to the influence of covariates. Effective Sample Size ($n_{\\text{eff}}$) and $\\hat{R}$ Values:\nThe effective sample sizes are large, suggesting good mixing of the chains. All $\\hat{R}$ values are equal to 1.00, confirming that the model has converged correctly. The log-normal survival model with censored data has fit the data well, with meaningful estimates for the regression coefficients and the standard deviation. The positive effects of Bilirubin and Albumin are particularly strong, suggesting these covariates play a key role in predicting survival times. Additionally, the inclusion of censored data has improved the model\u0026rsquo;s robustness by accounting for individuals whose survival times are unknown by the end of the study.\nPosterior Predictive Check for the Log-Normal Survival Model with Censored Data The final step involves performing a posterior predictive check (PPC) to evaluate how well the log-normal survival model with censored data fits the observed data. This process involves generating predictions from the model based on the posterior samples and comparing them to the actual observed data.\nCode Explanation Posterior Predictive Samples:\nWe use the Predictive class to sample from the posterior distribution of the model parameters (obtained from the MCMC) and generate new predicted survival times. Inference Data:\nThe generated posterior predictive samples are converted into InferenceData format using ArviZ to facilitate easy analysis and visualization. Trace Plot:\nThe trace plot visualizes the posterior distributions of the model parameters and the convergence of the MCMC chains. Posterior Predictive Check (PPC):\nThe PPC compares the predicted survival times (both censored and uncensored) with the observed survival times. This allows us to assess the model\u0026rsquo;s fit by evaluating how closely the predicted values match the actual data. # posterior predictive check for the Weibull model posterior_samples_lognormal_censored = mcmc_lognormal_censored.get_samples() posterior_predictive_lognormal_censored = Predictive(survival_model_lognormal_with_censored, posterior_samples_lognormal_censored) posterior_predictive_samples_lognormal_censored = posterior_predictive_lognormal_censored(random.PRNGKey(42), covariates=covariates_survival, survival_times=survival_times, event_occurred=event_occurred) # convert to InferenceData and plot PPC az_data_lognormal_censored = az.from_numpyro(mcmc_lognormal_censored, posterior_predictive=posterior_predictive_samples_lognormal_censored) az.plot_trace(trace_survival_with_covariates_lognormal, compact = False) plt.tight_layout() plt.show() az.plot_ppc(az_data_lognormal_censored, group=\u0026#34;posterior\u0026#34;) plt.show() Interpretation of the Results Trace Plot:\nThe trace plot shows well-mixed chains for each parameter (the $\\beta$ coefficients and $\\sigma$). The chains appear to have converged properly, with no visible signs of autocorrelation or poor mixing, further confirmed by the high effective sample sizes and $\\hat{R}$ values from the summary. Posterior Predictive Plot:\nThe PPC plot shows the predicted survival times (for both censored and uncensored data) overlaid with the actual observed data. The uncensored data (right plot) shows that the model does a reasonable job of capturing the distribution of the observed survival times, with the posterior predictive mean aligning well with the data. For the censored data (left plot), the model also appears to fit well, indicating that the log-normal model appropriately captures the uncertainty introduced by censoring. The posterior predictive check indicates that the log-normal model with censored data provides a good fit to the data. The model successfully handles both censored and uncensored survival times, making accurate predictions for the observed survival times. This confirms the model\u0026rsquo;s robustness in dealing with time-to-event data in the presence of censoring.\nThe two plots presented below show the posterior predictive checks (PPC) for both uncensored and censored data from our log-normal survival model with censored observations.\nUncensored Data Plot: The first plot compares the observed uncensored data (green line) with the posterior predictive distribution (blue line).\nObserved Uncensored Data: These are individuals for whom the event (e.g., death) has occurred within the study period, and the exact survival times are known. Posterior Predictive Uncensored: The blue line represents the model\u0026rsquo;s predicted distribution for these uncensored individuals, given the posterior distribution of parameters. In this plot, we observe a reasonable fit between the observed and predicted distributions. The general shape of both lines aligns, with some differences in fluctuation, which could be attributed to noise or model limitations. The model captures the overall distribution of survival times for individuals who experienced the event.\nCensored Data Plot: The second plot compares the observed censored data (green line) with the posterior predictive distribution (blue line).\nObserved Censored Data: This represents individuals whose event did not occur by the end of the study period, so we only know they survived at least until a certain time. The x-axis shows the proportion of time for which the event had not occurred (between 0 and 1). Posterior Predictive Censored: The blue line shows the predicted distribution for censored individuals based on the posterior samples from the model. The fit is close, with both lines following similar trends. The model correctly predicts that most censored individuals have survival probabilities close to either 0 or 1, reflecting the individuals who were not likely to experience the event by the end of the study. This shows that the model is appropriately accounting for the censored data.\nOverall, both PPC plots demonstrate that the model is capable of predicting survival times effectively, accounting for both censored and uncensored data, and matching the general distribution patterns in the dataset.\naz.plot_kde( az_data_lognormal_censored[\u0026#34;observed_data\u0026#34;][\u0026#34;obs_uncensored\u0026#34;] .to_numpy() .flatten(), plot_kwargs={\u0026#34;color\u0026#34;: \u0026#34;C2\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Observed Data Uncensored\u0026#34;}, ) az.plot_kde( az_data_lognormal_censored[\u0026#34;posterior_predictive\u0026#34;][\u0026#34;obs_uncensored\u0026#34;] .to_numpy() .flatten(), plot_kwargs={\u0026#34;color\u0026#34;: \u0026#34;C0\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Posterior Predictive Uncensored\u0026#34;}, ) plt.show() az.plot_kde( az_data_lognormal_censored[\u0026#34;observed_data\u0026#34;][\u0026#34;obs_censored\u0026#34;] .to_numpy() .flatten(), plot_kwargs={\u0026#34;color\u0026#34;: \u0026#34;C2\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Observed Data Censored\u0026#34;}, ) az.plot_kde( az_data_lognormal_censored[\u0026#34;posterior_predictive\u0026#34;][\u0026#34;obs_censored\u0026#34;] .to_numpy() .flatten(), plot_kwargs={\u0026#34;color\u0026#34;: \u0026#34;C0\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Posterior Predictive Censored\u0026#34;}, ) plt.show() 5. Conclusion In this notebook, we conducted a comprehensive analysis using Bayesian survival modeling techniques to predict survival times in a dataset of cirrhosis patients. We began with exploratory data analysis (EDA), examining key variables and correlations to gain an understanding of the dataset. We then moved to classification modeling using a Bayesian logistic regression model to predict survival status, where we explored the relationships between covariates and the binary outcome.\nAfterwards, we shifted our focus to survival analysis. We first implemented a Weibull survival model to model survival times based solely on the observed uncensored data. We explored the shortcomings of this model, particularly when considering the flexibility of the data. Subsequently, we introduced a log-normal survival model, which proved more appropriate for capturing the distribution of survival times.\nTo account for individual-specific factors, we introduced covariates into the survival models, enabling more personalized predictions based on factors such as bilirubin levels, age, and albumin. This allowed us to model survival times as a function of these covariates, further improving the predictive power of the model.\nFinally, we tackled the challenge of censored data, which occurs when the exact event time is unknown for some individuals. We adjusted the log-normal model to account for both censored and uncensored observations, demonstrating that Bayesian modeling can handle these complexities effectively. We used posterior predictive checks (PPC) to validate the models and showed that the predicted survival times align well with the observed data, both for censored and uncensored cases.\nThrough this journey, we demonstrated how Bayesian inference and probabilistic programming provide powerful tools for survival analysis. By leveraging MCMC sampling, we were able to estimate the posterior distributions of key parameters and perform robust predictions, while accounting for the uncertainty inherent in the data. Overall, we illustrated the flexibility and capability of Bayesian methods for complex survival analysis tasks.\n","permalink":"http://localhost:1313/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/","summary":"Applying Bayesian Classification and Survival Analysis to the Cirrhosis Patient Dataset.","title":"Bayesian Classification and Survival Analysis with NumPyro"},{"content":"\nProblem Statement In this notebook, we will explore the relationship between height and weight using Bayesian linear regression. Our goal is to fit a linear model of the form:\n$$ y = \\alpha + \\beta x + \\varepsilon $$\nwhere:\n$y$ represents the weight, $x$ represents the height, $\\alpha$ is the intercept, $\\beta$ is the slope, $\\varepsilon$ is the error term, modeled as Gaussian white noise, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma)$, where $\\sigma$ is the standard deviation of the noise. We will use Bayesian inference to estimate the posterior distributions of $\\alpha$ and $\\beta$ given our data and prior assumptions. Bayesian methods provide a natural way to quantify uncertainty in our parameter estimates and predictions.\nApproach To achieve our goal, we will:\nLoad Real Data: We will use an actual dataset representing the heights and weights of individuals, sourced from Kaggle. Define the Bayesian Model: Using the probabilistic programming package PyMC, we will define our Bayesian linear regression model, specifying our priors for $\\alpha$, $\\beta$, and $\\sigma$. Perform Inference: We will use Markov Chain Monte Carlo (MCMC) algorithms, such as the No-U-Turn Sampler (NUTS), to sample from the posterior distributions of our model parameters. Visualization and Prediction: We will visualize the results, including the regression lines sampled from the posterior, the uncertainty intervals, and make predictions on new, unobserved data points. Reference This notebook is inspired by examples from the PyMC documentation, specifically the Generalized Linear Regression tutorial. It also builds upon a similar implementation in Julia using Turing.jl. This PyMC recreation aims at providing a more complete illustration of the use of probabilistic programming languages.\nInitial setup Import the necessary packages.\nAdditionally, this notebook is supposed to be used in Google Colab. The data set (CSV) file is hosted in a private github repo. Therefore, include the github cloning to the temporary session so that the data can be accessed and used in the Colab session.\nimport os import arviz as az import pymc as pm import pandas as pd import xarray as xr import numpy as np import matplotlib.pyplot as plt %matplotlib inline plt.rcParams[\u0026#39;text.usetex\u0026#39;] = True plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;STIXGeneral\u0026#39; Bayesian Workflow For this exercise, I will implement the following workflow:\nCollect data: this will be implemented by downloading the relevant data set Build a Bayesian model: this will be built using PyMC Infer the posterior distributions of the parameters $\\alpha$ and $\\beta$, as well as the model noise Evaluate the fit of the model Collecting the data The data to be analyzed will be the height vs. weight data from https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset\n# load the data and print the header csv_path = \u0026#39;data/SOCR-HeightWeight.csv\u0026#39; data = pd.read_csv(csv_path) data.head() Index\rHeight(Inches)\rWeight(Pounds)\r0\r1\r65.78331\r112.9925\r1\r2\r71.51521\r136.4873\r2\r3\r69.39874\r153.0269\r3\r4\r68.21660\r142.3354\r4\r5\r67.78781\r144.2971\rLet\u0026rsquo;s instead work with the International System.\nConvert the values to centimeters and kilograms.\n# Renaming columns 2 and 3 new_column_names = {data.columns[1]: \u0026#39;Height (cm)\u0026#39;, data.columns[2]: \u0026#39;Weight (kg)\u0026#39;} data.rename(columns = new_column_names, inplace = True) # convert the values to SI units data[data.columns[1]] = data[data.columns[1]]*2.54 data[data.columns[2]] = data[data.columns[2]]*0.454 # assign the relevant data to variables for easier manipulation height = data[\u0026#39;Height (cm)\u0026#39;][:1000] weight = data[\u0026#39;Weight (kg)\u0026#39;][:1000] data.head() Index\rHeight (cm)\rWeight (kg)\r0\r1\r167.089607\r51.298595\r1\r2\r181.648633\r61.965234\r2\r3\r176.272800\r69.474213\r3\r4\r173.270164\r64.620272\r4\r5\r172.181037\r65.510883\rVisualize the data # scatter plot of the data plt.scatter(height, weight, s = 20, edgecolor = \u0026#39;black\u0026#39;, alpha = 0.5) plt.title(\u0026#39;Height vs. Weight\u0026#39;) plt.xlabel(\u0026#39;Height (cm)\u0026#39;) plt.ylabel(\u0026#39;Weight (kg)\u0026#39;) plt.grid(True, linestyle=\u0026#39;--\u0026#39;, linewidth=0.5, color=\u0026#39;gray\u0026#39;, alpha=0.5) # plt.show() Building a Bayesian model with PyMC First, we assume that the weight is a variable dependent on the height. Thus, we can express the Bayesian model as:\n$$y \\sim \\mathcal{N}(\\alpha + \\beta \\mathbf{X}, \\sigma^2)$$\nSince we want to infer the posterior distribution of the parameters $\\theta = {\\alpha, \\beta, \\sigma }$, we need to assign priors to those variables. Remember that $\\sigma$ is a measure of the uncertainty in the model.\n$$ \\begin{align*} \\alpha \u0026amp;\\sim \\mathcal{N}(0,10) \\\\ \\beta \u0026amp;\\sim \\mathcal{N}(0,1) \\\\ \\sigma \u0026amp;\\sim \\mathcal{TN}(0,100; 0, \\infty) \\end{align*} $$ The last distribution is a truncated normal distribution bounded from 0 to $\\infty$.\nNote: Here, we define the input data height as a MutableData container. The reason for this is because, later, we will want to change this input data, to make predictions. This will become clear a bit later.\nwith pm.Model() as blr_model: x = pm.MutableData(\u0026#39;height\u0026#39;, height) # define the priors alpha = pm.Normal(\u0026#39;alpha\u0026#39;, 0, 10) beta = pm.Normal(\u0026#39;beta\u0026#39;, 0, 10) sigma = pm.TruncatedNormal(\u0026#39;sigma\u0026#39;, mu = 0, sigma = 100, lower = 0) # define the likelihood - assign the variable name \u0026#34;y\u0026#34; to the observations y = pm.Normal(\u0026#39;y\u0026#39;, mu = alpha + (beta * x), sigma = sigma, observed = weight, shape = x.shape) # inference - crank up the bayes! trace = pm.sample(1000, chains = 4) Auto-assigning NUTS sampler...\rInitializing NUTS using jitter+adapt_diag...\rMultiprocess sampling (4 chains in 4 jobs)\rNUTS: [alpha, beta, sigma]\r100.00% [8000/8000 00:37\u0026lt;00:00 Sampling 4 chains, 0 divergences]\rSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 53 seconds.\rWe can explore the trace object.\ntrace.to_dataframe().columns Index([ 'chain',\r'draw',\r('posterior', 'alpha'),\r('posterior', 'beta'),\r('posterior', 'sigma'),\r('sample_stats', 'perf_counter_diff'),\r('sample_stats', 'perf_counter_start'),\r('sample_stats', 'smallest_eigval'),\r('sample_stats', 'step_size_bar'),\r('sample_stats', 'index_in_trajectory'),\r('sample_stats', 'energy'),\r('sample_stats', 'max_energy_error'),\r('sample_stats', 'energy_error'),\r('sample_stats', 'acceptance_rate'),\r('sample_stats', 'tree_depth'),\r('sample_stats', 'process_time_diff'),\r('sample_stats', 'step_size'),\r('sample_stats', 'n_steps'),\r('sample_stats', 'largest_eigval'),\r('sample_stats', 'diverging'),\r('sample_stats', 'lp'),\r('sample_stats', 'reached_max_treedepth')],\rdtype='object')\rVisualize the inference diagnostics Now that we have performed Bayesian inference using the NUTS() algorithm, we can visualize the results. Additionally, call for a summary of the statistics of the inferred posterior distributions of $\\theta$.\n# visualize the results # az.style.use(\u0026#39;arviz-darkgrid\u0026#39;) labeller = az.labels.MapLabeller(var_name_map = {\u0026#39;alpha\u0026#39;: r\u0026#39;$\\alpha$\u0026#39;, \u0026#39;beta\u0026#39;: r\u0026#39;$\\beta$\u0026#39;, \u0026#39;sigma\u0026#39;: r\u0026#39;$\\sigma$\u0026#39;}) az.plot_trace(trace, var_names = [\u0026#39;alpha\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;], labeller = labeller, compact = False) plt.tight_layout() # plt.show() Interpreting the MCMC Diagnostics Plots Trace plots are crucial for diagnosing the performance of Markov Chain Monte Carlo (MCMC) algorithms. These plots typically consist of two parts for each parameter: the trace plot and the posterior density plot.\nThe trace plot shows the sampled values of a parameter across iterations. A well-behaved trace plot should look like a \u0026ldquo;hairy caterpillar,\u0026rdquo; indicating good mixing. This means the trace should move around the parameter space without getting stuck and should not display any apparent patterns or trends. If the trace shows a clear trend or drift, it suggests that the chain has not yet converged. For the parameters $\\alpha$ (intercept), $\\beta$ (slope), and $\\sigma$ (standard deviation of noise), we want to see the traces for different chains mixing well and stabilizing around a constant mean.\nThe posterior density plot shows the distribution of the sampled values of a parameter. This plot helps visualize the posterior distribution of the parameter. A good density plot should be smooth and unimodal, indicating that the parameter has a well-defined posterior distribution. If multiple chains are used, their density plots should overlap significantly, suggesting that all chains are sampling from the same distribution. For $\\alpha$, $\\beta$, and $\\sigma$, overlapping density plots indicate that the chains have converged to the same posterior distribution.\nNext, we can visualize the posterior distributions of the inferred parameters.eters.\n# visualize the posterior distributions az.plot_posterior(trace, var_names = [\u0026#39;alpha\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;], labeller = labeller) plt.show() After visualizing the inference diagnostics and the posterior distributions of the paramters, we can also obtain the summary statistics.\n# get the summary statistics of the posterior distributions pm.summary(trace, kind = \u0026#34;stats\u0026#34;) mean\rsd\rhdi_3%\rhdi_97%\ralpha\r-28.557\r4.558\r-36.650\r-19.619\rbeta\r0.500\r0.026\r0.449\r0.548\rsigma\r4.657\r0.100\r4.474\r4.850\rVisualize the results Now that we have posterior distributions for the parameters $\\theta$, we can plot the the resulting linear regression functions. The following is an excerpt from PyMC\u0026rsquo;s Generalized Linear Regression tutorial:\nIn GLMs, we do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. We can manually generate these regression lines using the posterior samples directly.\nBelow, what we will effectively be doing is:\n$$ y_i = \\alpha_i + \\beta_i \\mathbf{X} \\ \\ \\ , \\ \\ \\ {i = 1, \\ldots , N_{samples}}$$\nwhere $N_{samples}$ are the number of samples from the posterior. This number comes from the inference procedure, and in practical terms is the umber of samples we asked PyMC to produce.\nIn other words, plotting the samples from the posterior distribution involves plotting the regression lines sampled from the posterior. Each sample represents a possible realization of the regression line based on the sampled values of the parameters $\\alpha$ (intercept) and $\\beta$ (slope).\nThese sample regression lines ullustrate the uncertainty in the regression model\u0026rsquo;s parameters and how this uncertainty propagates into the predictions (of the regression line).\n# use the posterior to create regression line samples # equivalent to: y[i] = alpha[i] + beta[i]*X trace.posterior[\u0026#34;y_posterior\u0026#34;] = trace.posterior[\u0026#34;alpha\u0026#34;] + trace.posterior[\u0026#34;beta\u0026#34;]*xr.DataArray(height) # plot the regression lines _, ax = plt.subplots(figsize=(7,7)) az.plot_lm(idata = trace, y = weight, x = height, axes=ax, y_model=\u0026#34;y_posterior\u0026#34;, y_kwargs={\u0026#34;color\u0026#34;:\u0026#34;b\u0026#34;, \u0026#34;alpha\u0026#34;:0.2, \u0026#34;markeredgecolor\u0026#34;:\u0026#34;k\u0026#34;, \u0026#34;label\u0026#34;:\u0026#34;Observed Data\u0026#34;, \u0026#34;markersize\u0026#34;:10}, y_model_plot_kwargs={\u0026#34;alpha\u0026#34;: 0.2, \u0026#34;zorder\u0026#34;: 10, \u0026#34;color\u0026#34;:\u0026#34;#00cc99\u0026#34;}, y_model_mean_kwargs={\u0026#34;color\u0026#34;:\u0026#34;red\u0026#34;} ) plt.show() Using the Linear Regression Model to Make Predictions Now that we have a fitted Bayesian linear regression model, we can use it to make predictions. This involves sampling from the posterior predictive distribution, which allows us to generate predictions for new data points while incorporating the uncertainty from the posterior distribution of the parameters.\nSample from the Posterior Predictive Distribution: This step involves using the inferred trace from our Bayesian linear regression model blr_model to generate predictions. The pm.sample_posterior_predictive function in PyMC allows us to do this. It uses the posterior samples of the parameters to compute the predicted values of the outcome variable. # now predict the outcomes using the inferred trace with blr_model: # use the updated values and predict outcomes and probabilities: pm.sample_posterior_predictive( trace, var_names = [\u0026#39;y\u0026#39;], return_inferencedata=True, extend_inferencedata=True, ) Sampling: [y]\r100.00% [4000/4000 00:00\u0026lt;00:00]\rExploring the Trace Object The trace object stores the results of our inference. Initially, it contained the posterior samples of the model parameters (e.g., intercept and slope).\nAfter running pm.sample_posterior_predictive, the trace object is extended to include the posterior predictive samples. These are the predicted values for the outcome variable, given the posterior distribution of the model parameters.\n# explore the trace object again trace.to_dataframe().columns Index([ 'chain',\r'draw',\r('posterior', 'alpha'),\r('posterior', 'beta'),\r('posterior', 'sigma'),\r('posterior', 'y_posterior[0]', 0),\r('posterior', 'y_posterior[100]', 100),\r('posterior', 'y_posterior[101]', 101),\r('posterior', 'y_posterior[102]', 102),\r('posterior', 'y_posterior[103]', 103),\r...\r('sample_stats', 'energy_error'),\r('sample_stats', 'acceptance_rate'),\r('sample_stats', 'tree_depth'),\r('sample_stats', 'process_time_diff'),\r('sample_stats', 'step_size'),\r('sample_stats', 'n_steps'),\r('sample_stats', 'largest_eigval'),\r('sample_stats', 'diverging'),\r('sample_stats', 'lp'),\r('sample_stats', 'reached_max_treedepth')],\rdtype='object', length=2022)\rWe can observe how now we have another inference data container: posterior_predictive. This was generated by passing the extend_inferencedata argument to the pm.sample_posterior_predictive function above.\nThis data contains predictions by passing the observed heights through our linear model and making predictions. Note that these \u0026ldquo;predictions\u0026rdquo; are made on observed data. This is similar to using validating the predictions on training data in machine learning, i.e. comparing the model predictions to the actual data on an observed input.\nWe can use the linear regression model to make predictions. It should be noted that, again, the linear regression model is not a single regression line, but rather a set of regression lines generated from the posterior probability of $\\theta$.\nVisualize the Prediction Confidence Interval After we sampled from the posterior, we might want to visualize this to understand the posterior predictive distribution.\nIn the code below, there are two things going on, let\u0026rsquo;s go through them.\nPlotting the samples from the posterior distribution This part is exactly what we did before, which is plotting the sample posteriors of the regression line. These sample regression lines are a natural product of propagating the uncertainty from the parameters unto the prediction line.\nPlotting the uncertainty in the mean and the observations Now we can add a ribbon to show the uncertainty not only in the regression line, but in the prediction points themselves. That is, that ribbon will tell us where we might expect a prediction point $i+1$, i.e.\n$$ y_{i+1} = \\alpha_{i+1} + \\beta_{i+1} x^* $$\nwhere $x^*$ is a test input point. In other words, and more specific to this demonstration:\nwhat is the interval where we would expect a predicted weight $y_{i+1}$ of an individual with a height $x*$.\n# use the posterior to create regression line samples # trace.posterior[\u0026#34;y_posterior\u0026#34;] = trace.posterior[\u0026#34;alpha\u0026#34;] + trace.posterior[\u0026#34;beta\u0026#34;]*xr.DataArray(height) # y_posterior = alpha + beta*x _, ax = plt.subplots(figsize=(7,7)) az.plot_lm(idata = trace, y = weight, x = height, axes=ax, y_model=\u0026#34;y_posterior\u0026#34;, y_kwargs={\u0026#34;color\u0026#34;:\u0026#34;b\u0026#34;, \u0026#34;alpha\u0026#34;:0.2, \u0026#34;markeredgecolor\u0026#34;:\u0026#34;k\u0026#34;, \u0026#34;label\u0026#34;:\u0026#34;Observed Data\u0026#34;, \u0026#34;markersize\u0026#34;:10}, y_model_plot_kwargs={\u0026#34;alpha\u0026#34;: 0.2, \u0026#34;zorder\u0026#34;: 10, \u0026#34;color\u0026#34;:\u0026#34;#00cc99\u0026#34;}, y_model_mean_kwargs={\u0026#34;color\u0026#34;:\u0026#34;red\u0026#34;} ); # plot the prediction interval az.plot_hdi( height, trace.posterior_predictive[\u0026#34;y\u0026#34;], hdi_prob=0.6, fill_kwargs={\u0026#34;alpha\u0026#34;: 0.8}, ) plt.show() Making Predictions on Unobserved Data Inputs Now, how about the case when we want to make predictions on test data that we have not seen? That is, predict the weight of an individual whose height/weight we have not observed (measured)\nIn other words, we have some test input data, i.e. some heights for which we want to predict the weights.\nSome references of where I learned how to do this:\nIn this example and this other example it says that we can generate out-of-sample predictions by using pm.sample_posterior_predictive and it shows an example of how to use the syntax.\nMore recently, this demo blog post clarifies how to make predictions on out-of-model samples.\nLet\u0026rsquo;s do just that now. First, we will define the test inputs we want to predict for, pred_height. Then, inside the model, we replace the data (which was defined as MutableData, with the new data we want to make predictions on. This is done as follows:\n# set new data inputs: pred_height = np.array([ \u0026#39;new_data\u0026#39; ]) with blr_model: pm.set_data({\u0026#39;height\u0026#39;: pred_height}) What this is effectively doing is telling sample_posterior_predictive that we need to make predictions on height which now happens to be different.\n# define the out-of-sample predictors pred_height = [158.0, 185.5, 165.2, 178.0, 180.0, 170.2] print(pred_height) with blr_model: # set the new data we want to make predictions for pm.set_data({\u0026#39;height\u0026#39;: pred_height}) post_pred = pm.sample_posterior_predictive( trace, predictions = True ) Sampling: [y]\r[158.0, 185.5, 165.2, 178.0, 180.0, 170.2]\r100.00% [4000/4000 00:00\u0026lt;00:00]\rWhat we have done above is create an inference data object called post_pred. This object contains the samples of the predictions on the new data. Specifically, it includes two containers: predictions and predictions_constant_data.\nThe predictions container holds the predicted samples for our new heights. The predictions_constant_data holds the new heights we passed into the model.\npost_pred.to_dataframe() chain\rdraw\r(y[0], 0)\r(y[1], 1)\r(y[2], 2)\r(y[3], 3)\r(y[4], 4)\r(y[5], 5)\r0\r0\r0\r48.981930\r62.971186\r62.143385\r59.300742\r56.100237\r54.329348\r1\r0\r1\r55.481192\r65.132876\r54.761877\r61.312254\r59.220124\r51.817360\r2\r0\r2\r49.471550\r66.016910\r60.646273\r57.876344\r56.203720\r60.318281\r3\r0\r3\r53.373737\r66.593653\r53.085799\r63.437949\r64.336626\r45.372830\r4\r0\r4\r52.981309\r69.320059\r51.590686\r60.372046\r62.210738\r48.188656\r...\r...\r...\r...\r...\r...\r...\r...\r...\r3995\r3\r995\r52.303814\r61.931117\r47.544216\r60.824401\r61.469545\r62.353284\r3996\r3\r996\r56.032295\r56.979040\r54.584837\r55.894216\r65.943908\r50.929285\r3997\r3\r997\r56.062352\r50.889499\r51.441003\r57.841533\r62.898654\r52.749139\r3998\r3\r998\r48.228772\r65.983383\r52.381164\r55.283946\r65.468049\r70.367514\r3999\r3\r999\r58.434184\r54.739363\r56.773260\r53.128112\r61.695469\r54.874142\r4000 rows × 8 columns\nWe can visualize the posterior distributions of the predictions.\naz.plot_posterior(post_pred, group=\u0026#34;predictions\u0026#34;); We can obtain point estimates by taking the mean of each prediction distribution. This is done by taking the mean of the predictions over the chain and draw dimensions, as follows:\npred_weight = post_pred.predictions[\u0026#39;y\u0026#39;].mean(dim = [\u0026#39;chain\u0026#39;, \u0026#39;draw\u0026#39;]) print(\u0026#34;Predicted weights: \u0026#34;, pred_weight.values) Predicted weights: [50.37415152 64.29241929 54.02070975 60.60276731 61.36759368 56.53983895]\rFinally, we can visualize where the predictions fall by adding a scatter plot with the new ${x^, y^}$ data.\n# use the posterior to create regression line samples # trace.posterior[\u0026#34;y_posterior\u0026#34;] = trace.posterior[\u0026#34;alpha\u0026#34;] + trace.posterior[\u0026#34;beta\u0026#34;]*xr.DataArray(height) # y_posterior = alpha + beta*x _, ax = plt.subplots(figsize=(7,7)) az.plot_lm(idata = trace, y = weight, x = height, axes=ax, y_model=\u0026#34;y_posterior\u0026#34;, y_kwargs={\u0026#34;color\u0026#34;:\u0026#34;b\u0026#34;, \u0026#34;alpha\u0026#34;:0.2, \u0026#34;markeredgecolor\u0026#34;:\u0026#34;k\u0026#34;, \u0026#34;label\u0026#34;:\u0026#34;Observed Data\u0026#34;, \u0026#34;markersize\u0026#34;:10}, y_model_plot_kwargs={\u0026#34;alpha\u0026#34;: 0.2, \u0026#34;zorder\u0026#34;: 10, \u0026#34;color\u0026#34;:\u0026#34;#00cc99\u0026#34;}, y_model_mean_kwargs={\u0026#34;color\u0026#34;:\u0026#34;red\u0026#34;} ); # plot the prediction interval az.plot_hdi( height, trace.posterior_predictive[\u0026#34;y\u0026#34;], hdi_prob=0.6, fill_kwargs={\u0026#34;alpha\u0026#34;: 0.8}, ) # add predicted weights to the plot ax.scatter(pred_height, pred_weight.values, color = \u0026#39;blue\u0026#39;, label = \u0026#39;Predicted Weights\u0026#39;, zorder = 15 ) ax.legend() plt.show() Thank you! This demo focused on a relatively simple task. Here, however, we focused more on what a Bayesian approach means in the context of a linear regression. Additionally, we focused on using PyMC for developing the model, visualizing the results and, just as importantly, on making predictions using those results.\nVictor\n","permalink":"http://localhost:1313/posts/20240527_baylinreg_pymc/e01_baylinreg_pymc/","summary":"Learn the basics of Bayesian linear regression using the excellent PyMC Probabilistic Programming package. This focuses on model formulation in PyMC, interpretation, and how to make predictions on out-of-sample data.","title":"Bayesian Linear Regression with PyMC"},{"content":" Introduction This guide demonstrates how to apply transfer learning using a pre-trained vision model to classify cat moods based on their facila expressions. We\u0026rsquo;ll learn how to handle custom data setups.\nIn this demonstration, we recreate the exercise done in PyTorch, available here. Since that demonstration is quite detailed, we keep it pretty straightforward here.\nMotivation \u0026amp; Credit When I thought about learning how to implement a computer vision classification model for transfer learning in Julia and Flux, I immediately came upon two roadblocks:\nSince I am not an expert in Julia, I found the documentation to be a bit difficult to access (again, this is just me!). There are not many tutorials or resources to illustrate this particular case. Therefore I took it upon myself to put things together and make a demonstration that would hopefully be useful for someone who might not be an expert in Flux (or Julia).\nThis particular demo was inspired by a combination of the following resources:\nTransfer Learning and Twin Network for Image Classification using Flux.jl Flux.jl\u0026rsquo;s Model Zoo Tutorial PyTorch Transfer Learning for Computer Vision Tutorial Getting Started We will use a pre-trained ResNet18 model, initially trained on a general dataset, and fine-tune it for our specific task of classifying cat moods.\nInitialization First, we activate the current directory as our project environment by calling the package manager Pkg:\nusing Pkg Pkg.activate(\u0026#34;.\u0026#34;) Then we will import the required packages. Of course, this is also assuming that one has already added the relevant packages into the environment.\nusing Pkg Pkg.activate(\u0026#34;.\u0026#34;) using Random: shuffle! import Base: length, getindex using Images using Flux using Flux: update! using DataAugmentation using Metalhead using MLUtils using DataFrames, CSV using Plots \u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m project at `H:\\My Drive\\Projects\\Coding\\Portfolio\\Machine Learning\\Julia\\Transfer Learning with Flux`\rRetrieve the Data and Initial Setup First, we specify the paths to the dataset and labels CSV files for training, validation, and test sets. Then, we load these CSV files into DataFrames. Finally, we create vectors of absolute file paths for each image in the dataset.\nThis setup is essential for organizing the data and ensuring that our model can access the correct images and labels during training and evaluation.\nLabel Structure The data set we are using consists of three folders: train, val, test. Each of them contain a set of images of cats. The labels in this case, are in the form of a CSV file that maps the filename with a one-hot encoding to label the classification of the image, i.e. the cat\u0026rsquo;s mood - alarmed, angry, calm, pleased.\nThe dataset was obtained here.\n# specify the paths to the dataset and labels CSV train_data_path = \u0026#34;data/cat_expression_data/train\u0026#34; train_data_csv = \u0026#34;data/cat_expression_data/train/_classes.csv\u0026#34; val_data_path = \u0026#34;data/cat_expression_data/val\u0026#34; val_data_csv = \u0026#34;data/cat_expression_data/val/_classes.csv\u0026#34; test_data_path = \u0026#34;data/cat_expression_data/test\u0026#34; test_data_csv = \u0026#34;data/cat_expression_data/test/_classes.csv\u0026#34; # load the CSV file containing the labels train_labels_df = CSV.read(train_data_csv, DataFrame) test_labels_df = CSV.read(test_data_csv, DataFrame) val_labels_df = CSV.read(val_data_csv, DataFrame) # setup filepaths to the files as vectors train_filepaths = [abspath(joinpath(train_data_path, filename)) for filename in train_labels_df[!, 1] ] test_filepaths = [abspath(joinpath(test_data_path, filename)) for filename in test_labels_df[!, 1] ] val_filepaths = [abspath(joinpath(val_data_path, filename)) for filename in val_labels_df[!, 1] ] 110-element Vector{String}:\r⋮\rData Exploration As usual, we take a look at the data to understand what we are working with.\nBelow we make a couple of functions to visualize the data.\nNote that the helper function label_from_row will come in handy later on.\n# -----------------------------------------------------------------------# # helper function to extract label from the DataFrame function label_from_row(filename, labels_df, label_dict) # retrieve the label for the image from the DataFrame label_row = filter(row -\u0026gt; row.filename == filename, labels_df) label_index = findfirst(x -\u0026gt; label_row[1, x] == 1, names(labels_df)[2:end]) return label_dict[label_index] end # -----------------------------------------------------------------------# # function to display a selection of images and their labels function show_sample_images_and_labels(labels_df, label_dict; num_samples = 4) # randomly pick indices for sampling images sample_indices = rand(1:nrow(labels_df), num_samples) sample_filenames = labels_df.filename[sample_indices] # calculate number of rows and columns for the grid layuot num_cols = ceil(Int, num_samples / 2) num_rows = 2 # prepare a plot with a grid layout for the images p = plot(layout = (num_rows, num_cols), size(800, 200), legend = false, axis = false, grid = false) # load and plot each sampled image for (index, filename) in enumerate(sample_filenames) img_path = joinpath(train_data_path, filename) img = load(img_path) # load the image from the file # retrieve the label for the image from the DataFrame label = label_from_row(filename, labels_df, label_dict) plot!(p[index], img, title = label, axis = false) end display(p) # display the plot end # define a dictionary for label descriptions: label_dict = Dict(1 =\u0026gt; \u0026#34;alarmed\u0026#34;, 2 =\u0026gt; \u0026#34;angry\u0026#34;, 3 =\u0026gt; \u0026#34;calm\u0026#34;, 4 =\u0026gt; \u0026#34;pleased\u0026#34;) # run the function to show images show_sample_images_and_labels(train_labels_df, label_dict) Working with Custom Datasets When working with custom datasets in Julia, the concepts are similar as in PyTorch, but obviously following Julia\u0026rsquo;s syntax.\nIn essence, we read the CSV files containing image file paths and their corresponding labels into DataFrames. We then create functions to handle data loading and transformations, such as resizing and normalizing images. This approach is similar to PyTorch\u0026rsquo;s Dataset.\nLet\u0026rsquo;s have a quick look.\nCreate a Custom Dataset We define a custom dataset using a struct, which is similar to using a class in Python. The ImageContainer struct stores the image file paths and their corresponding labels in a DataFrame. We then create instances of this struct for the training, validation, and test datasets.\nstruct ImageContainer{T\u0026lt;:Vector} img::T labels_df::DataFrame end # generate dataset train_dataset = ImageContainer(train_filepaths, train_labels_df); val_dataset = ImageContainer(val_filepaths, val_labels_df); test_dataset = ImageContainer(test_filepaths, test_labels_df); Create the Data Loaders In this section, we set up data loaders for our custom dataset in Julia, similar to how data loaders are used in PyTorch to manage batching and shuffling of data.\nCall helper Function: label_from_row() : This function extracts the label from the DataFrame for a given image file. It finds the index of the column with a value of 1, indicating the class.\nLength and Indexing:\nlength(data::ImageContainer): Defines the length method to return the number of images in the dataset. Similar to PyTorch\u0026rsquo;s __len__. getindex(data::ImageContainer, idx::Int): This method is similar to PyTorch’s __getitem__. It loads an image, applies transformations, and returns the processed image along with its label. Data Augmentation and Transformations: pipeline: Defines a transformation pipeline for scaling and cropping images. transforms(image, labels_df): Inside getindex, this function applies the transformations to the image and normalizes it using the predefined mean and standard deviation values. DataLoaders: train_loader and val_loader: These DataLoader objects manage batching, shuffling, and parallel processing of the training and validation datasets, similar to torch.utils.data.DataLoader in PyTorch Notes on Implementing Custom Data Containers According to the documentation for MLUtils.DataLoader (see here), custom data containers should implement Base.length instead of numobs, and Base.getindex instead of getobs, unless there\u0026rsquo;s a difference between these functions and the base methods for multi-dimensional arrays.\nBase.length: Should be implemented to return the number of observations. This is akin to PyTorch\u0026rsquo;s __len__. Base.getindex: Should be implemented to handle indexing of the dataset, similar to PyTorch\u0026rsquo;s __getitem__. These methods ensure that the data is returned in a form suitable for the learning algorithm, maintaining consistency whether the index is a scalar or vector.\nlength(data::ImageContainer) = length(data.img) const im_size = (224, 224) const DATA_MEAN = [0.485f0, 0.456f0, 0.406f0] const DATA_STD = [0.229f0, 0.224f0, 0.225f0] # define a transformation pipeline pipeline = DataAugmentation.compose(ScaleKeepAspect(im_size), CenterCrop(im_size)) function getindex(data::ImageContainer, idx::Int) image = data.img[idx] labels_df = data.labels_df function transforms(image, labels_df) pipeline = ScaleKeepAspect(im_size) |\u0026gt; CenterCrop(im_size) _img = Images.load(image) _img = apply(pipeline, Image(_img)) |\u0026gt; itemdata img = collect(channelview(float32.(RGB.(_img)))) img = permutedims((img .- DATA_MEAN) ./ DATA_STD, (3, 2, 1) ) label = label_from_row(labels_df[idx, 1] , labels_df) return img, label end return transforms(image, labels_df) end train_loader = DataLoader( train_dataset; batchsize = 16, collate = true, parallel = true, ) val_loader = DataLoader( val_dataset; batchsize = 16, collate = true, parallel = true, ); Model Definition Here we will load the model with Metalhead.jl and change the classifier \u0026ldquo;head\u0026rdquo; of the architecture to suit our classification need.\nWe will use this to select the classifier head of the model and change it.\nFor the fine-tuning portion of this exercise will follow the model zoo documentation:\nLet\u0026rsquo;s try it out with the ResNet18 model.\n# load the pre-trained model resnet_model = ResNet(18; pretrain = true).layers # let\u0026#39;s look at the model resnet_model Chain(\rChain(\rChain(\rConv((7, 7), 3 =\u0026gt; 64, pad=3, stride=2, bias=false), \u001b[90m# 9_408 parameters\u001b[39m\rBatchNorm(64, relu), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\rMaxPool((3, 3), pad=1, stride=2),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\rNNlib.relu,\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\rNNlib.relu,\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\r),\r),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\rChain(\rConv((1, 1), 64 =\u0026gt; 128, stride=2, bias=false), \u001b[90m# 8_192 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\r),\rChain(\rConv((3, 3), 64 =\u0026gt; 128, pad=1, stride=2, bias=false), \u001b[90m# 73_728 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\rNNlib.relu,\rConv((3, 3), 128 =\u0026gt; 128, pad=1, bias=false), \u001b[90m# 147_456 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 128 =\u0026gt; 128, pad=1, bias=false), \u001b[90m# 147_456 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\rNNlib.relu,\rConv((3, 3), 128 =\u0026gt; 128, pad=1, bias=false), \u001b[90m# 147_456 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\r),\r),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\rChain(\rConv((1, 1), 128 =\u0026gt; 256, stride=2, bias=false), \u001b[90m# 32_768 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\r),\rChain(\rConv((3, 3), 128 =\u0026gt; 256, pad=1, stride=2, bias=false), \u001b[90m# 294_912 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\rNNlib.relu,\rConv((3, 3), 256 =\u0026gt; 256, pad=1, bias=false), \u001b[90m# 589_824 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 256 =\u0026gt; 256, pad=1, bias=false), \u001b[90m# 589_824 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\rNNlib.relu,\rConv((3, 3), 256 =\u0026gt; 256, pad=1, bias=false), \u001b[90m# 589_824 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\r),\r),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\rChain(\rConv((1, 1), 256 =\u0026gt; 512, stride=2, bias=false), \u001b[90m# 131_072 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\r),\rChain(\rConv((3, 3), 256 =\u0026gt; 512, pad=1, stride=2, bias=false), \u001b[90m# 1_179_648 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\rNNlib.relu,\rConv((3, 3), 512 =\u0026gt; 512, pad=1, bias=false), \u001b[90m# 2_359_296 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 512 =\u0026gt; 512, pad=1, bias=false), \u001b[90m# 2_359_296 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\rNNlib.relu,\rConv((3, 3), 512 =\u0026gt; 512, pad=1, bias=false), \u001b[90m# 2_359_296 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\r),\r),\r),\r),\rChain(\rAdaptiveMeanPool((1, 1)),\rMLUtils.flatten,\rDense(512 =\u0026gt; 1000), \u001b[90m# 513_000 parameters\u001b[39m\r),\r) \u001b[90m # Total: 62 trainable arrays, \u001b[39m11_689_512 parameters,\r\u001b[90m # plus 40 non-trainable, 9_600 parameters, summarysize \u001b[39m44.654 MiB.\rNow we modify the head, by chaning the last Chain in the model. We change the last layer to output 4 classes (as opposed to the original 1000 classes).\n# modify the model resnet_infer = deepcopy(resnet_model[1]) resnet_tune = Chain(AdaptiveMeanPool((1, 1)), Flux.flatten, Dense(512 =\u0026gt; 4)) Chain(\rAdaptiveMeanPool((1, 1)),\rFlux.flatten,\rDense(512 =\u0026gt; 4), \u001b[90m# 2_052 parameters\u001b[39m\r) And that\u0026rsquo;s it! Now, let\u0026rsquo;s just explore both portions of the model.\nresnet_infer Chain(\rChain(\rConv((7, 7), 3 =\u0026gt; 64, pad=3, stride=2, bias=false), \u001b[90m# 9_408 parameters\u001b[39m\rBatchNorm(64, relu), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\rMaxPool((3, 3), pad=1, stride=2),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\rNNlib.relu,\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\rNNlib.relu,\rConv((3, 3), 64 =\u0026gt; 64, pad=1, bias=false), \u001b[90m# 36_864 parameters\u001b[39m\rBatchNorm(64), \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\r),\r),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\rChain(\rConv((1, 1), 64 =\u0026gt; 128, stride=2, bias=false), \u001b[90m# 8_192 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\r),\rChain(\rConv((3, 3), 64 =\u0026gt; 128, pad=1, stride=2, bias=false), \u001b[90m# 73_728 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\rNNlib.relu,\rConv((3, 3), 128 =\u0026gt; 128, pad=1, bias=false), \u001b[90m# 147_456 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 128 =\u0026gt; 128, pad=1, bias=false), \u001b[90m# 147_456 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\rNNlib.relu,\rConv((3, 3), 128 =\u0026gt; 128, pad=1, bias=false), \u001b[90m# 147_456 parameters\u001b[39m\rBatchNorm(128), \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\r),\r),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\rChain(\rConv((1, 1), 128 =\u0026gt; 256, stride=2, bias=false), \u001b[90m# 32_768 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\r),\rChain(\rConv((3, 3), 128 =\u0026gt; 256, pad=1, stride=2, bias=false), \u001b[90m# 294_912 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\rNNlib.relu,\rConv((3, 3), 256 =\u0026gt; 256, pad=1, bias=false), \u001b[90m# 589_824 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 256 =\u0026gt; 256, pad=1, bias=false), \u001b[90m# 589_824 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\rNNlib.relu,\rConv((3, 3), 256 =\u0026gt; 256, pad=1, bias=false), \u001b[90m# 589_824 parameters\u001b[39m\rBatchNorm(256), \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\r),\r),\r),\rChain(\rParallel(\raddact(NNlib.relu, ...),\rChain(\rConv((1, 1), 256 =\u0026gt; 512, stride=2, bias=false), \u001b[90m# 131_072 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\r),\rChain(\rConv((3, 3), 256 =\u0026gt; 512, pad=1, stride=2, bias=false), \u001b[90m# 1_179_648 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\rNNlib.relu,\rConv((3, 3), 512 =\u0026gt; 512, pad=1, bias=false), \u001b[90m# 2_359_296 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\r),\r),\rParallel(\raddact(NNlib.relu, ...),\ridentity,\rChain(\rConv((3, 3), 512 =\u0026gt; 512, pad=1, bias=false), \u001b[90m# 2_359_296 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\rNNlib.relu,\rConv((3, 3), 512 =\u0026gt; 512, pad=1, bias=false), \u001b[90m# 2_359_296 parameters\u001b[39m\rBatchNorm(512), \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\r),\r),\r),\r) \u001b[90m # Total: 60 trainable arrays, \u001b[39m11_176_512 parameters,\r\u001b[90m # plus 40 non-trainable, 9_600 parameters, summarysize \u001b[39m42.693 MiB.\rresnet_tune Chain(\rAdaptiveMeanPool((1, 1)),\rFlux.flatten,\rDense(512 =\u0026gt; 4), \u001b[90m# 2_052 parameters\u001b[39m\r) Define evaluation and training functions Again, will follow the model zoo documentation. Small adaptations will be needed. (These two functions were taken directly from the documentation).\nfunction eval_f(m_infer, m_tune, val_loader) good = 0 count = 0 for(x, y) in val_loader good += sum(Flux.onecold(m_tune(m_infer(x))) .== y) count += length(y) end acc = round(good / count, digits = 4) return acc end eval_f (generic function with 1 method)\rfunction train_epoch!(model_infer, model_tune, opt, loader) for (x, y) in loader infer = model_infer(x) grads = gradient(model_tune) do m Flux.Losses.logitcrossentropy(m(infer), Flux.onehotbatch(y, 1:4)) end update!(opt, model_tune, grads[1]) end end train_epoch! (generic function with 1 method)\rresnet_opt = Flux.setup(Flux.Optimisers.Adam(1e-3), resnet_tune); for iter = 1:5 @time train_epoch!(resnet_infer, resnet_tune, resnet_opt, train_loader) metric_train = eval_f(resnet_infer, resnet_tune, train_loader) metric_eval = eval_f(resnet_infer, resnet_tune, val_loader) @info \u0026#34;train\u0026#34; metric = metric_train @info \u0026#34;eval\u0026#34; metric = metric_eval end 176.283332 seconds (37.11 M allocations: 98.153 GiB, 6.06% gc time, 143.87% compilation time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.5744\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.5455\r70.815518 seconds (2.42 M allocations: 95.936 GiB, 11.25% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6823\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6273\r90.463025 seconds (2.42 M allocations: 95.936 GiB, 11.21% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.7032\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6455\r94.362892 seconds (2.42 M allocations: 95.936 GiB, 10.91% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.7433\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6727\r116.526515 seconds (2.42 M allocations: 95.936 GiB, 9.62% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.7885\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6909\rVision Transformers Similar to the PyTorch demonstration, we can do transfer learning by changing a different computer vision model (Vision Transformer).\nLet\u0026rsquo;s get into it.\nvit_model = ViT(:base; pretrain = true).layers # let\u0026#39;s have a look at the model head, to see how many inputs the head needs vit_model[2] Chain(\rLayerNorm(768), \u001b[90m# 1_536 parameters\u001b[39m\rDense(768 =\u0026gt; 1000), \u001b[90m# 769_000 parameters\u001b[39m\r) \u001b[90m # Total: 4 arrays, \u001b[39m770_536 parameters, 2.940 MiB.\r# modify the head vit_infer = deepcopy(vit_model[1]) # notice how we keep the input to the model head vit_tune = Chain( LayerNorm(768), Dense(768 =\u0026gt; 4), ) Chain(\rLayerNorm(768), \u001b[90m# 1_536 parameters\u001b[39m\rDense(768 =\u0026gt; 4), \u001b[90m# 3_076 parameters\u001b[39m\r) \u001b[90m # Total: 4 arrays, \u001b[39m4_612 parameters, 18.352 KiB.\rvit_opt = Flux.setup(Flux.Optimisers.Adam(1e-3), vit_tune); for iter = 1:5 @time train_epoch!(vit_infer, vit_tune, vit_opt, train_loader) metric_train = eval_f(vit_infer, vit_tune, train_loader) metric_eval = eval_f(vit_infer, vit_tune, val_loader) @info \u0026#34;train\u0026#34; metric = metric_train @info \u0026#34;eval\u0026#34; metric = metric_eval end 627.303072 seconds (17.32 M allocations: 291.924 GiB, 4.61% gc time, 3.66% compilation time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.7058\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6273\r565.986959 seconds (2.54 M allocations: 291.028 GiB, 4.71% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.8042\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6273\r516.041945 seconds (2.54 M allocations: 291.028 GiB, 4.92% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.866\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6818\r515.415614 seconds (2.54 M allocations: 291.028 GiB, 4.80% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.8973\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6818\r427.423410 seconds (2.54 M allocations: 291.028 GiB, 5.01% gc time)\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mtrain\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.9199\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39meval\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m metric = 0.6727\rSave the Models using JLD2 resnet_model_state = Flux.state(resnet_model) vit_model_state = Flux.state(vit_model) jldsave(\u0026#34;resnet_model.jld2\u0026#34;; resnet_model_state) jldsave(\u0026#34;vit_model.jld2\u0026#34;; vit_model_state) \u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mOpening file with JLD2.MmapIO failed, falling back to IOStream\r\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ JLD2 C:\\Users\\ingvi\\.julia\\packages\\JLD2\\7uAqU\\src\\JLD2.jl:300\u001b[39m\r\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mOpening file with JLD2.MmapIO failed, falling back to IOStream\r\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ JLD2 C:\\Users\\ingvi\\.julia\\packages\\JLD2\\7uAqU\\src\\JLD2.jl:300\u001b[39m\rusing BSON: @save @save \u0026#34;resnet_model_sate.bson\u0026#34; resnet_model @save \u0026#34;vit_model_state.bson\u0026#34; vit_model Thank you! I hope this demonstration on using Julia and Flux for transfer learning was helpful!\nVictor\n","permalink":"http://localhost:1313/posts/20240521_julia_transfer_learning_v5/20240521_julia_transfer_learning_v5/","summary":"Replicating the cat mood classifier, this time using Julia and Flux.jl.","title":"Transfer Learning Classifier Again... with Julia!"},{"content":"\nCat Expression Classifier Using Convolutional Neural Networks This project aims to build a cat expression classifier with convolutional neural networks (CNNs) using PyTorch. This project serves as an introduction to image classification and also dives into the nuances of handling a specific, custom dataset and adapting pre-trained models for our purposes.\nObjective The primary objective of this project is to develop a model capable of classifying images of cat faces into one of four moods: alarmed, angry, calm, and pleased. By the end of this tutorial, you will learn how to preprocess image data, leverage transfer learning for image classification, and evaluate a model\u0026rsquo;s performance.\nTools and Techniques We will employ PyTorch, a powerful and versatile deep learning library, to construct our CNN. The model of choice for this tutorial is ResNet18, a robust architecture that is commonly used in image recognition tasks. Given the straightforward nature of our classificaiton problem, ResNet18 provides an ecellent balance between complexity and performance.\nWhy Transfer Learning? In this tutorial, we utilize transfer learning to take advantage of a pre-trained ResNet18 model. This approach allows us to use a model that has already learned a significant amount of relevant features from a vast and diverse dataset (ImageNet). By fine-tuning this model to our specific task, we can achieve high accuracy with relatively little data and reduce the computational cost typycally associated with training a deep neural network from scratch.\nDataset The dataset comprises images of cat faces, labeled according to their expressed mood. These images are organized into training, validation, and testing sets, each with a corresponding CSV file which maps filenames to mood labels. This guide will walk you through the process of loading, preprocessing, and augmenting this data to suit the needs of our CNN.\nThe dataset was obtained here.\nLet\u0026rsquo;s get started!\nDataset Exploration Listing the Number of Images in Each Set and Visualizing The Set Below we will mount the drive to retrieve the data set files. Then, will use Python\u0026rsquo;s os module to list the number of images in the dataset. This will give us an idea of the size of the set.\nAdditionally, we will include a flag to tell the model whether we want to train it or to load a previously saved model\u0026hellip; this will become clear later.\nFinally, we will set up a function to visualize some sample images from each set.\nimport os # flag to control whether to train the model or load a saved model should_train_resnet = False should_train_vit = False def set_path(): # check if the notebook is running on google colab if \u0026#39;google.colab\u0026#39; in str(get_ipython()): print(\u0026#39;Running on Google Colab.\u0026#39;) from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) path = \u0026#39;/content/drive/PATH-TO-YOUR-DATA\u0026#39; else: print(\u0026#39;Running locally.\u0026#39;) path = \u0026#39;./data/cat_expression_data\u0026#39; return path base_dir = set_path() Running on Google Colab.\rMounted at /content/drive\r# base directories train_dir = os.path.join(base_dir, \u0026#39;train\u0026#39;) test_dir = os.path.join(base_dir, \u0026#39;test\u0026#39;) val_dir = os.path.join(base_dir, \u0026#39;val\u0026#39;) def list_images(directory): \u0026#34;\u0026#34;\u0026#34; list folders and count image files in each folder \u0026#34;\u0026#34;\u0026#34; dir_name = os.path.basename(directory) image_files = [f for f in os.listdir(directory) if f.lower().endswith((\u0026#39;.jpg\u0026#39;, \u0026#39;jpeg\u0026#39;, \u0026#39;.bmp\u0026#39;, \u0026#39;.gif\u0026#39;))] print(f\u0026#39;Total image files in {dir_name}: {len(image_files)}\u0026#39;) list_images(train_dir) list_images(val_dir) list_images(test_dir) Total image files in train: 1149\rTotal image files in val: 110\rTotal image files in test: 55\rFinally, we will make a dictionary that maps the classes to index-based labels, from the CSV file. We will need this way later, but we will define the dictionary this early on.\nVisualizing Some of the Data Let\u0026rsquo;s visualize a few images from each folder to ensure to have a better feel of the data.\nWe will do this by making a dataframe out of the annotations file where the labels are stored. We will use the test annotations, since this is the smallest dataset, and the other two have the same labels anyway.\nimport pandas as pd # define the annotations file annotations_filename = \u0026#39;_classes.csv\u0026#39; # define the full path to the annotations file test_annotations = os.path.join(test_dir, annotations_filename) # load the annotations file df = pd.read_csv(test_annotations) class_names = {index: col for index, col in enumerate(df.columns[1:])} # Adjust slicing if there are other columns print(class_names) {0: ' alarmed', 1: ' angry', 2: ' calm', 3: ' pleased'}\rimport matplotlib.pyplot as plt from PIL import Image import random def show_sample_images(main_directory, num_samples=5): \u0026#34;\u0026#34;\u0026#34;Display sample images from each subfolder within the main directory.\u0026#34;\u0026#34;\u0026#34; subfolders = [f.path for f in os.scandir(main_directory) if f.is_dir()] for directory in subfolders: image_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.lower().endswith((\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;))] if len(image_files) \u0026gt; 0: chosen_samples = random.sample(image_files, min(len(image_files), num_samples)) # Plot settings fig, axes = plt.subplots(1, min(len(image_files), num_samples), figsize=(15, 5)) fig.suptitle(f\u0026#39;Sample Images from {os.path.basename(directory)}\u0026#39;, fontsize=16) for ax, img_path in zip(axes.flatten(), chosen_samples): image = Image.open(img_path) ax.imshow(image) ax.axis(\u0026#39;off\u0026#39;) # ax.set_title(os.path.basename(img_path)) plt.show() else: print(f\u0026#34;No images to display in {os.path.basename(directory)}.\u0026#34;) # Example usage with the base directory containing train, validation, and test subfolders show_sample_images(base_dir, num_samples = 3) Data Preprocessing This steps involves preparing the dataset for training a PyTorch model by resizing, normalizing, and applying data augmentation.\nNOTE: At this point, it is important to know what is the model or CNN architecture we will be using. Important aspects to consider include the image size, any data transformations for training and validation, data augmentation techniques, and setting up data loaders later.\nIn this example, we will use ResNet18 . The inputs must follow a specific format, as per the PyTorch ResNet documentation found here:\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\nTools for Data Preprocessing in PyTorch torchvision.transforms: Provides common image transformations like resizing, normalization, and augmentation. torch.utils.data.Dataset: A base class for creating custom datasets. torch.utils.data.DataLoader: Loads and batches data for training. The Data The data set we are using consists of three folders: train, val, test. Each of them contain a set of images of cats. The labels in this case, are in the form of a CSV file that maps the filename with a one-hot encoding to label the classification of the image, i.e. the cat\u0026rsquo;s mood - alarmed, angry, calm, pleased.\nBecause this dataset structure is not exactly suitable for the ImageFolder module in PyTorch, whereby labelling is made easier and based on the folder structure, we need to create a custom dataset and loader. Let\u0026rsquo;s get started!\nDefine Image Transformations Specify resizing dimensions, normalization parameters, and augmentation techniques (like random rotation, flips, etc.). Create separate transformations for training and validation datasets. # import transforms import torch import torchvision.transforms as transforms # define the image size image_size = (224, 224) # adjusted for ResNet18 # define transformations for the training dataset train_transforms = transforms.Compose([ transforms.Resize(256), # resize to ensure minimum size transforms.CenterCrop(224), # center crop to 224x224 transforms.RandomHorizontalFlip(), # data augmentation transforms.RandomRotation(15), # data augmentation transforms.ConvertImageDtype(torch.float), # important, because the read_image reads as uint8, needs to be float # given that below we apply normalization transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) ]) val_transforms = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ConvertImageDtype(torch.float), transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) ]) With these transformations, the data pipeline will align with common practices for pre-trained models like ResNet18.\nCreate Custom Datasets and Data Loaders Given the structure of our dataset, where labels are provided in a CSV file rather than through directory structure, we need to use a custom dataset class. This will allow us to link echc image with its respective label based on our CSV file\u0026rsquo;s structure.\nCreating Custom Dataset We will extend the torch.utils.data.Dataset class to create our custom dataset. this class will override the necessary methods to handle our specific dataset setup:\nInitialization: Load the CSV file and set up the path to the images Length: Return the total number of images Get item: Load each image by index, apply the specified transformations, and parse the label from the CSV data import pandas as pd import torch from torch.utils.data import Dataset from torchvision.io import read_image class CustomImageDataset(Dataset): \u0026#34;\u0026#34;\u0026#34; a custom dataset class that loads images and their labels from a CSV \u0026#34;\u0026#34;\u0026#34; def __init__(self, annotations_file, img_dir, transform = None): \u0026#34;\u0026#34;\u0026#34; args: annotations_file (string): path to the CSV file with annotations img_dir (str): directory with all the images transform (callable, optional): transform to be applied on a sample \u0026#34;\u0026#34;\u0026#34; self.img_labels = pd.read_csv(annotations_file) # load annotations self.img_dir = img_dir self.transform = transform def __len__(self): \u0026#34;\u0026#34;\u0026#34; returns the number of items in the dataset \u0026#34;\u0026#34;\u0026#34; return len(self.img_labels) def __getitem__(self, idx): \u0026#34;\u0026#34;\u0026#34; fetches the image and label at the index idx from the dataset \u0026#34;\u0026#34;\u0026#34; img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) image = read_image(img_path) # convert one-hot encoded labels to a categorical label one_hot_label = self.img_labels.iloc[idx, 1:].values.astype(\u0026#39;float32\u0026#39;) # next find the index of the element in the slice which contains the \u0026#39;1\u0026#39; # since all other numbers will be 0; this will correspond to the label # 0, 1, 2, 3 label = torch.argmax(torch.tensor(one_hot_label)).item() if self.transform: image = self.transform(image) # apply transformations return image, label Now that we have defined the data classes, we can create objects for each of our datasets, as a CustomImageDataset class.\n# annotations_filename = \u0026#39;_classes.csv\u0026#39; # previously defined # paths to annotation files train_annotations = os.path.join(train_dir, annotations_filename) val_annotations = os.path.join(val_dir, annotations_filename) test_annotations = os.path.join(test_dir, annotations_filename) # previously defined # create dataset objects train_dataset = CustomImageDataset(train_annotations, train_dir, transform = train_transforms) val_dataset = CustomImageDataset(val_annotations, val_dir, transform = val_transforms) test_dataset = CustomImageDataset(test_annotations, test_dir, transform = val_transforms) Creating Data Loaders Data loaders in PyTorch provide the necessary functionality to batch, shuffle, and feed the data to your model during training in an efficient manner. They also handle parallel processing using multiple worker threads, which can significantly speed up data loading.\nIn short, data loaders take the dataset objects and handle the process of creating batches, shuffling the data, and parallelizing the data loading process.\nBelow we will create a data loaders for our datasets.\nfrom torch.utils.data import DataLoader train_loader = DataLoader( train_dataset, batch_size = 64, # defines how many samples per batch to load shuffle = True # shuffles the dataset at every epoch ) val_loader = DataLoader( val_dataset, batch_size = 64, shuffle = False # no need to shuffle validation data ) test_loader = DataLoader( test_dataset, batch_size = 64, shuffle = False ) In the loader above, we have the following main parts:\nBatch size: typycally set based on the system\u0026rsquo;s memory capacity and how large the model is. A larger batch size can speed up training but requires more memory. Shuffle: shuffling helps ensure that each batch sees a varierty of data across epochs, which can improve model generalization. Number of workers: this controls how many subproceses to use for data loading. More workers can lead to faster data preprocessing and reduced time to train each epoch but also increases memory usage. Integration with the Training Loop With the data loaders set up, we are now ready to integrate them into the model\u0026rsquo;s training and validation loops.\nThe code snippet below shows how this would be done. We will implement the actual integration when we get to the training section.\nfor epoch in range(num_epochs): for images, labels in train_loader: # Forward pass, backward pass, and optimize outputs = model(images) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() # Validation step at the end of each epoch model.eval() with torch.no_grad(): for images, labels in val_loader: predictions = model(images) # Calculate validation accuracy, loss, etc. Model Training Now that the data is ready and properly formatted for input into a neural network, the next step involves setting up and training the ResNet18 model. We will configure the model, define the loss function and optimizer, and implement the training and validation loops.\nNext Steps Model setup: Load the pre-trained ResNet18 model and modify it for our specific classification task (number of classes based on cat facial expressions) Loss function and optimizer: Define a loss function suitable for classification, e.g. CrossEntropyLoss Set up an optimizer (like Adam or SGD) to adjust the model weights during training based on the computed gradients Training loop: Implement the loop that processes the data through the model, computes the loss, updates the model parameters, and evaluates the model performance on the validation dataset periodically Monitoring and saving the model: Track performance metrics such as loss and accuracy Implement functionality to save the trained model for later use or further evaluation Model Setup In this section, we\u0026rsquo;ll configure a ResNet18 model to suit our specific classification task. Since the model is originally designed for ImageNet with 1000 classes, we\u0026rsquo;ll adapt it for our use case, which involves classifying images into four mood categories (alarmed, angry, calm, pleased).\nImport the Necessary Libraries # import torch # this has already been imported before import torch.nn as nn import torch.optim as optim from torchvision import models Load and Modify the Pre-trained ResNet18 We will load a pre-trained ResNet18 model and modify its final layer to suit our classification needs. This is known as transfer learning, and it is a technique that uses a pre-trained model and leverages its learned parameters to focus on a similar, more specific task. This is a powerful technique, since it uses the existing knowledge (such as edges and features) so that the new classification task is more robust, and faster to tune to the specific task.\nUnderstanding Transfer Learning Transfer Learning is a powerful technique in machine learning where a model developed for a particular task is reused as the starting point for a model on a second task. It\u0026rsquo;s especially popular in deep learning given the vast compute and time resources required to develop neural network models on large datasets and from scratch.\nWhy Use Transfer Learning? Efficiency: transfer learning allows us to leverage pre-trained networks that have already learned a good amount of features on large datasets. This is beneficial as it can drastically reduce the time and computational cost to achieve high performance. Performance: models trained on large-scalr datasets like ImageNet havve proven to generalize well to other datasets. Starting with these can provide a significand head-start in terms of performance. Applying Transfer Learning Model adaptation: for our specific task fo classifying cat moods, we take a pre-trained ResNet18 model and tailor it to our needs. The pre-trained model brings the advantage of learned features from ImageNet, a vast and diverse dataset. Feature extraction: by freezing (i.e. keeping the weight values as they are) the pre-trained layers, we utilize them as a feature extractor. Only the final layers are trained to adapt those features to our specific classification task. Model Setup with a Custom Classifier We have mentioned replacing the funal layer(s) as a transfer learning techniques. In this case, we replace the final fully connected (fc) layer of ResNet18 with a different layer which will suit our need to have 4 classes. Additionally, we will replace this fc layer with a more complex classifier portion, which involves adding additional layers such as ReLU for non-linearity, and dropout for regularization to prevent overfitting.\nReLU Activation: introduces non-linearity into the model, allowing it to learn more complex patterns. Dropout: Randomly zeros some of the elements of the input (to the layer, not input to the model) tensor with probability $p$ during training, which helps prevent overfitting. Let\u0026rsquo;s implement this classifier in our transfer learning setup.\n# assign the model weights weights = models.ResNet18_Weights.DEFAULT # create the model object with pre-trained weights model = models.resnet18(weights = weights) # freeze all the layers in the network for param in model.parameters(): param.requires_grad = False # replace the fc layer with a more complex classifier num_features = model.fc.in_features model.fc = nn.Sequential( nn.Linear(num_features, 256), # first linear layer nn.ReLU(), # non-linearity nn.Dropout(0.5), # dropout for regularization nn.Linear(256, len(class_names)) # output layer, 4 classes for cat moods ) # move model to GPU if available device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) model = model.to(device) Downloading: \u0026quot;https://download.pytorch.org/models/resnet18-f37072fd.pth\u0026quot; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\r100%|██████████| 44.7M/44.7M [00:00\u0026lt;00:00, 117MB/s]\rLoss Function and Optimizer For our classification task, we need a loss function that effectively measures the discrepancy between the predicted labels and the actual labels. Since we\u0026rsquo;ve configured out model outputs to be class indices (from our dataset\u0026rsquo;s one-hot encoded labels), we\u0026rsquo;ll use CrossEntropyLoss, which is ideal for such clasification tasks.\nWe\u0026rsquo;ll par this with the Adam optimizer, which is known for its efficiency in handling sparse gradients and adaptive learning rate capabilities, making it well-suited for this task.\nLet\u0026rsquo;s set up our loss function and optimizer.\n# loss function criterion = nn.CrossEntropyLoss() # optimizer # optimize only the final classifier layers optimizer = optim.Adam(model.fc.parameters(), lr = 0.001) Training and Validation Loops Now, let\u0026rsquo;s write the code to train and validate our model. This involves running the model over several epochs, making predictions, calculating loss, updating the model parameters, and evaluating the model\u0026rsquo;s perfomance on the validation dataset.\nTraining loop: here, the model learns by adjusting its weights based on the calculated loss from the training data Validation loop: validation occurs post the training phase in each epoch and helps in evaluating the model\u0026rsquo;s performance on unseen data, ensuring it generalizes well and doesn\u0026rsquo;t overfit Saving/Loading the Model Save the Trained Model If we have performed training, we can save the model to use next time, so that we can avoid re-training everytime we run the notebook.\nWe will implement this part as an if statement, that would run the training loop and save the model if we choose to, otherwise, we would just load the model weights.\nimport pickle # define a function to set the save model path def model_save_path(): # check if the notebook is running on google colab if \u0026#39;google.colab\u0026#39; in str(get_ipython()): print(\u0026#39;Running on Google Colab.\u0026#39;) path = \u0026#39;/content/drive/PATH-TO-YOUR-SAVE-FOLDER\u0026#39; else: print(\u0026#39;Running locally.\u0026#39;) path = \u0026#39;./saved_models\u0026#39; return path def training_loop(flag, model, criterion, optimizer, model_filename, model_data_filename, num_epochs = None): if flag: # num_epochs = 25 # define the number of epochs for training train_losses = [] val_losses = [] for epoch in range(num_epochs): model.train() # set the model to training mode total_train_loss = 0 for images, labels in train_loader: images, labels = images.to(device), labels.to(device) # forward pass to get outputs outputs = model(images) loss = criterion(outputs, labels) # backpropagation and optimization optimizer.zero_grad() loss.backward() optimizer.step() total_train_loss += loss.item() * images.size(0) # calculate average training loss for the epoch avg_train_loss = total_train_loss / len(train_loader.dataset) train_losses.append(avg_train_loss) # print average training loss per epoch\u0026#34; print(f\u0026#39;Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\u0026#39;) #------------------# # validation phase # #------------------# model.eval() # set the model to evaluation mode total_val_loss = 0 total_val_accuracy = 0 with torch.no_grad(): for images, labels in val_loader: images, labels = images.to(device), labels.to(device) outputs = model(images) loss = criterion(outputs, labels) total_val_loss += loss.item() * images.size(0) _, predicted = torch.max(outputs, 1) total_val_accuracy += (predicted == labels).sum().item() # calculate average validation loss for the epoch avg_val_loss = total_val_loss / len(val_loader.dataset) val_losses.append(avg_val_loss) # calculate validation accuracy val_accuracy = 100 * total_val_accuracy / len(val_loader.dataset) # print validation accuracy print(f\u0026#39; Validation loss: {avg_val_loss:.4f}, Validation accuracy: {val_accuracy:.2f}%\u0026#39;) # save model and training data save_model_path = model_save_path() # model_filename = \u0026#39;vit_cat_mood.pth\u0026#39; model_path = os.path.join(save_model_path, model_filename) torch.save(model.state_dict(), model_path) print(\u0026#39;Model saved to\u0026#39;, model_path) # save the training and validation losses def save_training_data(train_losses, val_losses, filename): with open(filename, \u0026#39;wb\u0026#39;) as f: pickle.dump({\u0026#39;train_losses\u0026#39;: train_losses, \u0026#39;val_losses\u0026#39;: val_losses}, f) print(f\u0026#34;Training data saved to {filename}\u0026#34;) # Specify the filename for saving training data training_data_filename = os.path.join(save_model_path, model_data_filename) save_training_data(train_losses, val_losses, training_data_filename) return train_losses, val_losses else: # load the trained model save_model_path = model_save_path() # model_filename = \u0026#39;vit_cat_mood.pth\u0026#39; model_path = os.path.join(save_model_path, model_filename) model.load_state_dict(torch.load(model_path, map_location = device)) model.eval() print(\u0026#39;Model loaded and set to evaluation mode.\u0026#39;) # load training data # Load the training and validation losses def load_training_data(filename): with open(filename, \u0026#39;rb\u0026#39;) as f: data = pickle.load(f) return data training_data_filename = os.path.join(save_model_path, model_data_filename) training_data = load_training_data(training_data_filename) train_losses = training_data[\u0026#39;train_losses\u0026#39;] val_losses = training_data[\u0026#39;val_losses\u0026#39;] print(\u0026#39;Training data loaded successfully.\u0026#39;) return train_losses, val_losses model_filename = \u0026#39;resnet18_cat_mood.pth\u0026#39; model_data_filename = \u0026#39;resnet18_training_data.pkl\u0026#39; train_losses, val_losses = training_loop(should_train_resnet, model, criterion, optimizer, model_filename, model_data_filename, num_epochs = 25) Running on Google Colab.\rModel loaded and set to evaluation mode.\rTraining data loaded successfully.\rModel Evaluation Now that the model is trained, the next step is to evaluate its performance more thoroughly, and possibly improve it based on the insights gained.\nEvaluating the model involves checking the accuracy and also looking at other metrics like precision, recall, and F1-score, especially if the dataset is imbalanced or if specific classes are more important than others.\nModel Evaluation on Validation Set After training a machine learning model, it\u0026rsquo;s crucial to evaluate its performance comprehensively. Here, we will detail three key diagnostic tools\u0026quot;\nConfusion matrix Plotting training and validation losses Visualization of the predictions Confusion Matrix A confusion matrix provides a detailed breakdown of the model\u0026rsquo;s predictions, showing exactly how many samples from each class were correctly or incorrectly predicted as each other class. This is crucial for understanding the model\u0026rsquo;s performance across different categories.\nfrom sklearn.metrics import confusion_matrix, classification_report import seaborn as sns import matplotlib.pyplot as plt def evaluate_model(model, data_loader, device): model.eval() true_labels = [] predictions = [] with torch.no_grad(): for images, labels in data_loader: images, labels = images.to(device), labels.to(device) outputs = model(images) _, preds = torch.max(outputs, 1) predictions.extend(preds.cpu().numpy()) true_labels.extend(labels.cpu().numpy()) # compute the confusion matrix cm = confusion_matrix(true_labels, predictions) clf_report = classification_report(true_labels, predictions, output_dict = True) # Convert classification report dictionary to DataFrame clf_report_df = pd.DataFrame(clf_report).transpose() return cm, clf_report_df cm, clf_report_df = evaluate_model(model, val_loader, device) # Plot the confusion matrix plt.figure(figsize=(10, 8)) sns.heatmap(cm, annot=True, fmt=\u0026#39;d\u0026#39;, cmap=\u0026#39;Blues\u0026#39;) plt.title(\u0026#39;Confusion Matrix\u0026#39;) plt.ylabel(\u0026#39;True Label\u0026#39;) plt.xlabel(\u0026#39;Predicted Label\u0026#39;) plt.show() # Print classification report print(\u0026#39;Classification Report:\u0026#39;) print(clf_report_df) Classification Report:\rprecision recall f1-score support\r0 0.333333 0.285714 0.307692 14.000000\r1 0.862069 0.714286 0.781250 35.000000\r2 0.487179 0.678571 0.567164 28.000000\r3 0.866667 0.787879 0.825397 33.000000\raccuracy 0.672727 0.672727 0.672727 0.672727\rmacro avg 0.637312 0.616613 0.620376 110.000000\rweighted avg 0.700728 0.672727 0.679728 110.000000\rPlotting Training and Validation Losses Plotting the training and validation losses over epochs allows us to monitor the learning process, identifying issues such as overfitting or underfitting.\ndef plot_losses(train_losses, val_losses): \u0026#34;\u0026#34;\u0026#34; Plot the training and validation losses. Parameters: - train_losses: list of training loss values per epoch - val_losses: list of validation loss values per epoch \u0026#34;\u0026#34;\u0026#34; plt.figure(figsize = (10, 6)) plt.plot(train_losses, label = \u0026#39;Training Loss\u0026#39;, color = \u0026#39;blue\u0026#39;, marker = \u0026#39;o\u0026#39;) plt.plot(val_losses, label = \u0026#39;Validation Loss\u0026#39;, color = \u0026#39;red\u0026#39;, marker = \u0026#39;o\u0026#39;) plt.title(\u0026#39;Training and Validation Losses Over Epochs\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.legend() plt.grid(True) plt.show() # take the tracked losses from thet training loop plot_losses(train_losses, val_losses) Visualization of the Predictions Visualizing model predictions on actual data points provides immediate qualitative feedback about model behavior. It helps identify paterns in which the model performs well or poorly, revealing potential biases, underfitting, or overfitting.\nimport numpy as np def visualize_predictions(model, data_loader, device, class_names, num_images = 10): model.eval() images_so_far = 0 rows = num_images // 2 if num_images % 2 == 0 else num_images // 2 + 1 fig, axes = plt.subplots(nrows = rows, ncols = 2, figsize = (12, num_images * 3)) # define the mean and std deviation used for normalization mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) with torch.no_grad(): for i, (images, labels) in enumerate(data_loader): images, labels = images.to(device), labels.to(device) outputs = model(images) _, preds = torch.max(outputs, 1) for j in range(images.size(0)): if images_so_far \u0026lt; num_images: ax = axes[images_so_far // 2, images_so_far % 2] if num_images \u0026gt; 1 else axes # arrange in grid ax.axis(\u0026#39;off\u0026#39;) # convert tensors to integers predicted_label = preds[j].item() actual_label = labels[j].item() # reverse normalization transform img = images.cpu().data[j].numpy().transpose((1, 2, 0)) # change CxHxW to HxWxC img = std * img + mean # reverse normalization img = np.clip(img, 0, 1) # clip values to ensure they fall between 0 and 1 # use converted integers to access class names ax.set_title(f\u0026#39;predicted: {class_names[predicted_label]} | actual: {class_names[actual_label]}\u0026#39;, fontsize = 12) ax.imshow(img) images_so_far += 1 else: plt.tight_layout() # adjust layout plt.show() return # make new loader for random samples vis_loader = DataLoader( val_dataset, batch_size = 10, shuffle = True, ) visualize_predictions(model, vis_loader, device, class_names, num_images = 4) Evaluation of Model Performance Our Cat Expression Classifier, built on a modified ResNet18 architecture, demonstrates a promising ability to classify cat expressions into four categories: alarmed, angry, calm, and pleased. Here, we provide a detailed analysis of the model\u0026rsquo;s performance based on our the training and validation efforts.\nOverall Performance Metrics The model achieves an overall accuracy of 68.18% on the validation set. This is a decent foundation but indicates room for further refinement, especially in distinguishing between expressions that share subtle features. Here is a breakdown of the key performance metrics:\nPrecision: Measures the accuracy of positive predictions. for example, the pleased category shows high precision, indicating that the model reliably identifies this expression. Recall: Reflects the model\u0026rsquo;s ability to identify all relevant instances of a class. The angry category has a high recall, suggesting that the model effectively captures most of the angry expressions. F1-Score: Balances precision and recall and is particularly useful in scenarios where class distribution is uneven. Confusion Matrix Insights the confusion matrix provides a granular view of the model\u0026rsquo;s performance across the different classes. It highlights specific areas where the model performs well and others where it struggles, such as:\nMisclassifications between alarmed and angry suggest that the model may be conflating these expressions due to their similar features. The high accuracy in identifying pleased expressions shows that distinct features of this mood are well captured by the model. Training and Validation Losses The training and validation loss plots reveal the learning dynamics over the epochs:\nA steady decrease in training loss indicates that the model is effectively learning from the training data. The pattern of validation loss provides insights into the model\u0026rsquo;s generalization ability. Increases in validation loss suggest moments where the model might be overfitting to the training data. Trying out VisionTransformers Data Transformations for Vision Transformer When transitioning from a CNN like ResNet18 to a Vision Transformer (ViT), it\u0026rsquo;s essential to evaluate whether the existing preprocessing steps - particulary the data transformations - are suitable for the new model architecture. For ViT , wed must consider their unique handling of image data, which relies on dividing the image into fixed-size patches and understanding global dependencies through self-attention mechanisms.\nFor this exercise, we will maintain the same transformations we have previously defined.\nThe decision to retain the initial transformations is based on the principle of consistency and the minimal impact expected by changing model architectures regarding how images are scaled and augmented. The chosen transformations ensure that the images are adequately prepared for the neural network without introducing complexities or distortions that could hinder the learning of global patterns, which are vital for Vision Transformers due to their reliance on self-attention mechanisms.\nAdditionally, maintaining these transformations allows for a more straightforward comparison between the ResNet18 model and the Vision Transformer model, as any changes in model performance can more confidently be attributed to the architectural differences rather than changes in data preprocessing.\nLoad Pre-Trained Vision Transformer Model First, we need to load the ViT model that has been pre-trained on a large dataset. We\u0026rsquo;ll then adapt the classifier head to our needs, which is classifying cat moods into four categories.\n# load the ViT pre-trained model weights_vit = models.ViT_B_16_Weights.DEFAULT model_vit = models.vit_b_16(weights = weights_vit) # print the model structure to understand what needs to be replaced print(model_vit) Downloading: \u0026quot;https://download.pytorch.org/models/vit_b_16-c867db91.pth\u0026quot; to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\r100%|██████████| 330M/330M [00:02\u0026lt;00:00, 124MB/s]\rVisionTransformer(\r(conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r(encoder): Encoder(\r(dropout): Dropout(p=0.0, inplace=False)\r(layers): Sequential(\r(encoder_layer_0): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_1): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_2): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_3): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_4): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_5): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_6): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_7): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_8): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_9): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_10): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_11): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r)\r(ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r)\r(heads): Sequential(\r(head): Linear(in_features=768, out_features=1000, bias=True)\r)\r)\rFreezing the Encoder Layers Freezing the encoder layers prevents their weights from being updated during training, which means they retain the knowledge they have already gained from ImageNet. We only want to train the classifier that we will modify to our specific task.\n# freeze all layers in the model by disabling gradient computation for param in model_vit.parameters(): param.requires_grad = False Modify the Classifier The standard ViT model includes a classifier at the end (usually named heads in torchvision models), which is a linear layer designed for the original classification task, e.g. 1000 classes for ImageNet. We will replace this with a new classifier suited for our task (4 cat moods).\n# replace the classifier head # as we saw in the architecture above, the classifier is called `heads` num_features = model_vit.heads.head.in_features # ge tthe number of input features # replace with a new head for len(class_names) = 4 model_vit.heads = nn.Sequential( nn.Linear(num_features, len(class_names)) ) # move model to appropriate device model_vit.to(device) VisionTransformer(\r(conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r(encoder): Encoder(\r(dropout): Dropout(p=0.0, inplace=False)\r(layers): Sequential(\r(encoder_layer_0): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_1): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_2): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_3): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_4): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_5): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_6): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_7): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_8): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_9): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_10): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r(encoder_layer_11): EncoderBlock(\r(ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(self_attention): MultiheadAttention(\r(out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\r)\r(dropout): Dropout(p=0.0, inplace=False)\r(ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r(mlp): MLPBlock(\r(0): Linear(in_features=768, out_features=3072, bias=True)\r(1): GELU(approximate='none')\r(2): Dropout(p=0.0, inplace=False)\r(3): Linear(in_features=3072, out_features=768, bias=True)\r(4): Dropout(p=0.0, inplace=False)\r)\r)\r)\r(ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\r)\r(heads): Sequential(\r(0): Linear(in_features=768, out_features=4, bias=True)\r)\r)\rDefine Loss Function and Optimizer Now, define the loss function and an optimizer. Since we are only training the classifier layer, ensure the optimizer is set to only update the parameters of the classifier.\n# optimizer will not change, but still show it here: criterion = nn.CrossEntropyLoss() # optimizer - only optimize the classifier parameters optimizer = optim.Adam(model_vit.heads.parameters(), lr = 0.001) Training Loop Here, we revisit the training process, adapting our previously established procedures to the Vision Transformer (ViT) model. Much like our approach with Resnet18, we utilize a similar training loop structure to ensure consistency and comparability. the core steps of training - forward pass, loss computation, backward pass, and parameters update - are maintained, but they are now applied to a differently structured model that leverages self-attention mechanisms rather than convolutional layers. This section briefly outlines these steps, focusing on any adjustments specific to the ViT to optimize it for our cat mood classification task.\nmodel_filename = \u0026#39;vit_cat_mood.pth\u0026#39; model_data_filename = \u0026#39;vit_training_data.pkl\u0026#39; train_losses, val_losses = training_loop(should_train_vit, model_vit, criterion, optimizer, model_filename, model_data_filename, num_epochs = 25) Running on Google Colab.\rModel loaded and set to evaluation mode.\rTraining data loaded successfully.\rModel Evaluation Confusion Matrix The confusion matrix below shows the following results:\nClass alarmed: moderate confusion with other classes, indicating difficulty in distinguishing alarmed from other moods. Class angry: high accuracy, showing that `angry is well-recognized, with few misclassifications Class calm: some confusion, particularly with pleased, suggesting similar features or expressions between these moods that the model confuses Class pleased: best performance, indicating clear distinguishing features that the model learnes effectively Considerations on Data Quality Throughout the development and evaluation of our models, it has become evident that the quality of the dataset significantly impacts the classification accuracy. Certain misclassifications observed, such as the confusion between \u0026lsquo;pleased\u0026rsquo; and \u0026lsquo;calm\u0026rsquo; or \u0026lsquo;alarmed\u0026rsquo; and \u0026lsquo;angry,\u0026rsquo; suggest that the labels may not always align perfectly with the visual cues present in the images. This discrepancy can stem from subjective interpretations of cat expressions during labeling. Improving the dataset by refining the labeling process, possibly with the assistance of animal behavior experts, or by curating a more consistently labeled dataset could enhance model performance. Enhancing data quality would help in training more accurate and reliable models, thereby increasing the robustness of the classification outcomes.\nClassification Report From the classification report, we can draw the following conclusions:\nClass pleased shows the highest precision, indicating a high rate of true positive predictions Class angry has the highes recall, suggesting effective identification of this mood Classes angry and pleased show high F1-scores, indicating robust performance. Overall Accuracy The model achieves and accuracy of 74.55%, which is a solid performance but suggests room for improvement, particularly in reducing misclassifications among less distinct moods.\ncm, clf_report_df = evaluate_model(model_vit, val_loader, device) # Plot the confusion matrix plt.figure(figsize=(10, 8)) sns.heatmap(cm, annot=True, fmt=\u0026#39;d\u0026#39;, cmap=\u0026#39;Blues\u0026#39;) plt.title(\u0026#39;Confusion Matrix\u0026#39;) plt.ylabel(\u0026#39;True Label\u0026#39;) plt.xlabel(\u0026#39;Predicted Label\u0026#39;) plt.show() # Print classification report print(\u0026#39;Classification Report:\u0026#39;) print(clf_report_df) Classification Report:\rprecision recall f1-score support\r0 0.461538 0.428571 0.444444 14.000000\r1 0.815789 0.885714 0.849315 35.000000\r2 0.625000 0.535714 0.576923 28.000000\r3 0.857143 0.909091 0.882353 33.000000\raccuracy 0.745455 0.745455 0.745455 0.745455\rmacro avg 0.689868 0.689773 0.688259 110.000000\rweighted avg 0.734544 0.745455 0.738361 110.000000\rTraining and Validation Losses The plot of training loss shows a consistent decrease, indicating that the model is effectively learning from the data. The vlaidation loss decreases alongside the training loss but begins to plateau, suggesting that the model might be nearing its learning capacity with the current configuration and dataset.\n# take the tracked losses from thet training loop plot_losses(train_losses, val_losses) visualize_predictions(model_vit, vis_loader, device, class_names, num_images = 4) Using the Model for Inference on New Data Let\u0026rsquo;s try out the model on new, unseen data. This photo did not come from a dataset, but rather from a friend of mine who wants to know her cat\u0026rsquo;s mood.\nFor inference, we first need to apply some simple transformation (not augmentation). In this case, we can use our previously defined val_transforms:\nval_transforms = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.ConvertImageDtype(torch.float), transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) ]) Then, we can define a simple function to open, transform and add a batch dimension to the file we want to pass.\nThen, the function will pass the image to the model to make inference. This latter step is done in a way so that the gradients are not computed, and the image data is passed as a forward pass only.\nFinally, we will include a de-normalization to the transformed image, so that we can display it along with the predicted label.\n# Function to classify a single image and display it with the predicted label def classify_and_display_image(image_path): image = Image.open(image_path) transformed_image = val_transforms(image) image_tensor = transformed_image.unsqueeze(0) # Add batch dimension with torch.no_grad(): # Disable gradient calculation output = model(image_tensor) _, predicted = torch.max(output, 1) label = class_names[predicted.item()] # Convert the transformed image tensor back to a PIL image for display transformed_image = transformed_image.permute(1, 2, 0) # Change from (C, H, W) to (H, W, C) transformed_image = transformed_image.numpy() # Denormalize the image mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) transformed_image = std * transformed_image + mean transformed_image = np.clip(transformed_image, 0, 1) # Display the image with the predicted label plt.imshow(transformed_image) plt.title(f\u0026#39;Predicted Label: {label}\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() # Example usage classify_and_display_image(\u0026#39;gato.jpg\u0026#39;) Conclusion This tutorial guides you through creating a cat expression classifier using convolutional neural networks with ResNet18 and later with a Vision Transformer (ViT). It demonstrates how to apply transfer learning to improve efficiency and accuracy with limited data.\nThe guide is structured to provide clear steps and practical examples for each phase of the project, from data preprocessing and model training to evaluation. By breaking down complex concepts and processes into manageable parts, it ensures that readers can easily follow along and apply these techniques to their projects.\n","permalink":"http://localhost:1313/posts/20240515_cat_mood_classification/20240515_cat_mood_classification/","summary":"Things we learn here include image data exploration, transfer learning, custom datasets, comparing ML models, saving/loading models and model data, conditional setup for different work environments.","title":"Transfer Learning Classifier Using PyTorch"},{"content":" Part 1: Collecting and Cleaning the Data import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.graph_objects as go import plotly.express as px import json Importing the excel files. These files come from the INE (Electoral Institute in Mexico). Then, we can do some data wrangling to filter the relevant data.\nThe data comes from these two sources:\nThe voter registration data: The Open Data page The elections results data: Election Results Let\u0026rsquo;s explore the data as it comes.\n(Note that, unfortunately, the dataframe output did not export correctly to the file displayed here).\ndf_sx = pd.read_excel(\u0026#39;data/padron_y_ln_sexo.xlsx\u0026#39;) print(len(df_sx.columns)) df_sx.head() 15\rCLAVE\\nENTIDAD\rNOMBRE\\nENTIDAD\rCLAVE\\nDISTRITO\rNOMBRE\\nDISTRITO\rCLAVE\\nMUNICIPIO\rNOMBRE\\nMUNICIPIO\rSECCION\rPADRON\\nHOMBRES\rPADRON\\nMUJERES\rPADRON\\nNO BINARIO\rPADRON\\nELECTORAL\rLISTA\\nHOMBRES\rLISTA\\nMUJERES\rLISTA\\nNO BINARIO\rLISTA\\nNOMINAL\r0\r1\rRESIDENTES EXTRANJERO\r0.0\r0\r0.0\r0\r0.0\r8444\r5756\r0\r14200\r3452\r2577\r0\r6029\r1\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r338.0\r973\r1013\r0\r1986\r970\r1011\r0\r1981\r2\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r339.0\r895\r954\r0\r1849\r893\r953\r0\r1846\r3\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r340.0\r951\r1001\r0\r1952\r949\r998\r0\r1947\r4\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r341.0\r1174\r1184\r0\r2358\r1172\r1184\r0\r2356\rWe start by changing the names of the columns for readability.\n# change column names col_names_sx = [\u0026#39;Clave Entidad\u0026#39;, \u0026#39;Nombre Entidad\u0026#39;, \u0026#39;Clave Distrito\u0026#39;, \u0026#39;Nombre Distrito\u0026#39;, \u0026#39;Clave Municipio\u0026#39;, \u0026#39;Nombre Municipio\u0026#39;, \u0026#39;Seccion\u0026#39;, \u0026#39;Padron Hombre\u0026#39;, \u0026#39;Padron Mujeres\u0026#39;, \u0026#39;Padron No Binario\u0026#39;, \u0026#39;Padron Electoral\u0026#39;, \u0026#39;Lista Hombres\u0026#39;, \u0026#39;Lista Mujeres\u0026#39;, \u0026#39;Lista No Binario\u0026#39;, \u0026#39;Lista Nominal\u0026#39;] df_sx.columns = col_names_sx df_sx.head() Clave Entidad\rNombre Entidad\rClave Distrito\rNombre Distrito\rClave Municipio\rNombre Municipio\rSeccion\rPadron Hombre\rPadron Mujeres\rPadron No Binario\rPadron Electoral\rLista Hombres\rLista Mujeres\rLista No Binario\rLista Nominal\r0\r1\rRESIDENTES EXTRANJERO\r0.0\r0\r0.0\r0\r0.0\r8444\r5756\r0\r14200\r3452\r2577\r0\r6029\r1\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r338.0\r973\r1013\r0\r1986\r970\r1011\r0\r1981\r2\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r339.0\r895\r954\r0\r1849\r893\r953\r0\r1846\r3\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r340.0\r951\r1001\r0\r1952\r949\r998\r0\r1947\r4\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r341.0\r1174\r1184\r0\r2358\r1172\r1184\r0\r2356\rVerify the data types in the dataframe and whether there are null values\nprint(df_sx.dtypes) print(df_sx.isnull().sum()) Clave Entidad object\rNombre Entidad object\rClave Distrito float64\rNombre Distrito object\rClave Municipio float64\rNombre Municipio object\rSeccion float64\rPadron Hombre int64\rPadron Mujeres int64\rPadron No Binario int64\rPadron Electoral int64\rLista Hombres int64\rLista Mujeres int64\rLista No Binario int64\rLista Nominal int64\rdtype: object\rClave Entidad 0\rNombre Entidad 1\rClave Distrito 1\rNombre Distrito 1\rClave Municipio 1\rNombre Municipio 1\rSeccion 1\rPadron Hombre 0\rPadron Mujeres 0\rPadron No Binario 0\rPadron Electoral 0\rLista Hombres 0\rLista Mujeres 0\rLista No Binario 0\rLista Nominal 0\rdtype: int64\rThere are indeed some null values in some of the columns. Since the null values live in rows that we will not need, i.e. we will filter the rows to include only the values for the State of \u0026ldquo;Quintana Roo\u0026rdquo;, we don\u0026rsquo;t need to bother in removing or doing any data gymnastics to those null values.\nLet\u0026rsquo;s filter the data we need now.\n# filter rows by state quintana roo df_sx_qroo = df_sx[df_sx[\u0026#39;Nombre Entidad\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] # select columns for padron electoral df_pe_sx_qroo = df_sx_qroo.iloc[:,:11] # select columns for lista nominal cols_to_drop = df_sx_qroo.columns[7:11] df_ln_sx_qroo = df_sx_qroo.drop(columns = cols_to_drop) df_ln_sx_qroo.head() Clave Entidad\rNombre Entidad\rClave Distrito\rNombre Distrito\rClave Municipio\rNombre Municipio\rSeccion\rLista Hombres\rLista Mujeres\rLista No Binario\rLista Nominal\r50685\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r182.0\r1046\r1015\r0\r2061\r50686\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r183.0\r1056\r1085\r0\r2141\r50687\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r184.0\r982\r981\r0\r1963\r50688\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r185.0\r1228\r1198\r0\r2426\r50689\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r186.0\r525\r465\r0\r990\rElection Results. First, we will work with the data related to the election results.\nLet\u0026rsquo;s load all the files separately (as they were available from the source).\n# load the files df_re_2009 = pd.read_csv(\u0026#39;data/DIPUTACIONES_FED_MR_2009/2009_SEE_DIP_FED_MR_NAL_SEC.csv\u0026#39;) df_re_2012 = pd.read_csv(\u0026#39;data/DIPUTACIONES_FED_MR_2012/2012_SEE_DIP_FED_MR_NAL_SEC.csv\u0026#39;) df_re_2015 = pd.read_csv(\u0026#39;data/DIPUTACIONES_FED_MR_2015/2015_SEE_DIP_FED_MR_NAL_SEC.csv\u0026#39;) df_re_2018 = pd.read_csv(\u0026#39;data/DIPUTACIONES_FED_MR_2018/2018_SEE_DIP_FED_MR_NAL_SEC.csv\u0026#39;) df_re_2021 = pd.read_csv(\u0026#39;data/DIPUTACIONES_FED_MR_2021/2021_SEE_DIP_FED_MR_NAL_SEC.csv\u0026#39;) # filter rows by state quintana roo df_re_2009_qroo = df_re_2009[df_re_2009[\u0026#39;NOMBRE_ESTADO\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] df_re_2012_qroo = df_re_2012[df_re_2012[\u0026#39;NOMBRE_ESTADO\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] df_re_2015_qroo = df_re_2015[df_re_2015[\u0026#39;NOMBRE_ESTADO\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] df_re_2018_qroo = df_re_2018[df_re_2018[\u0026#39;NOMBRE_ESTADO\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] df_re_2021_qroo = df_re_2021[df_re_2021[\u0026#39;NOMBRE_ESTADO\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] df_re_2021_qroo.head() CIRCUNSCRIPCION\rID_ESTADO\rNOMBRE_ESTADO\rID_DISTRITO_FEDERAL\rCABECERA_DISTRITAL_FEDERAL\rID_MUNICIPIO\rMUNICIPIO\rSECCION\rCASILLAS\rPAN\r...\rPVEM_PT\rPVEM_MORENA\rPT_MORENA\rCAND_IND1\rCAND_IND2\rCAND_IND3\rNUM_VOTOS_CAN_NREG\rNUM_VOTOS_NULOS\rTOTAL_VOTOS\rLISTA_NOMINAL\r49016\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r0\rNaN\r0\rNaN\r0.0\r...\r0.0\r0.0\r0.0\rNaN\rNaN\rNaN\r0.0\r0.0\r2.0\r2\r49017\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r182\r3.0\r180.0\r...\r0.0\r3.0\r2.0\rNaN\rNaN\rNaN\r0.0\r29.0\r1097.0\r2160\r49018\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r183\r3.0\r103.0\r...\r0.0\r5.0\r3.0\rNaN\rNaN\rNaN\r3.0\r30.0\r1083.0\r2159\r49019\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r184\r3.0\r91.0\r...\r0.0\r10.0\r6.0\rNaN\rNaN\rNaN\r1.0\r46.0\r1169.0\r1981\r49020\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r185\r4.0\r113.0\r...\r0.0\r3.0\r6.0\rNaN\rNaN\rNaN\r1.0\r38.0\r1377.0\r2371\r5 rows × 34 columns\nWe can already see that there are some columns that are not needed. These columns, in particular, are the independent candidate columns, and they are effectively empty. These columns\u0026rsquo; names all begin with CAND. We can use that to quickly drop all of them.\nWe can also drop the null value included in the municipality (MUNICIPIO) columns, which leaked after filtering by the State before. We need to drop this one too.\n# define a list of all the dataframes to loop over them below df_re_all_years = [df_re_2009_qroo, df_re_2012_qroo, df_re_2015_qroo, df_re_2018_qroo, df_re_2021_qroo] # drop independent candidates and any rows with NaN in the MUNICIPIO columns for df in df_re_all_years: # drop rows where \u0026#39;MUNICIPIO\u0026#39; is NaN df.dropna(subset=[\u0026#39;MUNICIPIO\u0026#39;], inplace = True) # select columns that begin with \u0026#34;CAND\u0026#34; cols_cand_ind = df.columns[df.columns.str.startswith(\u0026#39;CAND\u0026#39;)] # drop the identified columns df.drop(columns=cols_cand_ind, inplace = True) df_re_all_years[-1].head() CIRCUNSCRIPCION\rID_ESTADO\rNOMBRE_ESTADO\rID_DISTRITO_FEDERAL\rCABECERA_DISTRITAL_FEDERAL\rID_MUNICIPIO\rMUNICIPIO\rSECCION\rCASILLAS\rPAN\r...\rPAN_PRD\rPRI_PRD\rPVEM_PT_MORENA\rPVEM_PT\rPVEM_MORENA\rPT_MORENA\rNUM_VOTOS_CAN_NREG\rNUM_VOTOS_NULOS\rTOTAL_VOTOS\rLISTA_NOMINAL\r49017\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r182\r3.0\r180.0\r...\r0.0\r2.0\r5.0\r0.0\r3.0\r2.0\r0.0\r29.0\r1097.0\r2160\r49018\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r183\r3.0\r103.0\r...\r0.0\r2.0\r5.0\r0.0\r5.0\r3.0\r3.0\r30.0\r1083.0\r2159\r49019\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r184\r3.0\r91.0\r...\r0.0\r2.0\r12.0\r0.0\r10.0\r6.0\r1.0\r46.0\r1169.0\r1981\r49020\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r185\r4.0\r113.0\r...\r0.0\r1.0\r15.0\r0.0\r3.0\r6.0\r1.0\r38.0\r1377.0\r2371\r49021\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r186\r2.0\r37.0\r...\r0.0\r0.0\r3.0\r0.0\r11.0\r7.0\r0.0\r23.0\r650.0\r972\r5 rows × 31 columns\nNow we can extract the political party names from each of the dataframes. At each election year, the parties may be different, and also there are alliances which make this somewhat more complicated.\n# List to store party names lists for each DataFrame party_names_per_df = [] for df in df_re_all_years: party_names = [] # Empty list to store party names for current DataFrame # Get a list of column names column_names = list(df.columns) try: # Find the indices of \u0026#34;CASILLAS\u0026#34; and \u0026#34;NUM_VOTOS_CAN_NREG\u0026#34; (handle potential errors) casillas_index = column_names.index(\u0026#39;CASILLAS\u0026#39;) num_votos_index = column_names.index(\u0026#39;NUM_VOTOS_CAN_NREG\u0026#39;) # Extract party names between the indices (avoid out-of-bounds) party_names = column_names[casillas_index + 1:min(num_votos_index, len(column_names))] except ValueError: # Handle cases where \u0026#34;CASILLAS\u0026#34; or \u0026#34;NUM_VOTOS_CAN_NREG\u0026#34; might not exist print(f\u0026#34;\\nWARNING: \u0026#39;CASILLAS\u0026#39; or \u0026#39;NUM_VOTOS_CAN_NREG\u0026#39; not found in DataFrame\u0026#34;) # Append the party names list for this DataFrame party_names_per_df.append(party_names) party_names_per_df [['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'CONV',\r'NVA_ALIANZA',\r'PSD',\r'PRIMERO_MEXICO',\r'SALVEMOS_MEXICO'],\r['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'MC',\r'NVA_ALIANZA',\r'PRI_PVEM',\r'PRD_PT_MC',\r'PRD_PT',\r'PRD_MC',\r'PT_MC'],\r['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'MC',\r'NVA_ALIANZA',\r'MORENA',\r'PH',\r'ES',\r'PAN_NVA_ALIANZA',\r'PRI_PVEM',\r'PRD_PT'],\r['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'MC',\r'NA',\r'MORENA',\r'ES',\r'PAN_PRD_MC',\r'PAN_PRD',\r'PAN_MC',\r'PRD_MC',\r'PRI_PVEM_NA',\r'PRI_PVEM',\r'PRI_NA',\r'PVEM_NA',\r'PT_MORENA_ES',\r'PT_MORENA',\r'PT_ES',\r'MORENA_ES'],\r['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'MC',\r'MORENA',\r'PES',\r'RSP',\r'FXM',\r'PAN_PRI_PRD',\r'PAN_PRI',\r'PAN_PRD',\r'PRI_PRD',\r'PVEM_PT_MORENA',\r'PVEM_PT',\r'PVEM_MORENA',\r'PT_MORENA']]\rJust to get a feel of the data, we can see how many rows there are in each of the dataframes.\nprint(\u0026#39;Number of lines in the 2009 election dataframe: \u0026#39;, len(df_re_2009_qroo) ) print(\u0026#39;Number of lines in the 2012 election dataframe: \u0026#39;, len(df_re_2012_qroo) ) print(\u0026#39;Number of lines in the 2015 election dataframe: \u0026#39;, len(df_re_2015_qroo) ) print(\u0026#39;Number of lines in the 2018 election dataframe: \u0026#39;, len(df_re_2018_qroo) ) print(\u0026#39;Number of lines in the 2021 election dataframe: \u0026#39;, len(df_re_2021_qroo) ) Number of lines in the 2009 election dataframe: 729\rNumber of lines in the 2012 election dataframe: 831\rNumber of lines in the 2015 election dataframe: 938\rNumber of lines in the 2018 election dataframe: 939\rNumber of lines in the 2021 election dataframe: 1033\rPreliminary Visualization Visualizaremos la informacion agregando los datos por año. De esta manera, tendremos un historial de tiempo por cada municipio, en el cual podemos ver el historial de los resultados de cada partido politico en cada municipio.\nHere we visualize the information by aggregating the data per year. In this way, we will have a time history per municipality (municipio), where we can see the history of the results for each political party in each municipality.\nGroup by Political Party Para un analisis mas compacto, podemos agrupar los partidos politicos incluyendo sus alianzas. De este modo, por ejemplo, tendriamos que:\nFor a more compact analysis, we can group the political parties including the alliances. In this we, we would have, for example:\n\u0026#39;PAN\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PAN_NVA_ALIANZA\u0026#39;, \u0026#39;PAN_PRD_MC\u0026#39;, \u0026#39;PAN_PRD\u0026#39;, \u0026#39;PAN_MC\u0026#39;, \u0026#39;PAN_PRI_PRD\u0026#39;, \u0026#39;PAN_PRI\u0026#39;] Which means that the party PAN would also include the alliances with the other parties named Nueva Alianza, an alliance with PRD named PAN_PRD, etc.\nNOTE # write a dictionary with the alliances per party alliance_mapping = { \u0026#39;PAN\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PAN_NVA_ALIANZA\u0026#39;, \u0026#39;PAN_PRD\u0026#39;], \u0026#39;PRI\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PRI_PVEM\u0026#39;, \u0026#39;PRI_NA\u0026#39;, \u0026#39;PRI_PVEM_NA\u0026#39;, \u0026#39;PAN_PRI_PRD\u0026#39;, \u0026#39;PAN_PRI\u0026#39;, \u0026#39;PRI_PRD\u0026#39;], \u0026#39;PRD\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;PRD_PT\u0026#39;, \u0026#39;PAN_PRI_PRD\u0026#39;], \u0026#39;PVEM\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;PVEM_NA\u0026#39;, \u0026#39;PVEM_PT\u0026#39;], \u0026#39;PT\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;PT_ES\u0026#39;], \u0026#39;MC\u0026#39;: [\u0026#39;MC\u0026#39;, \u0026#39;PRD_PT_MC\u0026#39;, \u0026#39;PRD_MC\u0026#39;, \u0026#39;PT_MC\u0026#39;, \u0026#39;PAN_PRD_MC\u0026#39;, \u0026#39;PAN_MC\u0026#39;], \u0026#39;MORENA\u0026#39;: [\u0026#39;MORENA\u0026#39;, \u0026#39;PT_MORENA_ES\u0026#39;, \u0026#39;PT_MORENA\u0026#39;, \u0026#39;MORENA_ES\u0026#39;, \u0026#39;PVEM_PT_MORENA\u0026#39;, \u0026#39;PVEM_MORENA\u0026#39;], \u0026#39;NVA_ALIANZA\u0026#39;: [\u0026#39;NVA_ALIANZA\u0026#39;], \u0026#39;PSD\u0026#39;: [\u0026#39;PSD\u0026#39;], \u0026#39;PRIMERO_MEXICO\u0026#39;: [\u0026#39;PRIMERO_MEXICO\u0026#39;], \u0026#39;SALVEMOS_MEXICO\u0026#39;: [\u0026#39;SALVEMOS_MEXICO\u0026#39;], \u0026#39;PH\u0026#39;: [\u0026#39;PH\u0026#39;], \u0026#39;ES\u0026#39;: [\u0026#39;ES\u0026#39;], \u0026#39;NA\u0026#39;: [\u0026#39;NA\u0026#39;], \u0026#39;PES\u0026#39;: [\u0026#39;PES\u0026#39;], \u0026#39;RSP\u0026#39;: [\u0026#39;RSP\u0026#39;], \u0026#39;FXM\u0026#39;: [\u0026#39;FXM\u0026#39;], } main_parties = { \u0026#39;PAN\u0026#39;: \u0026#39;PAN\u0026#39;, \u0026#39;PRI\u0026#39;: \u0026#39;PRI\u0026#39;, \u0026#39;PRD\u0026#39;: \u0026#39;PRD\u0026#39;, \u0026#39;PVEM\u0026#39;: \u0026#39;PVEM\u0026#39;, \u0026#39;PT\u0026#39;: \u0026#39;PT\u0026#39;, \u0026#39;MC\u0026#39;: \u0026#39;MC\u0026#39;, \u0026#39;MORENA\u0026#39;: \u0026#39;MORENA\u0026#39;, \u0026#39;NVA_ALIANZA\u0026#39;: \u0026#39;NVA_ALIANZA\u0026#39;, \u0026#39;PSD\u0026#39;: [\u0026#39;PSD\u0026#39;], \u0026#39;PRIMERO_MEXICO\u0026#39;: [\u0026#39;PRIMERO_MEXICO\u0026#39;], \u0026#39;SALVEMOS_MEXICO\u0026#39;: [\u0026#39;SALVEMOS_MEXICO\u0026#39;], \u0026#39;PH\u0026#39;: [\u0026#39;PH\u0026#39;], \u0026#39;ES\u0026#39;: [\u0026#39;ES\u0026#39;], \u0026#39;NA\u0026#39;: [\u0026#39;NA\u0026#39;], \u0026#39;PES\u0026#39;: [\u0026#39;PES\u0026#39;], \u0026#39;RSP\u0026#39;: [\u0026#39;RSP\u0026#39;], \u0026#39;FXM\u0026#39;: [\u0026#39;FXM\u0026#39;], # Add more as needed for each unique party or alliance... } Vote Distribution Among Alliances The way this works is that the votes for any alliance are equally divided amongs the parties that conform the alliance.\ndf_re_2018_qroo.head() CIRCUNSCRIPCION\rID_ESTADO\rNOMBRE_ESTADO\rID_DISTRITO\rCABECERA_DISTRITAL\rID_MUNICIPIO\rMUNICIPIO\rSECCION\rCASILLAS\rPAN\r...\rPRI_NA\rPVEM_NA\rPT_MORENA_ES\rPT_MORENA\rPT_ES\rMORENA_ES\rNUM_VOTOS_CAN_NREG\rNUM_VOTOS_NULOS\rTOTAL_VOTOS\rLISTA_NOMINAL\r48293\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r1\rBENITO JUAREZ\r962\r3\r79.0\r...\r3.0\r0.0\r18.0\r10.0\r0.0\r7.0\r9.0\r25.0\r879.0\r1574\r48294\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r182\r3\r279.0\r...\r0.0\r0.0\r4.0\r5.0\r3.0\r5.0\r4.0\r50.0\r1275.0\r2116\r48295\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r183\r3\r190.0\r...\r1.0\r0.0\r4.0\r1.0\r0.0\r0.0\r1.0\r30.0\r1171.0\r2117\r48296\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r184\r3\r237.0\r...\r6.0\r3.0\r4.0\r5.0\r0.0\r0.0\r0.0\r64.0\r1227.0\r1919\r48297\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r185\r4\r229.0\r...\r2.0\r1.0\r7.0\r5.0\r1.0\r3.0\r0.0\r52.0\r1446.0\r2308\r5 rows × 34 columns\nalliance_votes_mapping = { \u0026#39;PAN_NVA_ALIANZA\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;NVA_ALIANZA\u0026#39;], \u0026#39;PAN_PRD\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRD\u0026#39;], \u0026#39;PRI_PVEM\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PVEM\u0026#39;], \u0026#39;PRI_NA\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;NA\u0026#39;], \u0026#39;PRI_PVEM_NA\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PVEM\u0026#39;, \u0026#39;NA\u0026#39;], \u0026#39;PAN_PRI_PRD\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRI\u0026#39;, \u0026#39;PRD\u0026#39;], \u0026#39;PAN_PRI\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRI\u0026#39;], \u0026#39;PRI_PRD\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PRD\u0026#39;], \u0026#39;PRD_PT\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;PT\u0026#39;], \u0026#39;PVEM_NA\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;NA\u0026#39;], \u0026#39;PVEM_PT\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;PT\u0026#39;], \u0026#39;PT_ES\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;ES\u0026#39;], \u0026#39;PRD_PT_MC\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;PT\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PRD_MC\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PT_MC\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PAN_PRD_MC\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRD\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PAN_MC\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;MORENA_ES\u0026#39;: [\u0026#39;MORENA\u0026#39;, \u0026#39;ES\u0026#39;], \u0026#39;PT_MORENA_ES\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;MORENA\u0026#39;, \u0026#39;ES\u0026#39;], \u0026#39;PT_MORENA\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;MORENA\u0026#39;], \u0026#39;PVEM_PT_MORENA\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;PT\u0026#39;, \u0026#39;MORENA\u0026#39;], \u0026#39;PVEM_MORENA\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;MORENA\u0026#39;], } def distribute_alliance_votes(df, alliances): # ensure that party columns exist in the dataframe, add them if the do not all_parties = set(party for parties in alliances.values() for party in parties) for party in all_parties: if party not in df.columns: df[party] = 0 # distribute the votes from each alliance to the respective parties for alliance, parties in alliances.items(): if alliance in df.columns: split_votes = df[alliance] / len(parties) for party in parties: df[party] += split_votes # optionally remove the alliance columns df.drop(columns = list(alliances.keys()), inplace = True, errors = \u0026#39;ignore\u0026#39;) return df # apply the vote split function to all the dataframes: df_re_2009_qroo = distribute_alliance_votes(df_re_2009_qroo, alliance_votes_mapping) df_re_2012_qroo = distribute_alliance_votes(df_re_2012_qroo, alliance_votes_mapping) df_re_2015_qroo = distribute_alliance_votes(df_re_2015_qroo, alliance_votes_mapping) df_re_2018_qroo = distribute_alliance_votes(df_re_2018_qroo, alliance_votes_mapping) df_re_2021_qroo = distribute_alliance_votes(df_re_2021_qroo, alliance_votes_mapping) df_re_2018_qroo.head() CIRCUNSCRIPCION\rID_ESTADO\rNOMBRE_ESTADO\rID_DISTRITO\rCABECERA_DISTRITAL\rID_MUNICIPIO\rMUNICIPIO\rSECCION\rCASILLAS\rPAN\r...\rPT\rMC\rNA\rMORENA\rES\rNUM_VOTOS_CAN_NREG\rNUM_VOTOS_NULOS\rTOTAL_VOTOS\rLISTA_NOMINAL\rNVA_ALIANZA\r48293\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r1\rBENITO JUAREZ\r962\r3\r80.166667\r...\r64.000000\r26.166667\r14.166667\r525.500000\r39.500000\r9.0\r25.0\r879.0\r1574\r0\r48294\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r182\r3\r281.000000\r...\r42.333333\r48.500000\r14.666667\r405.333333\r26.333333\r4.0\r50.0\r1275.0\r2116\r0\r48295\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r183\r3\r192.000000\r...\r44.833333\r61.000000\r14.166667\r445.833333\r24.333333\r1.0\r30.0\r1171.0\r2117\r0\r48296\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r184\r3\r240.000000\r...\r45.833333\r52.500000\r24.166667\r444.833333\r19.333333\r0.0\r64.0\r1227.0\r1919\r0\r48297\r3\r23\rQUINTANA ROO\r1\rPLAYA DEL CARMEN\r2\rCOZUMEL\r185\r4\r231.500000\r...\r43.333333\r62.000000\r19.500000\r502.333333\r24.333333\r0.0\r52.0\r1446.0\r2308\r0\r5 rows × 23 columns\n# list to store party names lists for each DataFrame party_names_per_df = [] for df in df_re_all_years: party_names = [] # empty list to store party names for current DataFrame # get a list of column names column_names = list(df.columns) try: # find the indices of \u0026#34;CASILLAS\u0026#34; and \u0026#34;NUM_VOTOS_CAN_NREG\u0026#34; (handle potential errors) casillas_index = column_names.index(\u0026#39;CASILLAS\u0026#39;) num_votos_index = column_names.index(\u0026#39;NUM_VOTOS_CAN_NREG\u0026#39;) # extract party names between the indices (avoid out-of-bounds) party_names = column_names[casillas_index + 1:min(num_votos_index, len(column_names))] except ValueError: # handle cases where \u0026#34;CASILLAS\u0026#34; or \u0026#34;NUM_VOTOS_CAN_NREG\u0026#34; might not exist print(f\u0026#34;\\nWARNING: \u0026#39;CASILLAS\u0026#39; or \u0026#39;NUM_VOTOS_CAN_NREG\u0026#39; not found in DataFrame\u0026#34;) # append the party names list for this DataFrame party_names_per_df.append(party_names) party_names_per_df [['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'CONV',\r'NVA_ALIANZA',\r'PSD',\r'PRIMERO_MEXICO',\r'SALVEMOS_MEXICO'],\r['PAN', 'PRI', 'PRD', 'PVEM', 'PT', 'MC', 'NVA_ALIANZA'],\r['PAN',\r'PRI',\r'PRD',\r'PVEM',\r'PT',\r'MC',\r'NVA_ALIANZA',\r'MORENA',\r'PH',\r'ES'],\r['PAN', 'PRI', 'PRD', 'PVEM', 'PT', 'MC', 'NA', 'MORENA', 'ES'],\r['PAN', 'PRI', 'PRD', 'PVEM', 'PT', 'MC', 'MORENA', 'PES', 'RSP', 'FXM']]\rVisualizations Time History Here we develop a function to plot the historical number of votes per party. Each plot will show the data for a selected municipality. The idea is to have an interactive dashboard where one could easily select these parameters and access the plot.\n# function for plotting the time history, given a eyar, and a municipality def plot_aggregated_votes_by_main_party_px(df_list, main_parties, selected_municipality, election_years): \u0026#34;\u0026#34;\u0026#34; Plots an interactive line plot with filled areas to zero for each main party and its alliances, in a selected municipality across elections using Plotly Express. This approximates the non-stacked area plot behavior of the original function. \u0026#34;\u0026#34;\u0026#34; # initialize dictionary to hold vote totals for main parties votes_by_main_party = {main_party: [0] * len(election_years) for main_party in main_parties} # loop through each DataFrame and year for i, (df, year) in enumerate(zip(df_list, election_years)): # filter the DataFrame for the selected municipality if selected_municipality in df[\u0026#39;MUNICIPIO\u0026#39;].values: filtered_df = df[df[\u0026#39;MUNICIPIO\u0026#39;] == selected_municipality] # loop through each main party and its alliances for party in main_parties: # aggregate votes for each party in the alliance, adding to the main party\u0026#39;s total if party in filtered_df.columns: votes_by_main_party[party][i] += filtered_df[party].sum() # prepare the data for plotting data_for_plotting = [] for main_party, votes in votes_by_main_party.items(): for year, vote in zip(election_years, votes): data_for_plotting.append({\u0026#39;Election Year\u0026#39;: year, \u0026#39;Total Votes\u0026#39;: vote, \u0026#39;Party\u0026#39;: main_party}) df_plot = pd.DataFrame(data_for_plotting) # create the plot fig = px.line(df_plot, x=\u0026#39;Election Year\u0026#39;, y=\u0026#39;Total Votes\u0026#39;, color=\u0026#39;Party\u0026#39;, line_shape=\u0026#39;linear\u0026#39;, title=f\u0026#39;Total Votes per Party (Including Alliances), in {selected_municipality}\u0026#39;) # customize the layout fig.update_traces(mode=\u0026#39;lines\u0026#39;, line=dict(width=2.5), fill=\u0026#39;tozeroy\u0026#39;) fig.update_layout(xaxis_title=\u0026#39;Election Year\u0026#39;, yaxis_title=\u0026#39;Total Votes\u0026#39;, legend_title=\u0026#39;Party\u0026#39;, font=dict(family=\u0026#34;Arial, sans-serif\u0026#34;, size=12, color=\u0026#34;#333\u0026#34;), hovermode=\u0026#39;x unified\u0026#39;, legend = dict( orientation = \u0026#39;h\u0026#39;, yanchor = \u0026#39;bottom\u0026#39;, y = -0.6, # adjuist to fit layout xanchor = \u0026#39;center\u0026#39;, x = 0.5 )) return fig Let\u0026rsquo;s look at an example, by calling the municipality called Benito Juarez.\nmunicipality = \u0026#39;BENITO JUAREZ\u0026#39; # as an example election_years = [year for year in range(2009, 2022, 3)] plot_aggregated_votes_by_main_party_px(df_re_all_years, main_parties, municipality, election_years) Pie Chart def plot_election_pie_chart(selected_year, selected_municipality, df_re_all_years, main_parties): # mapping years to their indices in the list of dataframes year_to_index = {2009: 0, 2012: 1, 2015:2, 2018: 3, 2021: 4} selected_year_index = year_to_index.get(selected_year) if selected_year_index is None: print(f\u0026#34;No data available for the year {selected_year}.\u0026#34;) return # extract the dataframe for the selected year df_selected_year = df_re_all_years[selected_year_index] # filtering the df for the selected municipality df_municipality = df_selected_year[df_selected_year[\u0026#39;MUNICIPIO\u0026#39;] == selected_municipality] if df_municipality.empty: print(f\u0026#39;No data available for {selected_municipality}.\u0026#39;) return # aggregating votes for each main party votes_by_party = {main_party: 0 for main_party in main_parties} for party in main_parties: if party in df_municipality.columns: votes_by_party[party] += df_municipality[party].sum() # create the pie chart df_votes = pd.DataFrame(list(votes_by_party.items()), columns = [\u0026#39;Party\u0026#39;, \u0026#39;Votes\u0026#39;]) fig = px.pie(df_votes, values = \u0026#39;Votes\u0026#39;, names = \u0026#39;Party\u0026#39;, title = f\u0026#39;Vote Distribution in {selected_municipality}, {selected_year}\u0026#39;) # Update the traces to remove the text labels fig.update_traces(textinfo=\u0026#39;none\u0026#39;, hoverinfo=\u0026#39;label+percent\u0026#39;) return fig plot_election_pie_chart(2012, municipality, df_re_all_years, main_parties) Choropleth Now we can build some choropleths. To do this, we collected a geojson file for the municipalities in the State of Quintana Roo, Mexico.\nThe file can be found in this GitHub repository.\n# define some colors for each party party_colors = { \u0026#39;PAN\u0026#39;: \u0026#39;#0052CC\u0026#39;, # Blue \u0026#39;PRI\u0026#39;: \u0026#39;#013369\u0026#39;, # Dark Blue \u0026#39;PRD\u0026#39;: \u0026#39;#FFD700\u0026#39;, # Gold \u0026#39;PVEM\u0026#39;: \u0026#39;#00A550\u0026#39;, # Green \u0026#39;PT\u0026#39;: \u0026#39;#E00000\u0026#39;, # Red \u0026#39;MC\u0026#39;: \u0026#39;#FF7F00\u0026#39;, # Orange \u0026#39;MORENA\u0026#39;: \u0026#39;#6813D5\u0026#39;, # Purple \u0026#39;NVA_ALIANZA\u0026#39;: \u0026#39;#00AAAA\u0026#39;, # Teal \u0026#39;PSD\u0026#39;: \u0026#39;#555555\u0026#39;, # Dark Gray \u0026#39;PRIMERO_MEXICO\u0026#39;: \u0026#39;#9C2AA0\u0026#39;, # Magenta \u0026#39;SALVEMOS_MEXICO\u0026#39;: \u0026#39;#6CACE4\u0026#39;, # Light Blue \u0026#39;PH\u0026#39;: \u0026#39;#F0A3A3\u0026#39;, # Pink \u0026#39;ES\u0026#39;: \u0026#39;#2AD2C9\u0026#39;, # Cyan \u0026#39;NA\u0026#39;: \u0026#39;#F68B1F\u0026#39;, # Amber \u0026#39;PES\u0026#39;: \u0026#39;#93C572\u0026#39;, # Lime \u0026#39;RSP\u0026#39;: \u0026#39;#CC317C\u0026#39;, # Rose \u0026#39;FXM\u0026#39;: \u0026#39;#8B4513\u0026#39;, # SaddleBrown # Add more entries for each party as needed... } Now, for this map to work, we need the data in the geojson file to coincide exactly with the names of the municipalities in the dataframes.\nFirst, we explore the names of the municipalities in the dataframe for the recent elections, since this is the dataframe that contains all of the municipalities, including the most recently incorporated (i.e. in earlier elections, some of these municipalities did not exist).\n# from aggregate df, we pull the latest one, and see the unique names for the column \u0026#39;MUNICIPIO\u0026#39; df_re_all_years[-1][\u0026#39;MUNICIPIO\u0026#39;].unique() array(['COZUMEL', 'SOLIDARIDAD', 'TULUM', 'ISLA MUJERES',\r'LAZARO CARDENAS', 'BENITO JUAREZ', 'FELIPE CARRILLO PUERTO',\r'JOSE MARIA MORELOS', 'OTHON P. BLANCO', 'BACALAR',\r'PUERTO MORELOS'], dtype=object)\rLikewise, we explore the goejson to see how the municipalities are named. Let\u0026rsquo;s explore the geojson file.\n# Load the GeoJSON file geojson_file_path = \u0026#39;utils/mexico-geojson/2022/states/Quintana Roo.json\u0026#39; with open(geojson_file_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: geojson_data = json.load(file) # Extract \u0026#34;NOM_MUN\u0026#34; values nom_mun_list = [feature[\u0026#39;properties\u0026#39;][\u0026#39;NOMGEO\u0026#39;] for feature in geojson_data[\u0026#39;features\u0026#39;]] # Print the list to see what values are stored print(nom_mun_list) ['Felipe Carrillo Puerto', 'Cozumel', 'Isla Mujeres', 'Othón P. Blanco', 'Solidaridad', 'Puerto Morelos', 'Benito Juárez', 'José María Morelos', 'Lázaro Cárdenas', 'Tulum', 'Bacalar']\rNOTE After trying a few things out, the geojson was not working as intended. This was due to a difference in the name encoding. To fix this, we adjusted the geojson file in the property properties.NOMGEO/\nWe will do this programmatically, and save a new file with the new names.\n# Define a mapping of GeoJSON names to desired names, based on your DataFrame # This is a manual step but only needs to be done once name_mapping = { \u0026#39;Felipe Carrillo Puerto\u0026#39;: \u0026#39;FELIPE CARRILLO PUERTO\u0026#39;, \u0026#39;Cozumel\u0026#39;: \u0026#39;COZUMEL\u0026#39;, \u0026#39;Isla Mujeres\u0026#39;: \u0026#39;ISLA MUJERES\u0026#39;, \u0026#39;Othón P. Blanco\u0026#39;: \u0026#39;OTHON P. BLANCO\u0026#39;, \u0026#39;Solidaridad\u0026#39;: \u0026#39;SOLIDARIDAD\u0026#39;, \u0026#39;Puerto Morelos\u0026#39;: \u0026#39;PUERTO MORELOS\u0026#39;, \u0026#39;Benito Juárez\u0026#39;: \u0026#39;BENITO JUAREZ\u0026#39;, \u0026#39;José María Morelos\u0026#39;: \u0026#39;JOSE MARIA MORELOS\u0026#39;, \u0026#39;Lázaro Cárdenas\u0026#39;: \u0026#39;LAZARO CARDENAS\u0026#39;, \u0026#39;Tulum\u0026#39;: \u0026#39;TULUM\u0026#39;, \u0026#39;Bacalar\u0026#39;: \u0026#39;BACALAR\u0026#39; } # Iterate over each feature and adjust the names for feature in geojson_data[\u0026#39;features\u0026#39;]: original_name = feature[\u0026#39;properties\u0026#39;][\u0026#39;NOMGEO\u0026#39;] if original_name in name_mapping: feature[\u0026#39;properties\u0026#39;][\u0026#39;NOMGEO\u0026#39;] = name_mapping[original_name] #------------------------------------------------------------------------------# #--------------------- Save the modified GeoJSON to a new file ----------------# #------------------------------------------------------------------------------# # modified_geojson_file_path = \u0026#39;qroo_geojson_2022.json\u0026#39; # with open(modified_geojson_file_path, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: # json.dump(geojson_data, file, ensure_ascii=False, indent=4) # create year mapping dictionary # Mapping each election year to its corresponding dataframe df_dict = { 2009: df_re_2009_qroo, 2012: df_re_2012_qroo, 2015: df_re_2015_qroo, 2018: df_re_2018_qroo, 2021: df_re_2021_qroo, } # load the new geojson file here election_years = [year for year in range(2009, 2022, 3)] geojson_file_path = \u0026#39;data/shapefiles/qroo_geojson_2022.json\u0026#39; with open(geojson_file_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: geojson_data = json.load(file) Choropleth for Winning Party per Municipality Now let\u0026rsquo;s create a function to generate the choropleth. We note that for each election year, there will be a different municipality map. Therefore, in some years, some municipalities will be missing altogether.\nThis choropleth will show the winning party per municipality, at a given year.\ndef create_winning_party_per_year_choropleth(selected_year, geojson, main_parties, df_dict): # This function now handles a single year\u0026#39;s DataFrame and generates a choropleth map for that year. df_year = df_dict[selected_year] winning_party_by_municipality = {} for municipality in df_year[\u0026#39;MUNICIPIO\u0026#39;].unique(): votes_by_party = {main_party: 0 for main_party in main_parties} # for main_party, parties in alliance_mapping.items(): # for party in parties: # if party in df_year.columns: # votes_by_party[main_party] += df_year.loc[df_year[\u0026#39;MUNICIPIO\u0026#39;] == municipality, party].sum() for party in main_parties: if party in df_year.columns: votes_by_party[party] += df_year.loc[df_year[\u0026#39;MUNICIPIO\u0026#39;] == municipality, party].sum() winning_party = max(votes_by_party, key=votes_by_party.get) winning_party_by_municipality[municipality] = winning_party df_map = pd.DataFrame(list(winning_party_by_municipality.items()), columns=[\u0026#39;MUNICIPIO\u0026#39;, \u0026#39;Partido Ganador\u0026#39;]) df_map[\u0026#39;Year\u0026#39;] = selected_year fig = px.choropleth( df_map, geojson=geojson, locations=\u0026#39;MUNICIPIO\u0026#39;, color=\u0026#39;Partido Ganador\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_discrete_map=party_colors, projection=\u0026#34;mercator\u0026#34;, ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) fig.update_layout(title=f\u0026#34;Winning Party per Municipality, {selected_year}\u0026#34;) return fig # Return the figure for this specific year # Example of how to call the function for a single year year = 2021 df_year = df_dict[year] # Assuming df_dict is defined with years as keys fig = create_winning_party_per_year_choropleth(year, geojson_data, main_parties, df_dict) fig.show() Maps Showing the Gender Proportion per Municipality For this choropleth, we will need the voter registration dataframe, so we call it again.\nprint(df_ln_sx_qroo.columns) df_ln_sx_qroo.head() Index(['Clave Entidad', 'Nombre Entidad', 'Clave Distrito', 'Nombre Distrito',\r'Clave Municipio', 'Nombre Municipio', 'Seccion', 'Lista Hombres',\r'Lista Mujeres', 'Lista No Binario', 'Lista Nominal'],\rdtype='object')\rClave Entidad\rNombre Entidad\rClave Distrito\rNombre Distrito\rClave Municipio\rNombre Municipio\rSeccion\rLista Hombres\rLista Mujeres\rLista No Binario\rLista Nominal\r50685\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r182.0\r1046\r1015\r0\r2061\r50686\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r183.0\r1056\r1085\r0\r2141\r50687\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r184.0\r982\r981\r0\r1963\r50688\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r185.0\r1228\r1198\r0\r2426\r50689\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r186.0\r525\r465\r0\r990\rdef create_gender_proportion_choropleth(df, geojson_data): # Aggregate data by MUNICIPIO if not already aggregated df_grouped = df.groupby(\u0026#39;Nombre Municipio\u0026#39;).sum().reset_index() # Calculate the percentage of women registered voters df_grouped[\u0026#39;Porcentaje Mujeres\u0026#39;] = (df_grouped[\u0026#39;Lista Mujeres\u0026#39;] / df_grouped[\u0026#39;Lista Nominal\u0026#39;]) * 100 # Assuming `geojson` is your GeoJSON object for the municipalities fig = px.choropleth( df_grouped, geojson=geojson_data, locations=\u0026#39;Nombre Municipio\u0026#39;, color=\u0026#39;Porcentaje Mujeres\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_continuous_scale=px.colors.sequential.Plasma, projection=\u0026#34;mercator\u0026#34;, title=\u0026#34;Percentage of Women in Voter Registration\u0026#34; ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) # Update layout for colorbar position fig.update_layout( coloraxis_colorbar=dict( title=\u0026#39;Women Percentage\u0026#39;, orientation=\u0026#39;h\u0026#39;, x=0.5, xanchor=\u0026#39;center\u0026#39;, y=-0.2, thickness=10, # Adjust the thickness of the colorbar len=0.65 # Set the length as a fraction of the plot area width ) ) return fig create_gender_proportion_choropleth(df_ln_sx_qroo, geojson_data) Map Grouped by Age Range df_age = pd.read_excel(\u0026#39;data/padron_y_ln_rango_edad.xlsx\u0026#39;) print(len(df_age.columns)) df_age.head() 87\rCLAVE\\nENTIDAD\rNOMBRE\\nENTIDAD\rCLAVE\\nDISTRITO\rNOMBRE\\nDISTRITO\rCLAVE\\nMUNICIPIO\rNOMBRE\\nMUNICIPIO\rSECCION\rPADRON\\nHOMBRES\rPADRON\\nMUJERES\rPADRON\\nNO BINARIO\r...\rLISTA_50_54_NOBINARIO\rLISTA_55_59_HOMBRES\rLISTA_55_59_MUJERES\rLISTA_55_59_NOBINARIO\rLISTA_60_64_HOMBRES\rLISTA_60_64_MUJERES\rLISTA_60_64_NOBINARIO\rLISTA_65_Y_MAS_HOMBRES\rLISTA_65_Y_MAS_MUJERES\rLISTA_65_Y_MAS_NOBINARIO\r0\r1\rRESIDENTES EXTRANJERO\r0.0\r0\r0.0\r0\r0.0\r8444\r5756\r0\r...\r0.0\r355.0\r234.0\r0.0\r180.0\r149.0\r0.0\r206.0\r139.0\r0.0\r1\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r338.0\r973\r1013\r0\r...\r0.0\r56.0\r72.0\r0.0\r39.0\r37.0\r0.0\r88.0\r109.0\r0.0\r2\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r339.0\r895\r954\r0\r...\r0.0\r55.0\r60.0\r0.0\r38.0\r43.0\r0.0\r88.0\r97.0\r0.0\r3\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r340.0\r951\r1001\r0\r...\r0.0\r56.0\r66.0\r0.0\r46.0\r48.0\r0.0\r103.0\r83.0\r0.0\r4\r1\rAGUASCALIENTES\r1.0\rJESUS MARIA ...\r2.0\rASIENTOS\r341.0\r1174\r1184\r0\r...\r0.0\r59.0\r60.0\r0.0\r50.0\r62.0\r0.0\r110.0\r105.0\r0.0\r5 rows × 87 columns\ndf_age.columns Index(['CLAVE\\nENTIDAD', 'NOMBRE\\nENTIDAD', 'CLAVE\\nDISTRITO',\r'NOMBRE\\nDISTRITO', 'CLAVE\\nMUNICIPIO', 'NOMBRE\\nMUNICIPIO', 'SECCION',\r'PADRON\\nHOMBRES', 'PADRON\\nMUJERES', 'PADRON\\nNO BINARIO',\r'PADRON\\nELECTORAL', 'LISTA\\nHOMBRES', 'LISTA\\nMUJERES',\r'LISTA\\nNO BINARIO', 'LISTA\\nNOMINAL', 'PADRON_18_HOMBRES',\r'PADRON_18_MUJERES', 'PADRON_18_NOBINARIO', 'PADRON_19_HOMBRES',\r'PADRON_19_MUJERES', 'PADRON_19_NOBINARIO', 'PADRON_20_24_HOMBRES',\r'PADRON_20_24_MUJERES', 'PADRON_20_24_NOBINARIO',\r'PADRON_25_29_HOMBRES', 'PADRON_25_29_MUJERES',\r'PADRON_25_29_NOBINARIO', 'PADRON_30_34_HOMBRES',\r'PADRON_30_34_MUJERES', 'PADRON_30_34_NOBINARIO',\r'PADRON_35_39_HOMBRES', 'PADRON_35_39_MUJERES',\r'PADRON_35_39_NOBINARIO', 'PADRON_40_44_HOMBRES',\r'PADRON_40_44_MUJERES', 'PADRON_40_44_NOBINARIO',\r'PADRON_45_49_HOMBRES', 'PADRON_45_49_MUJERES',\r'PADRON_45_49_NOBINARIO', 'PADRON_50_54_HOMBRES',\r'PADRON_50_54_MUJERES', 'PADRON_50_54_NOBINARIO',\r'PADRON_55_59_HOMBRES', 'PADRON_55_59_MUJERES',\r'PADRON_55_59_NOBINARIO', 'PADRON_60_64_HOMBRES',\r'PADRON_60_64_MUJERES', 'PADRON_60_64_NOBINARIO',\r'PADRON_65_Y_MAS_HOMBRES', 'PADRON_65_Y_MAS_MUJERES',\r'PADRON_65_Y_MAS_NOBINARIO', 'LISTA_18_HOMBRES', 'LISTA_18_MUJERES',\r'LISTA_18_NOBINARIO', 'LISTA_19_HOMBRES', 'LISTA_19_MUJERES',\r'LISTA_19_NOBINARIO', 'LISTA_20_24_HOMBRES', 'LISTA_20_24_MUJERES',\r'LISTA_20_24_NOBINARIO', 'LISTA_25_29_HOMBRES', 'LISTA_25_29_MUJERES',\r'LISTA_25_29_NOBINARIO', 'LISTA_30_34_HOMBRES', 'LISTA_30_34_MUJERES',\r'LISTA_30_34_NOBINARIO', 'LISTA_35_39_HOMBRES', 'LISTA_35_39_MUJERES',\r'LISTA_35_39_NOBINARIO', 'LISTA_40_44_HOMBRES', 'LISTA_40_44_MUJERES',\r'LISTA_40_44_NOBINARIO', 'LISTA_45_49_HOMBRES', 'LISTA_45_49_MUJERES',\r'LISTA_45_49_NOBINARIO', 'LISTA_50_54_HOMBRES', 'LISTA_50_54_MUJERES',\r'LISTA_50_54_NOBINARIO', 'LISTA_55_59_HOMBRES', 'LISTA_55_59_MUJERES',\r'LISTA_55_59_NOBINARIO', 'LISTA_60_64_HOMBRES', 'LISTA_60_64_MUJERES',\r'LISTA_60_64_NOBINARIO', 'LISTA_65_Y_MAS_HOMBRES',\r'LISTA_65_Y_MAS_MUJERES', 'LISTA_65_Y_MAS_NOBINARIO'],\rdtype='object')\r# Select columns by their positions: 1-7, 12-15, and the last 36 cols_to_keep = list(range(0, 7)) + list(range(11, 15)) + list(range(-36, 0)) # Now, select these columns from the DataFrame df_ln_age = df_age.iloc[:, cols_to_keep] # filter rows by state quintana roo df_ln_age_qroo = df_ln_age[df_ln_age[\u0026#39;NOMBRE\\nENTIDAD\u0026#39;] == \u0026#39;QUINTANA ROO\u0026#39;] # select columns for padron electoral # df_ln_age_qroo = df_age_qroo.iloc[:,:11] print(df_ln_age_qroo.columns) df_ln_age_qroo.head() Index(['CLAVE\\nENTIDAD', 'NOMBRE\\nENTIDAD', 'CLAVE\\nDISTRITO',\r'NOMBRE\\nDISTRITO', 'CLAVE\\nMUNICIPIO', 'NOMBRE\\nMUNICIPIO', 'SECCION',\r'LISTA\\nHOMBRES', 'LISTA\\nMUJERES', 'LISTA\\nNO BINARIO',\r'LISTA\\nNOMINAL', 'LISTA_18_HOMBRES', 'LISTA_18_MUJERES',\r'LISTA_18_NOBINARIO', 'LISTA_19_HOMBRES', 'LISTA_19_MUJERES',\r'LISTA_19_NOBINARIO', 'LISTA_20_24_HOMBRES', 'LISTA_20_24_MUJERES',\r'LISTA_20_24_NOBINARIO', 'LISTA_25_29_HOMBRES', 'LISTA_25_29_MUJERES',\r'LISTA_25_29_NOBINARIO', 'LISTA_30_34_HOMBRES', 'LISTA_30_34_MUJERES',\r'LISTA_30_34_NOBINARIO', 'LISTA_35_39_HOMBRES', 'LISTA_35_39_MUJERES',\r'LISTA_35_39_NOBINARIO', 'LISTA_40_44_HOMBRES', 'LISTA_40_44_MUJERES',\r'LISTA_40_44_NOBINARIO', 'LISTA_45_49_HOMBRES', 'LISTA_45_49_MUJERES',\r'LISTA_45_49_NOBINARIO', 'LISTA_50_54_HOMBRES', 'LISTA_50_54_MUJERES',\r'LISTA_50_54_NOBINARIO', 'LISTA_55_59_HOMBRES', 'LISTA_55_59_MUJERES',\r'LISTA_55_59_NOBINARIO', 'LISTA_60_64_HOMBRES', 'LISTA_60_64_MUJERES',\r'LISTA_60_64_NOBINARIO', 'LISTA_65_Y_MAS_HOMBRES',\r'LISTA_65_Y_MAS_MUJERES', 'LISTA_65_Y_MAS_NOBINARIO'],\rdtype='object')\rCLAVE\\nENTIDAD\rNOMBRE\\nENTIDAD\rCLAVE\\nDISTRITO\rNOMBRE\\nDISTRITO\rCLAVE\\nMUNICIPIO\rNOMBRE\\nMUNICIPIO\rSECCION\rLISTA\\nHOMBRES\rLISTA\\nMUJERES\rLISTA\\nNO BINARIO\r...\rLISTA_50_54_NOBINARIO\rLISTA_55_59_HOMBRES\rLISTA_55_59_MUJERES\rLISTA_55_59_NOBINARIO\rLISTA_60_64_HOMBRES\rLISTA_60_64_MUJERES\rLISTA_60_64_NOBINARIO\rLISTA_65_Y_MAS_HOMBRES\rLISTA_65_Y_MAS_MUJERES\rLISTA_65_Y_MAS_NOBINARIO\r50685\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r182.0\r1046\r1015\r0\r...\r0.0\r77.0\r77.0\r0.0\r69.0\r57.0\r0.0\r99.0\r111.0\r0.0\r50686\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r183.0\r1056\r1085\r0\r...\r0.0\r65.0\r68.0\r0.0\r50.0\r62.0\r0.0\r100.0\r97.0\r0.0\r50687\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r184.0\r982\r981\r0\r...\r0.0\r75.0\r65.0\r0.0\r43.0\r36.0\r0.0\r105.0\r112.0\r0.0\r50688\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r185.0\r1228\r1198\r0\r...\r0.0\r76.0\r83.0\r0.0\r50.0\r56.0\r0.0\r119.0\r113.0\r0.0\r50689\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r186.0\r525\r465\r0\r...\r0.0\r21.0\r30.0\r0.0\r29.0\r34.0\r0.0\r56.0\r40.0\r0.0\r5 rows × 47 columns\ndf_ln_age_qroo.dtypes CLAVE\\nENTIDAD object\rNOMBRE\\nENTIDAD object\rCLAVE\\nDISTRITO float64\rNOMBRE\\nDISTRITO object\rCLAVE\\nMUNICIPIO float64\rNOMBRE\\nMUNICIPIO object\rSECCION float64\rLISTA\\nHOMBRES int64\rLISTA\\nMUJERES int64\rLISTA\\nNO BINARIO int64\rLISTA\\nNOMINAL int64\rLISTA_18_HOMBRES float64\rLISTA_18_MUJERES float64\rLISTA_18_NOBINARIO float64\rLISTA_19_HOMBRES float64\rLISTA_19_MUJERES float64\rLISTA_19_NOBINARIO float64\rLISTA_20_24_HOMBRES float64\rLISTA_20_24_MUJERES float64\rLISTA_20_24_NOBINARIO float64\rLISTA_25_29_HOMBRES float64\rLISTA_25_29_MUJERES float64\rLISTA_25_29_NOBINARIO float64\rLISTA_30_34_HOMBRES float64\rLISTA_30_34_MUJERES float64\rLISTA_30_34_NOBINARIO float64\rLISTA_35_39_HOMBRES float64\rLISTA_35_39_MUJERES float64\rLISTA_35_39_NOBINARIO float64\rLISTA_40_44_HOMBRES float64\rLISTA_40_44_MUJERES float64\rLISTA_40_44_NOBINARIO float64\rLISTA_45_49_HOMBRES float64\rLISTA_45_49_MUJERES float64\rLISTA_45_49_NOBINARIO float64\rLISTA_50_54_HOMBRES float64\rLISTA_50_54_MUJERES float64\rLISTA_50_54_NOBINARIO float64\rLISTA_55_59_HOMBRES float64\rLISTA_55_59_MUJERES float64\rLISTA_55_59_NOBINARIO float64\rLISTA_60_64_HOMBRES float64\rLISTA_60_64_MUJERES float64\rLISTA_60_64_NOBINARIO float64\rLISTA_65_Y_MAS_HOMBRES float64\rLISTA_65_Y_MAS_MUJERES float64\rLISTA_65_Y_MAS_NOBINARIO float64\rdtype: object\rChange the Column Names to Avoid Special Characters ln_age_col_names = [\u0026#39;CLAVE ENTIDAD\u0026#39;, \u0026#39;NOMBRE ENTIDAD\u0026#39;, \u0026#39;CLAVE DISTRITO\u0026#39;, \u0026#39;NOMBRE DISTRITO\u0026#39;, \u0026#39;CLAVE MUNICIPIO\u0026#39;, \u0026#39;MUNICIPIO\u0026#39;, \u0026#39;SECCION\u0026#39;, \u0026#39;LISTA HOMBRES\u0026#39;, \u0026#39;LISTA MUJERES\u0026#39;, \u0026#39;LISTA NO BINARIO\u0026#39;, \u0026#39;LISTA NOMINAL\u0026#39;, \u0026#39;LISTA_18_HOMBRES\u0026#39;, \u0026#39;LISTA_18_MUJERES\u0026#39;, \u0026#39;LISTA_18_NOBINARIO\u0026#39;, \u0026#39;LISTA_19_HOMBRES\u0026#39;, \u0026#39;LISTA_19_MUJERES\u0026#39;, \u0026#39;LISTA_19_NOBINARIO\u0026#39;, \u0026#39;LISTA_20_24_HOMBRES\u0026#39;, \u0026#39;LISTA_20_24_MUJERES\u0026#39;, \u0026#39;LISTA_20_24_NOBINARIO\u0026#39;, \u0026#39;LISTA_25_29_HOMBRES\u0026#39;, \u0026#39;LISTA_25_29_MUJERES\u0026#39;, \u0026#39;LISTA_25_29_NOBINARIO\u0026#39;, \u0026#39;LISTA_30_34_HOMBRES\u0026#39;, \u0026#39;LISTA_30_34_MUJERES\u0026#39;, \u0026#39;LISTA_30_34_NOBINARIO\u0026#39;, \u0026#39;LISTA_35_39_HOMBRES\u0026#39;, \u0026#39;LISTA_35_39_MUJERES\u0026#39;, \u0026#39;LISTA_35_39_NOBINARIO\u0026#39;, \u0026#39;LISTA_40_44_HOMBRES\u0026#39;, \u0026#39;LISTA_40_44_MUJERES\u0026#39;, \u0026#39;LISTA_40_44_NOBINARIO\u0026#39;, \u0026#39;LISTA_45_49_HOMBRES\u0026#39;, \u0026#39;LISTA_45_49_MUJERES\u0026#39;, \u0026#39;LISTA_45_49_NOBINARIO\u0026#39;, \u0026#39;LISTA_50_54_HOMBRES\u0026#39;, \u0026#39;LISTA_50_54_MUJERES\u0026#39;, \u0026#39;LISTA_50_54_NOBINARIO\u0026#39;, \u0026#39;LISTA_55_59_HOMBRES\u0026#39;, \u0026#39;LISTA_55_59_MUJERES\u0026#39;, \u0026#39;LISTA_55_59_NOBINARIO\u0026#39;, \u0026#39;LISTA_60_64_HOMBRES\u0026#39;, \u0026#39;LISTA_60_64_MUJERES\u0026#39;, \u0026#39;LISTA_60_64_NOBINARIO\u0026#39;, \u0026#39;LISTA_65_Y_MAS_HOMBRES\u0026#39;, \u0026#39;LISTA_65_Y_MAS_MUJERES\u0026#39;, \u0026#39;LISTA_65_Y_MAS_NOBINARIO\u0026#39;] df_ln_age_qroo.columns = ln_age_col_names df_ln_age_qroo[[\u0026#39;CLAVE ENTIDAD\u0026#39;, \u0026#39;NOMBRE ENTIDAD\u0026#39;, \u0026#39;NOMBRE DISTRITO\u0026#39;]].astype(str) df_ln_age_qroo.head() CLAVE ENTIDAD\rNOMBRE ENTIDAD\rCLAVE DISTRITO\rNOMBRE DISTRITO\rCLAVE MUNICIPIO\rMUNICIPIO\rSECCION\rLISTA HOMBRES\rLISTA MUJERES\rLISTA NO BINARIO\r...\rLISTA_50_54_NOBINARIO\rLISTA_55_59_HOMBRES\rLISTA_55_59_MUJERES\rLISTA_55_59_NOBINARIO\rLISTA_60_64_HOMBRES\rLISTA_60_64_MUJERES\rLISTA_60_64_NOBINARIO\rLISTA_65_Y_MAS_HOMBRES\rLISTA_65_Y_MAS_MUJERES\rLISTA_65_Y_MAS_NOBINARIO\r50685\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r182.0\r1046\r1015\r0\r...\r0.0\r77.0\r77.0\r0.0\r69.0\r57.0\r0.0\r99.0\r111.0\r0.0\r50686\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r183.0\r1056\r1085\r0\r...\r0.0\r65.0\r68.0\r0.0\r50.0\r62.0\r0.0\r100.0\r97.0\r0.0\r50687\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r184.0\r982\r981\r0\r...\r0.0\r75.0\r65.0\r0.0\r43.0\r36.0\r0.0\r105.0\r112.0\r0.0\r50688\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r185.0\r1228\r1198\r0\r...\r0.0\r76.0\r83.0\r0.0\r50.0\r56.0\r0.0\r119.0\r113.0\r0.0\r50689\r23\rQUINTANA ROO\r1.0\rSOLIDARIDAD ...\r2.0\rCOZUMEL\r186.0\r525\r465\r0\r...\r0.0\r21.0\r30.0\r0.0\r29.0\r34.0\r0.0\r56.0\r40.0\r0.0\r5 rows × 47 columns\n# refactor into a function for later use on the dashboard def create_age_choropleth(df, geojson): # Aggregate data by MUNICIPIO df_grouped = df.groupby(\u0026#39;MUNICIPIO\u0026#39;).sum().reset_index() # Determine the predominant age range for each municipality age_groups = df_grouped.columns[11:] df_grouped[\u0026#39;Rango de Edad Predominante\u0026#39;] = df_grouped[age_groups].idxmax(axis=1) # when summing, pandas also concatenates the strings in \u0026#34;NOMBRE ENTIDAD\u0026#34; # so do some housekeeping df_grouped.drop(columns=[\u0026#39;NOMBRE ENTIDAD\u0026#39;]) # Assuming `geojson` is your GeoJSON object for the municipalities fig = px.choropleth( df_grouped, geojson=geojson, locations=\u0026#39;MUNICIPIO\u0026#39;, color=\u0026#39;Rango de Edad Predominante\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_continuous_scale=px.colors.sequential.Plasma, projection=\u0026#34;mercator\u0026#34;, title=\u0026#34;Predominant Gender and Age Range in Voter Registration\u0026#34; ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) return fig create_age_choropleth(df_ln_age_qroo, geojson_data) Total Voter Registration per Municipality We will use the data in the column LISTA NOMINAL (i.e. the number of registered voters) from the dataframe df_ln_sx_qroo. Based on this, we\u0026rsquo;ll obtain the totals per municipality so we can visualize them.\nLet\u0026rsquo;s get a refresher on the column names.\ndf_ln_sx_qroo.columns Index(['Clave Entidad', 'Nombre Entidad', 'Clave Distrito', 'Nombre Distrito',\r'Clave Municipio', 'Nombre Municipio', 'Seccion', 'Lista Hombres',\r'Lista Mujeres', 'Lista No Binario', 'Lista Nominal'],\rdtype='object')\r# refactor into a function for later use on dashboard def create_total_bar_plot(df): # group data df_ln_qroo_totals = df.groupby([\u0026#39;Nombre Municipio\u0026#39;])[[\u0026#39;Lista Hombres\u0026#39;, \u0026#39;Lista Mujeres\u0026#39;, \u0026#39;Lista Nominal\u0026#39;]].sum().reset_index() fig_bar_totals = px.bar( df_ln_qroo_totals, x=\u0026#39;Nombre Municipio\u0026#39;, y=[\u0026#39;Lista Hombres\u0026#39;,\u0026#39;Lista Mujeres\u0026#39;], labels = {\u0026#39;value\u0026#39;: \u0026#39;Lista Nominal\u0026#39;, \u0026#39;variable\u0026#39;: \u0026#39;\u0026#39;}, title=\u0026#34;Registered Voters per Municipality\u0026#34;, color_discrete_sequence=px.colors.qualitative.Dark24 ) # make a dictionary for abbreviated municipality names abb_mun_dict = { \u0026#39;BACALAR\u0026#39;: \u0026#39;BCL\u0026#39;, \u0026#39;BENITO JUAREZ\u0026#39;: \u0026#39;BJ\u0026#39;, \u0026#39;COZUMEL\u0026#39;: \u0026#39;CZ\u0026#39;, \u0026#39;FELIPE CARRILLO PUERTO\u0026#39;: \u0026#39;FCP\u0026#39;, \u0026#39;ISLA MUJERES\u0026#39;: \u0026#39;IM\u0026#39;, \u0026#39;JOSE MARIA MORELOS\u0026#39;: \u0026#39;JMM\u0026#39;, \u0026#39;LAZARO CARDENAS\u0026#39;: \u0026#39;LC\u0026#39;, \u0026#39;OTHON P. BLANCO\u0026#39;: \u0026#39;OPB\u0026#39;, \u0026#39;PUERTO MORELOS\u0026#39;: \u0026#39;PM\u0026#39;, \u0026#39;SOLIDARIDAD\u0026#39;: \u0026#39;SLD\u0026#39;, \u0026#39;TULUM\u0026#39;: \u0026#39;TLM\u0026#39; } fig_bar_totals.update_layout( xaxis = dict( tickvals = df_ln_qroo_totals[\u0026#39;Nombre Municipio\u0026#39;], # Original names ticktext = [abb_mun_dict.get(name, name) for name in df_ln_qroo_totals[\u0026#39;Nombre Municipio\u0026#39;]] # Abbreviated names ), yaxis = dict(title = \u0026#39;Registered Voters\u0026#39;), plot_bgcolor = \u0026#39;rgba(0,0,0,0)\u0026#39;, # transparent background uniformtext_minsize = 8, # ensure text size is legible uniformtext_mode = \u0026#39;hide\u0026#39;, # hide text if it doesn\u0026#39;t fit ) fig_bar_totals.update_traces( hoverinfo=\u0026#39;x+y\u0026#39;, # Show the municipio name and the count on hover hovertemplate=\u0026#34;\u0026lt;b\u0026gt;%{x}\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;Total: %{y}\u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt;\u0026#34; # Custom hover template ) return fig_bar_totals create_total_bar_plot(df_ln_sx_qroo) # refactor into a function for later use on dashboard def create_total_choropleth(df, geojson): df_ln_qroo_totals = df.groupby([\u0026#39;Nombre Municipio\u0026#39;])[[\u0026#39;Lista Hombres\u0026#39;, \u0026#39;Lista Mujeres\u0026#39;, \u0026#39;Lista Nominal\u0026#39;]].sum().reset_index() fig_choropleth_totals = px.choropleth(df_ln_qroo_totals, geojson=geojson, locations=\u0026#39;Nombre Municipio\u0026#39;, color=\u0026#39;Lista Nominal\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, # Adjust based on your GeoJSON properties projection=\u0026#34;mercator\u0026#34;, color_continuous_scale=\u0026#34;Portland\u0026#34;, title=\u0026#34;Registered Voters per Municipality\u0026#34;) fig_choropleth_totals.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) fig_choropleth_totals.update_layout( coloraxis_colorbar = dict( title = \u0026#39;Total Registered Voters\u0026#39;, orientation= \u0026#39;h\u0026#39;, x = 0.5, xanchor = \u0026#39;center\u0026#39;, y = -0.2, thickness = 10, len = 0.65 ) ) return fig_choropleth_totals create_total_choropleth(df_ln_sx_qroo, geojson_data) Map with Voter Proportion vs Total Registered Voters per Municipality To create a map with the proportion of voters vs the total registered voters, we need the dataframe with the election results. In these dataframes, there are columns with the total of people that went out to vote.\ndf_grouped = df_re_2021_qroo.groupby(\u0026#39;MUNICIPIO\u0026#39;).agg({ \u0026#39;TOTAL_VOTOS\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;LISTA_NOMINAL\u0026#39;: \u0026#39;sum\u0026#39; }).reset_index() df_grouped[\u0026#39;Porcentaje\u0026#39;] = df_grouped[\u0026#39;TOTAL_VOTOS\u0026#39;] / df_grouped[\u0026#39;LISTA_NOMINAL\u0026#39;] df_grouped MUNICIPIO\rTOTAL_VOTOS\rLISTA_NOMINAL\rPorcentaje\r0\rBACALAR\r19204.0\r31916\r0.601704\r1\rBENITO JUAREZ\r245654.0\r630987\r0.389317\r2\rCOZUMEL\r38876.0\r70987\r0.547650\r3\rFELIPE CARRILLO PUERTO\r36950.0\r55894\r0.661073\r4\rISLA MUJERES\r13456.0\r21729\r0.619265\r5\rJOSE MARIA MORELOS\r21985.0\r27940\r0.786865\r6\rLAZARO CARDENAS\r16346.0\r21981\r0.743642\r7\rOTHON P. BLANCO\r79889.0\r174372\r0.458153\r8\rPUERTO MORELOS\r11189.0\r21716\r0.515242\r9\rSOLIDARIDAD\r80806.0\r229306\r0.352394\r10\rTULUM\r21607.0\r35739\r0.604578\rselected_year = 2015 def create_voter_turnout_proportion_choropleth(df_resultados, selected_year, geojson_data): # Aggregate data by MUNICIPIO if not already aggregated df_grouped = df_resultados.groupby(\u0026#39;MUNICIPIO\u0026#39;).agg({ \u0026#39;TOTAL_VOTOS\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;LISTA_NOMINAL\u0026#39;: \u0026#39;sum\u0026#39; }).reset_index() df_grouped[\u0026#39;Porcentaje Votantes\u0026#39;] = df_grouped[\u0026#39;TOTAL_VOTOS\u0026#39;] / df_grouped[\u0026#39;LISTA_NOMINAL\u0026#39;] * 100 # Assuming `geojson` is your GeoJSON object for the municipalities fig = px.choropleth( df_grouped, geojson=geojson_data, locations=\u0026#39;MUNICIPIO\u0026#39;, color=\u0026#39;Porcentaje Votantes\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_continuous_scale=px.colors.sequential.YlOrRd, projection=\u0026#34;mercator\u0026#34;, title=f\u0026#34;Total Voter Turnout in {selected_year}\u0026#34; ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) # Update layout for colorbar position fig.update_layout( coloraxis_colorbar=dict( title=(f\u0026#39;Voter Turnout Percentage in {selected_year}\u0026#39;), orientation=\u0026#39;h\u0026#39;, x=0.5, xanchor=\u0026#39;center\u0026#39;, y=-0.2, thickness=10, # Adjust the thickness of the colorbar len=0.65 # Set the length as a fraction of the plot area width ) ) return fig create_voter_turnout_proportion_choropleth(df_re_2018_qroo, selected_year, geojson_data) ","permalink":"http://localhost:1313/posts/20240509_election_dash_part_1-data_cleaning/election_dash_part_1-data_cleaning/","summary":"In this project, election data is collected, explored and cleaned. Visualization functions are refactored to be used on a dashboard later on.","title":"Election Data Dashboard Pt. 1: Collection and Cleaning"},{"content":" Creating the Dashboard using dash The cleaned data is loadsed and the plot generating functions are refactored here. Then a dashboard is created using dash.\nThis dashboard is deployed (at the time of writing this post) here:\nhttps://elections-dash.onrender.com/\nHere are some screenshots of the final dashboard, in case the deployed dashboard takes took long to load or is unavailable:\nHere is the full code:\nimport requests import pandas as pd import dash from dash import Dash, dcc, html, Input, Output import plotly.express as px import dash_bootstrap_components as dbc # generate variables and constants election_years = [year for year in range(2009, 2022, 3)] # Mapping each election year to its corresponding dataframe def distribute_alliance_votes(df, alliances): # ensure that party columns exist in the dataframe, add them if the do not all_parties = set(party for parties in alliances.values() for party in parties) for party in all_parties: if party not in df.columns: df[party] = 0 # distribute the votes from each alliance to the respective parties for alliance, parties in alliances.items(): if alliance in df.columns: split_votes = df[alliance] / len(parties) for party in parties: df[party] += split_votes # optionally remove the alliance columns df.drop(columns = list(alliances.keys()), inplace = True, errors = \u0026#39;ignore\u0026#39;) return df main_parties = { \u0026#39;PAN\u0026#39;: \u0026#39;PAN\u0026#39;, \u0026#39;PRI\u0026#39;: \u0026#39;PRI\u0026#39;, \u0026#39;PRD\u0026#39;: \u0026#39;PRD\u0026#39;, \u0026#39;PVEM\u0026#39;: \u0026#39;PVEM\u0026#39;, \u0026#39;PT\u0026#39;: \u0026#39;PT\u0026#39;, \u0026#39;MC\u0026#39;: \u0026#39;MC\u0026#39;, \u0026#39;MORENA\u0026#39;: \u0026#39;MORENA\u0026#39;, \u0026#39;NVA_ALIANZA\u0026#39;: \u0026#39;NVA_ALIANZA\u0026#39;, \u0026#39;PSD\u0026#39;: [\u0026#39;PSD\u0026#39;], \u0026#39;PRIMERO_MEXICO\u0026#39;: [\u0026#39;PRIMERO_MEXICO\u0026#39;], \u0026#39;SALVEMOS_MEXICO\u0026#39;: [\u0026#39;SALVEMOS_MEXICO\u0026#39;], \u0026#39;PH\u0026#39;: [\u0026#39;PH\u0026#39;], \u0026#39;ES\u0026#39;: [\u0026#39;ES\u0026#39;], \u0026#39;NA\u0026#39;: [\u0026#39;NA\u0026#39;], \u0026#39;PES\u0026#39;: [\u0026#39;PES\u0026#39;], \u0026#39;RSP\u0026#39;: [\u0026#39;RSP\u0026#39;], \u0026#39;FXM\u0026#39;: [\u0026#39;FXM\u0026#39;], # Add more as needed for each unique party or alliance... } alliance_votes_mapping = { \u0026#39;PAN_NVA_ALIANZA\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;NVA_ALIANZA\u0026#39;], \u0026#39;PAN_PRD\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRD\u0026#39;], \u0026#39;PRI_PVEM\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PVEM\u0026#39;], \u0026#39;PRI_NA\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;NA\u0026#39;], \u0026#39;PRI_PVEM_NA\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PVEM\u0026#39;, \u0026#39;NA\u0026#39;], \u0026#39;PAN_PRI_PRD\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRI\u0026#39;, \u0026#39;PRD\u0026#39;], \u0026#39;PAN_PRI\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRI\u0026#39;], \u0026#39;PRI_PRD\u0026#39;: [\u0026#39;PRI\u0026#39;, \u0026#39;PRD\u0026#39;], \u0026#39;PRD_PT\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;PT\u0026#39;], \u0026#39;PVEM_NA\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;NA\u0026#39;], \u0026#39;PVEM_PT\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;PT\u0026#39;], \u0026#39;PT_ES\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;ES\u0026#39;], \u0026#39;PRD_PT_MC\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;PT\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PRD_MC\u0026#39;: [\u0026#39;PRD\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PT_MC\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PAN_PRD_MC\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;PRD\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;PAN_MC\u0026#39;: [\u0026#39;PAN\u0026#39;, \u0026#39;MC\u0026#39;], \u0026#39;MORENA_ES\u0026#39;: [\u0026#39;MORENA\u0026#39;, \u0026#39;ES\u0026#39;], \u0026#39;PT_MORENA_ES\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;MORENA\u0026#39;, \u0026#39;ES\u0026#39;], \u0026#39;PT_MORENA\u0026#39;: [\u0026#39;PT\u0026#39;, \u0026#39;MORENA\u0026#39;], \u0026#39;PVEM_PT_MORENA\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;PT\u0026#39;, \u0026#39;MORENA\u0026#39;], \u0026#39;PVEM_MORENA\u0026#39;: [\u0026#39;PVEM\u0026#39;, \u0026#39;MORENA\u0026#39;], # Add any other specific alliances as needed } # load data # URL of the GeoJSON file on GitHub geojson_url = \u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/qroo_geojson_2022.json\u0026#39; # Fetch the GeoJSON data response = requests.get(geojson_url) geojson_data = response.json() if response.status_code == 200 else None df_ln_sx_qroo = pd.read_csv(\u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_lista_nominal_sexo.csv\u0026#39;) df_ln_age_qroo = pd.read_csv(\u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_lista_nominal_edad.csv\u0026#39;) csv_urls = [ \u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_results_2009.csv\u0026#39;, \u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_results_2012.csv\u0026#39;, \u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_results_2015.csv\u0026#39;, \u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_results_2018.csv\u0026#39;, \u0026#39;https://raw.githubusercontent.com/vflores-io/elections_dash/main/data/cleaned_results_2021.csv\u0026#39; # Add more URLs as needed ] # Load each CSV file into a DataFrame df_re_2009_qroo = pd.read_csv(csv_urls[0]) df_re_2012_qroo = pd.read_csv(csv_urls[1]) df_re_2015_qroo = pd.read_csv(csv_urls[2]) df_re_2018_qroo = pd.read_csv(csv_urls[3]) df_re_2021_qroo = pd.read_csv(csv_urls[4]) df_re_2009_qroo = distribute_alliance_votes(df_re_2009_qroo, alliance_votes_mapping) df_re_2012_qroo = distribute_alliance_votes(df_re_2012_qroo, alliance_votes_mapping) df_re_2015_qroo = distribute_alliance_votes(df_re_2015_qroo, alliance_votes_mapping) df_re_2018_qroo = distribute_alliance_votes(df_re_2018_qroo, alliance_votes_mapping) df_re_2021_qroo = distribute_alliance_votes(df_re_2021_qroo, alliance_votes_mapping) df_re_all_years = [df_re_2009_qroo, df_re_2012_qroo, df_re_2015_qroo, df_re_2018_qroo, df_re_2021_qroo] df_dict = { 2009: df_re_2009_qroo, 2012: df_re_2012_qroo, 2015: df_re_2015_qroo, 2018: df_re_2018_qroo, 2021: df_re_2021_qroo, } def create_total_bar_plot(df): df.rename(columns={\u0026#39;Lista Hombres\u0026#39;: \u0026#39;Registered Men\u0026#39;, \u0026#39;Lista Mujeres\u0026#39;: \u0026#39;Registered Women\u0026#39;, \u0026#39;Lista Nominal\u0026#39;: \u0026#39;Total Registered\u0026#39;}, inplace = True) # group data df_ln_qroo_totals = df.groupby([\u0026#39;Nombre Municipio\u0026#39;])[[\u0026#39;Registered Men\u0026#39;, \u0026#39;Registered Women\u0026#39;, \u0026#39;Total Registered\u0026#39;]].sum().reset_index() fig_bar_totals = px.bar( df_ln_qroo_totals, x=\u0026#39;Nombre Municipio\u0026#39;, y=[\u0026#39;Registered Men\u0026#39;, \u0026#39;Registered Women\u0026#39;], labels = {\u0026#39;value\u0026#39;: \u0026#39;Lista Nominal\u0026#39;, \u0026#39;variable\u0026#39;: \u0026#39;\u0026#39;}, title=\u0026#34;Registered Voters per Municipality\u0026#34;, color_discrete_sequence=px.colors.qualitative.T10 ) # make a dictionary for abbreviated municipality names abb_mun_dict = { \u0026#39;BACALAR\u0026#39;: \u0026#39;BCL\u0026#39;, \u0026#39;BENITO JUAREZ\u0026#39;: \u0026#39;BJ\u0026#39;, \u0026#39;COZUMEL\u0026#39;: \u0026#39;CZ\u0026#39;, \u0026#39;FELIPE CARRILLO PUERTO\u0026#39;: \u0026#39;FCP\u0026#39;, \u0026#39;ISLA MUJERES\u0026#39;: \u0026#39;IM\u0026#39;, \u0026#39;JOSE MARIA MORELOS\u0026#39;: \u0026#39;JMM\u0026#39;, \u0026#39;LAZARO CARDENAS\u0026#39;: \u0026#39;LC\u0026#39;, \u0026#39;OTHON P. BLANCO\u0026#39;: \u0026#39;OPB\u0026#39;, \u0026#39;PUERTO MORELOS\u0026#39;: \u0026#39;PM\u0026#39;, \u0026#39;SOLIDARIDAD\u0026#39;: \u0026#39;SLD\u0026#39;, \u0026#39;TULUM\u0026#39;: \u0026#39;TLM\u0026#39; } fig_bar_totals.update_layout( xaxis = dict( tickvals = df_ln_qroo_totals[\u0026#39;Nombre Municipio\u0026#39;], # Original names ticktext = [abb_mun_dict.get(name, name) for name in df_ln_qroo_totals[\u0026#39;Nombre Municipio\u0026#39;]] # Abbreviated names ), yaxis = dict(title = \u0026#39;Registered Voters\u0026#39;), plot_bgcolor = \u0026#39;rgba(0,0,0,0)\u0026#39;, # transparent background uniformtext_minsize = 8, # ensure text size is legible uniformtext_mode = \u0026#39;hide\u0026#39;, # hide text if it doesn\u0026#39;t fit ) fig_bar_totals.update_traces( hoverinfo=\u0026#39;x+y\u0026#39;, # Show the municipio name and the count on hover hovertemplate=\u0026#34;\u0026lt;b\u0026gt;%{x}\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;Total: %{y}\u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt;\u0026#34; # Custom hover template ) return fig_bar_totals def create_total_choropleth(df, geojson): df_ln_qroo_totals = df.groupby([\u0026#39;Nombre Municipio\u0026#39;])[[\u0026#39;Lista Nominal\u0026#39;]].sum().reset_index() ochre_scale = [ [0.0, \u0026#39;#4c78c8\u0026#39;], # Blue [1.0, \u0026#39;#f58518\u0026#39;], # Light ochre (yellow) ] fig_choropleth_totals = px.choropleth(df_ln_qroo_totals, geojson=geojson, locations=\u0026#39;Nombre Municipio\u0026#39;, color=\u0026#39;Lista Nominal\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, # Adjust based on your GeoJSON properties projection=\u0026#34;mercator\u0026#34;, color_continuous_scale=ochre_scale, title=\u0026#34;Registered Voters per Municipality\u0026#34;) fig_choropleth_totals.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) fig_choropleth_totals.update_layout( coloraxis_colorbar = dict( title = \u0026#39;Total Registered Voters\u0026#39;, orientation= \u0026#39;h\u0026#39;, x = 0.5, xanchor = \u0026#39;center\u0026#39;, y = -0.2, thickness = 10, len = 0.65 ) ) return fig_choropleth_totals #-------------------------------------------------------------------------- def create_age_choropleth(df, geojson): # Aggregate data by MUNICIPIO df_grouped = df.groupby(\u0026#39;MUNICIPIO\u0026#39;).sum().reset_index() # Determine the predominant age range for each municipality # age_groups = df_grouped.columns[11:] age_groups = [\u0026#39;Men 18\u0026#39;, \u0026#39;Women 18\u0026#39;, \u0026#39;Non-Binary 18\u0026#39;, \u0026#39;Men 19\u0026#39;, \u0026#39;Women 19\u0026#39;, \u0026#39;Non-Binary 19\u0026#39;, \u0026#39;Men 20-24\u0026#39;, \u0026#39;Women 20-24\u0026#39;, \u0026#39;Non-Binary 20-24\u0026#39;, \u0026#39;Men 25-29\u0026#39;, \u0026#39;Women 25-29\u0026#39;, \u0026#39;Non-Binary 25-29\u0026#39;, \u0026#39;Men 30-34\u0026#39;, \u0026#39;Women 30-34\u0026#39;, \u0026#39;Non-Binary 30-34\u0026#39;, \u0026#39;Men 35-39\u0026#39;, \u0026#39;Women 35-39\u0026#39;, \u0026#39;Non-Binary 35-39\u0026#39;, \u0026#39;Men 40-44\u0026#39;, \u0026#39;Women 40-44\u0026#39;, \u0026#39;Non-Binary 40-44\u0026#39;, \u0026#39;Men 45-49\u0026#39;, \u0026#39;Women 45-49\u0026#39;, \u0026#39;Non-Binary 45-49\u0026#39;, \u0026#39;Men 50-54\u0026#39;, \u0026#39;Women 50-54\u0026#39;, \u0026#39;Non-Binary 50-54\u0026#39;, \u0026#39;Men 55-59\u0026#39;, \u0026#39;Women 55-59\u0026#39;, \u0026#39;Non-Binary 55-59\u0026#39;, \u0026#39;Men 60-64\u0026#39;, \u0026#39;Women 60-64\u0026#39;, \u0026#39;Non-Binary 60-64\u0026#39;, \u0026#39;Men 65+\u0026#39;, \u0026#39;Women 65+\u0026#39;, \u0026#39;Non-Binary 65+\u0026#39;] # Rename the columns from the 12th column onwards df_grouped.columns = list(df_grouped.columns[:11]) + age_groups[:len(df_grouped.columns) - 11] df_grouped[\u0026#39;Predominant Age and Gender Group\u0026#39;] = df_grouped[age_groups].idxmax(axis=1) # when summing, pandas also concatenates the strings in \u0026#34;NOMBRE ENTIDAD\u0026#34; # so do some housekeeping df_grouped.drop(columns=[\u0026#39;NOMBRE ENTIDAD\u0026#39;]) # Assuming `geojson` is your GeoJSON object for the municipalities fig = px.choropleth( df_grouped, geojson=geojson, locations=\u0026#39;MUNICIPIO\u0026#39;, color=\u0026#39;Predominant Age and Gender Group\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_discrete_sequence=px.colors.qualitative.T10, projection=\u0026#34;mercator\u0026#34;, title=\u0026#34;Predominant Gender and Age Range in Voter Registration\u0026#34; ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) return fig #------------------------------------------------------------------------------------- def create_gender_proportion_choropleth(df, geojson_data): # Aggregate data by MUNICIPIO if not already aggregated df_grouped = df.groupby(\u0026#39;Nombre Municipio\u0026#39;).sum().reset_index() # Calculate the percentage of women registered voters df_grouped[\u0026#39;Women Percentage\u0026#39;] = (df_grouped[\u0026#39;Lista Mujeres\u0026#39;] / df_grouped[\u0026#39;Lista Nominal\u0026#39;]) * 100 ochre_scale = [ [0.0, \u0026#39;#4c78c8\u0026#39;], # Blue [1.0, \u0026#39;#f58518\u0026#39;], # Light ochre (yellow) ] # Assuming `geojson` is your GeoJSON object for the municipalities fig = px.choropleth( df_grouped, geojson=geojson_data, locations=\u0026#39;Nombre Municipio\u0026#39;, color=\u0026#39;Women Percentage\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_continuous_scale=ochre_scale, projection=\u0026#34;mercator\u0026#34;, title=\u0026#34;Percentage of Women in Voter Registration\u0026#34; ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) # Update layout for colorbar position fig.update_layout( coloraxis_colorbar=dict( title=\u0026#39;Women Percentage\u0026#39;, orientation=\u0026#39;h\u0026#39;, x=0.5, xanchor=\u0026#39;center\u0026#39;, y=-0.2, thickness=10, # Adjust the thickness of the colorbar len=0.65 # Set the length as a fraction of the plot area width ) ) return fig #----------------------------------------------------------------------------- def create_winning_party_per_year_choropleth(selected_year, geojson, main_parties, df_dict): # This function now handles a single year\u0026#39;s DataFrame and generates a choropleth map for that year. df_year = df_dict[selected_year] winning_party_by_municipality = {} for municipality in df_year[\u0026#39;MUNICIPIO\u0026#39;].unique(): votes_by_party = {main_party: 0 for main_party in main_parties} # for main_party, parties in alliance_mapping.items(): # for party in parties: # if party in df_year.columns: # votes_by_party[main_party] += df_year.loc[df_year[\u0026#39;MUNICIPIO\u0026#39;] == municipality, party].sum() for party in main_parties: if party in df_year.columns: votes_by_party[party] += df_year.loc[df_year[\u0026#39;MUNICIPIO\u0026#39;] == municipality, party].sum() winning_party = max(votes_by_party, key=votes_by_party.get) winning_party_by_municipality[municipality] = winning_party df_map = pd.DataFrame(list(winning_party_by_municipality.items()), columns=[\u0026#39;MUNICIPIO\u0026#39;, \u0026#39;Winning Party\u0026#39;]) df_map[\u0026#39;Year\u0026#39;] = selected_year fig = px.choropleth( df_map, geojson=geojson, locations=\u0026#39;MUNICIPIO\u0026#39;, color=\u0026#39;Winning Party\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, projection=\u0026#34;mercator\u0026#34;, color_discrete_sequence=px.colors.qualitative.T10, ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) fig.update_layout(title=f\u0026#34;Winning Party per Municipality, {selected_year}\u0026#34;) return fig #-------------------------------------------------------------------------------------- def plot_election_pie_chart(selected_year, selected_municipality, df_re_all_years, main_parties): # mapping years to their indices in the list of dataframes year_to_index = {2009: 0, 2012: 1, 2015:2, 2018: 3, 2021: 4} selected_year_index = year_to_index.get(selected_year) if selected_year_index is None: print(f\u0026#34;No data available for the year {selected_year}.\u0026#34;) return # extract the dataframe for the selected year df_selected_year = df_re_all_years[selected_year_index] # filtering the df for the selected municipality df_municipality = df_selected_year[df_selected_year[\u0026#39;MUNICIPIO\u0026#39;] == selected_municipality] if df_municipality.empty: print(f\u0026#39;No data available for {selected_municipality}.\u0026#39;) return # aggregating votes for each main party votes_by_party = {main_party: 0 for main_party in main_parties} for party in main_parties: if party in df_municipality.columns: votes_by_party[party] += df_municipality[party].sum() # create the pie chart df_votes = pd.DataFrame(list(votes_by_party.items()), columns = [\u0026#39;Party\u0026#39;, \u0026#39;Votes\u0026#39;]) fig = px.pie(df_votes, values = \u0026#39;Votes\u0026#39;, names = \u0026#39;Party\u0026#39;, title = f\u0026#39;Vote Distribution in {selected_municipality}, {selected_year}\u0026#39;, color_discrete_sequence=px.colors.qualitative.T10) # Update the traces to remove the text labels fig.update_traces(textinfo=\u0026#39;none\u0026#39;, hoverinfo=\u0026#39;label+percent\u0026#39;) return fig #------------------------------------------------------------------------------------ def plot_aggregated_votes_by_main_party_px(df_list, main_parties, selected_municipality, election_years): \u0026#34;\u0026#34;\u0026#34; Plots an interactive line plot with filled areas to zero for each main party and its alliances, in a selected municipality across elections using Plotly Express. This approximates the non-stacked area plot behavior of the original function. \u0026#34;\u0026#34;\u0026#34; # initialize dictionary to hold vote totals for main parties votes_by_main_party = {main_party: [0] * len(election_years) for main_party in main_parties} # loop through each DataFrame and year for i, (df, year) in enumerate(zip(df_list, election_years)): # filter the DataFrame for the selected municipality if selected_municipality in df[\u0026#39;MUNICIPIO\u0026#39;].values: filtered_df = df[df[\u0026#39;MUNICIPIO\u0026#39;] == selected_municipality] # loop through each main party and its alliances for party in main_parties: # aggregate votes for each party in the alliance, adding to the main party\u0026#39;s total if party in filtered_df.columns: votes_by_main_party[party][i] += filtered_df[party].sum() # prepare the data for plotting data_for_plotting = [] for main_party, votes in votes_by_main_party.items(): for year, vote in zip(election_years, votes): data_for_plotting.append({\u0026#39;Election Year\u0026#39;: year, \u0026#39;Total Votes\u0026#39;: vote, \u0026#39;Party\u0026#39;: main_party}) df_plot = pd.DataFrame(data_for_plotting) # create the plot fig = px.line(df_plot, x=\u0026#39;Election Year\u0026#39;, y=\u0026#39;Total Votes\u0026#39;, color=\u0026#39;Party\u0026#39;, line_shape=\u0026#39;linear\u0026#39;, title=f\u0026#39;Total Votes per Party (Including Alliances), in {selected_municipality}\u0026#39;, color_discrete_sequence=px.colors.qualitative.T10) # customize the layout fig.update_traces(mode=\u0026#39;lines\u0026#39;, line=dict(width=2.5), fill=\u0026#39;tozeroy\u0026#39;) fig.update_layout(xaxis_title=\u0026#39;Election Year\u0026#39;, yaxis_title=\u0026#39;Total Votes\u0026#39;, legend_title=\u0026#39;Party\u0026#39;, font=dict(family=\u0026#34;Arial, sans-serif\u0026#34;, size=12, color=\u0026#34;#333\u0026#34;), hovermode=\u0026#39;x unified\u0026#39;, legend = dict( orientation = \u0026#39;h\u0026#39;, yanchor = \u0026#39;bottom\u0026#39;, y = -0.6, # adjuist to fit layout xanchor = \u0026#39;center\u0026#39;, x = 0.5 )) return fig #---------------------------------------------------------------------------------------------- # HELPER function to get the municipalities per selected year def get_municipalities_per_year(df_dict, selected_year): df_selected = df_dict.get(selected_year) if df_selected is None: print(f\u0026#34;No data available for the year {selected_year}.\u0026#34;) return [] # Retrieve and return a sorted list of unique municipalities return sorted(df_selected[\u0026#39;MUNICIPIO\u0026#39;].unique()) #------------------------------------------------------------------------------------------------- def create_voter_turnout_proportion_choropleth(df_resultados, selected_year, geojson_data): # Aggregate data by MUNICIPIO if not already aggregated df_grouped = df_resultados.groupby(\u0026#39;MUNICIPIO\u0026#39;).agg({ \u0026#39;TOTAL_VOTOS\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;LISTA_NOMINAL\u0026#39;: \u0026#39;sum\u0026#39; }).reset_index() df_grouped[\u0026#39;Porcentaje Votantes\u0026#39;] = df_grouped[\u0026#39;TOTAL_VOTOS\u0026#39;] / df_grouped[\u0026#39;LISTA_NOMINAL\u0026#39;] * 100 ochre_scale = [ [0.0, \u0026#39;#4c78c8\u0026#39;], # Blue [1.0, \u0026#39;#f58518\u0026#39;], # Light ochre (yellow) ] # Assuming `geojson` is your GeoJSON object for the municipalities fig = px.choropleth( df_grouped, geojson=geojson_data, locations=\u0026#39;MUNICIPIO\u0026#39;, color=\u0026#39;Porcentaje Votantes\u0026#39;, featureidkey=\u0026#34;properties.NOMGEO\u0026#34;, color_continuous_scale=ochre_scale, projection=\u0026#34;mercator\u0026#34;, title=f\u0026#34;Voter Turnout Percentage in {selected_year}\u0026#34; ) fig.update_geos(fitbounds=\u0026#34;locations\u0026#34;, visible=False) # Update layout for colorbar position fig.update_layout( coloraxis_colorbar=dict( title=(f\u0026#39;Voter Turnout Percentage in {selected_year}\u0026#39;), orientation=\u0026#39;h\u0026#39;, x=0.5, xanchor=\u0026#39;center\u0026#39;, y=-0.2, thickness=10, # Adjust the thickness of the colorbar len=0.65 # Set the length as a fraction of the plot area width ) ) return fig # static figures: static_choropleth_percentage_women = create_gender_proportion_choropleth(df_ln_sx_qroo, geojson_data) static_choropleth_age = create_age_choropleth(df_ln_age_qroo, geojson_data) static_choropleth_totals = create_total_choropleth(df_ln_sx_qroo, geojson_data) static_bar_totals = create_total_bar_plot(df_ln_sx_qroo) # Create a Dash application # app = Dash(__name__) # Assuming you\u0026#39;re fine with adding Bootstrap to your project app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP]) app.css.append_css({\u0026#39;external_url\u0026#39;: \u0026#39;/assets/styles.css\u0026#39;}) colors = { \u0026#39;background\u0026#39;: \u0026#39;#111111\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;#7FDBFF\u0026#39; } server = app.server # Layout app.layout = dbc.Container([ html.H1(\u0026#34;Elections Dashboard\u0026#34;), dbc.Row([ dbc.Col(dcc.Dropdown( id=\u0026#39;year-dropdown\u0026#39;, options=[{\u0026#39;label\u0026#39;: year, \u0026#39;value\u0026#39;: year} for year in sorted(set(election_years))], value=sorted(set(election_years))[0], # Default to the earliest year className=\u0026#39;dropdown\u0026#39; ), width = 12, lg = 6, className = \u0026#39;mb-2\u0026#39;), dbc.Col(dcc.Dropdown( id=\u0026#39;municipio-dropdown\u0026#39;, className=\u0026#39;dropdown\u0026#39; ), width = 12, lg = 6, className = \u0026#39;mb-2\u0026#39;) ]), dbc.Row([ dbc.Col(dcc.Graph(id=\u0026#39;time-series-plot\u0026#39;, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6), dbc.Col(dcc.Graph(id=\u0026#39;pie-chart\u0026#39;, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6) ]), dbc.Row([ dbc.Col(dcc.Graph(id=\u0026#39;choropleth-total-voters\u0026#39;, figure = static_choropleth_totals, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6), dbc.Col(dcc.Graph(id=\u0026#39;choropleth-turnout\u0026#39;, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6) ]), dbc.Row([ dbc.Col(dcc.Graph(id=\u0026#39;choropleth-age\u0026#39;, figure=static_choropleth_age, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6), dbc.Col(dcc.Graph(id=\u0026#39;choropleth-women\u0026#39;, figure=static_choropleth_percentage_women, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6) ]), dbc.Row([ dbc.Col(dcc.Graph(id=\u0026#39;choropleth-winning\u0026#39;, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6), dbc.Col(dcc.Graph(id=\u0026#39;bar-total-voters\u0026#39;, figure=static_bar_totals, className=\u0026#39;graph-container\u0026#39;), width = 12, lg = 6) ]) ], fluid=True) # Callback to update municipio dropdown based on year selection @app.callback( Output(\u0026#39;municipio-dropdown\u0026#39;, \u0026#39;options\u0026#39;), Output(\u0026#39;municipio-dropdown\u0026#39;, \u0026#39;value\u0026#39;), [Input(\u0026#39;year-dropdown\u0026#39;, \u0026#39;value\u0026#39;)] ) def set_municipio_options(selected_year): # Assuming a function that returns municipios for a given year municipalities = get_municipalities_per_year(df_dict, selected_year) options = [{\u0026#39;label\u0026#39;: m, \u0026#39;value\u0026#39;: m} for m in municipalities] new_value = municipalities[0] if municipalities else None # Default to first municipality or None return options, new_value # Callback to update interactive visualizations @app.callback( [Output(\u0026#39;time-series-plot\u0026#39;, \u0026#39;figure\u0026#39;), Output(\u0026#39;pie-chart\u0026#39;, \u0026#39;figure\u0026#39;), Output(\u0026#39;choropleth-winning\u0026#39;, \u0026#39;figure\u0026#39;), Output(\u0026#39;choropleth-turnout\u0026#39;, \u0026#39;figure\u0026#39;)], [Input(\u0026#39;year-dropdown\u0026#39;, \u0026#39;value\u0026#39;), Input(\u0026#39;municipio-dropdown\u0026#39;, \u0026#39;value\u0026#39;)] ) def update_visualizations(selected_year, selected_municipality): time_series_chart = plot_aggregated_votes_by_main_party_px( df_re_all_years, main_parties, selected_municipality, election_years ) pie_chart_per_municipality_per_year = plot_election_pie_chart( selected_year, selected_municipality, df_re_all_years, main_parties ) choropleth_winning_party_per_year = create_winning_party_per_year_choropleth( selected_year, geojson_data, main_parties, df_dict ) df_resultados = df_dict[selected_year] voter_proportion_choropleth = create_voter_turnout_proportion_choropleth(df_resultados, selected_year, geojson_data) return (time_series_chart, pie_chart_per_municipality_per_year, choropleth_winning_party_per_year, voter_proportion_choropleth) if __name__ == \u0026#39;__main__\u0026#39;: app.run_server() ","permalink":"http://localhost:1313/posts/20240509_election_dash_part_2-dashboard/20240509_election_dash_part_2-dashboard/","summary":"The cleaned data is loadsed and the plot generating functions are refactored here. Then a dashboard is created using dash.","title":"Election Data Dashboard Pt. 2: Dashboard with Dash"},{"content":" Introduction In this tutorial, an AR(p) (Autoregressive model of order p) is employed to analyze the trneds of a time series and forecast the behavior of the signal.\nAuto-regressive models are based on the assumption the behavior of a time series or signal depends on past values. The order of the AR model tells \u0026ldquo;how far back\u0026rdquo; the past values will affect the current value.\nCredits This exercise is mostly following this tutorial.\nDefinition The AR(p) model is defined as:\n$$ X_t = \\sum_{i=1}^{p} \\phi_i X_{t-i} + \\varepsilon_t $$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is the model uncertainty represented as white Gaussian noise, i.e. it follows a normal distribution of mean $\\mu=0$ and standard deviation $\\sigma$.\nIt follows that an AR(2) model is defined as:\n$$ X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\varepsilon_t $$\nNaturally, we want to find the parameters $\\theta={\\phi_1, \\phi_2,\\sigma}$. Since these are unobserved quantities of interest, we need to use an inference method to reveal these parameters. We will use Bayesian inference to achieve this goal.\nData Exploration For this example, I will generate artificial data. This will be done by first defining some values for the parameters $\\theta$ and then we will generate random data using those parameters by initializing the $X_1, X_2$ values, and then applying the AR(2) equation to generate the subsequent values.\nFirst, we import the relevant packages.\nusing StatsPlots, Turing, LaTeXStrings, Random, DataFrames Random.seed!(42) TaskLocalRNG()\rNow we create some artificial data. The steps involved in this are as follows:\nDefine some values for the parameters $\\theta$ Set the number of timesteps t Initialize an empty vector of size $\\mathbb{R}^{t+p}$ Initialize the first two $X$ values with randomly generated numbers using rand Populate the vector by using the equation for $X_t$ # define true values for θ true_phi_1 = -0.4 true_phi_2 = 0.3 true_sigma = 0.12 # define the time steps time = 100 # create an empty X vector X = Vector{Float64}(undef, time+2) # initialize the X vector with two random values at time steps 1 and 2 # to do this, use a random normally distributed number with mean zero and standard deviation σ, i.e., ε~N(0, σ) X[1] = rand(Normal(0, true_sigma)) X[2] = rand(Normal(0, true_sigma)) # populate vector X for t in 3:(time+2) X[t] = true_phi_1*X[t-1] + true_phi_2*X[t-2] + rand(Normal(0, true_sigma)) end\tVisualize the (Artificial) Data p_data = plot(X[3:end], legend = false, linewidth = 2, # xlims = (0, 60), # ylims = (-0.6, 0.6), title = \u0026#34;Bayesian Autoregressive AR(2) Model\u0026#34;, xlabel = L\u0026#34;t\u0026#34;, ylabel = L\u0026#34;X_t\u0026#34;, widen = true ) Modeling The next step is to construct our probabilistic model. Again, the goal here is to infer the values of the model parameters $\\theta$. Once we have inferred these parameters, we can make probabilistic predictions on the future behavior of the signal $X$.\nBayesian model Since we are using a Bayesian approach, our goal, in Bayesian terms, is to find the posterior distribution of the parameters $\\theta$, given a prior distribution, or prior knowledge, of the parameters before making any observations, i.e., seeing any data, and also a likelihood function, which reflects what kind of distribution (we assume) that the data is sourced from. Another way of understanding the likelihood function is the probability of making a set of observations $X$ given the parameters $\\theta$.\nThis relationship is established by Bayes\u0026rsquo; Theorem:\n$$ P(\\theta | X) \\propto P(X | \\theta)P(\\theta) $$\nIn summary, constructing the Bayesian model in this case comprises a selection of prior distributions for our unknown parameters $\\theta$ and a likelihood function. We will do this using the Turing.jl package.\nThe model therefore will consist of the prior distributions:\n$$ \\begin{align*} \\phi_1 \u0026amp; \\sim \\mathcal{N}(0, 1) \\ \\phi_2 \u0026amp; \\sim \\mathcal{N}(0, 1) \\ \\sigma \u0026amp; \\sim \\text{Exp}(1) \\end{align*} $$\nAnd the likelihood:\n$$ X_t \\sim \\mathcal{N}(\\mu_t, \\sigma) $$\nwhere $\\mu_t = \\sum_{i=1}^{p} \\phi_i X_{t-i}$ is the mean function of the distribution that governs X_t.\nA comment on the choice of priors For autoregressive parameters, using a normal distribution is a common choice. This is because the normal distribution is convenient and allows for a range of plausible values.\nFor the prior on the model uncertainty, the exponential distribution is sometimes used for non-negative parameters and has a similar role to the inverse gamma.\nFurthermore, the inverse gamma distribution is often chosen as a prior for the standard deviation because it is conjugate to the normal likelihood. This means that the posterior distribution will have a known form, making computations more tractable.\nBayesian model using Turing.jl Now we proceed to set up the model using the Turing.jl package.\n@model function ar(X, time) # pass the data X and the time vector # priors phi_1 ~ Normal(0, 1) phi_2 ~ Normal(0, 1) sigma ~ Exponential(1) # likelihood # initialize with random initial values X[1] ~ Normal(0, sigma) X[2] ~ Normal(0, sigma) # populate with samples for i in 3:(time+2) mu = phi_1*X[i-1] + phi_2*X[i-2] X[i] ~ Normal(mu, sigma) end end ar (generic function with 2 methods)\rmodel = ar(X, time) sampler = NUTS() samples = 1_000 chain = sample(model, sampler, samples) \u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.4\r\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:01\u001b[39m\rChains MCMC chain (1000×15×1 Array{Float64, 3}):\rIterations = 501:1:1500\rNumber of chains = 1\rSamples per chain = 1000\rWall duration = 11.59 seconds\rCompute duration = 11.59 seconds\rparameters = phi_1, phi_2, sigma\rinternals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\rSummary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m e\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m \u001b[0m ⋯\rphi_1 -0.3830 0.1047 0.0036 836.6151 762.4445 0.9996 ⋯\rphi_2 0.1587 0.1012 0.0035 838.3014 749.6718 1.0002 ⋯\rsigma 0.1083 0.0079 0.0003 755.4034 743.3822 1.0014 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rQuantiles\r\u001b[1m parameters \u001b[0m \u001b[1m 2.5% \u001b[0m \u001b[1m 25.0% \u001b[0m \u001b[1m 50.0% \u001b[0m \u001b[1m 75.0% \u001b[0m \u001b[1m 97.5% \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\rphi_1 -0.5733 -0.4562 -0.3858 -0.3141 -0.1771\rphi_2 -0.0339 0.0913 0.1562 0.2256 0.3549\rsigma 0.0943 0.1030 0.1079 0.1130 0.1257\rVisualize and Summarize the Results Next we can access the MCMC Diagnostics and generate a summary of the results.\nplot(chain) DataFrame(summarystats(chain)) 3×8 DataFrameRowparametersmeanstdmcseess_bulkess_tailrhatess_per_secSymbolFloat64Float64Float64Float64Float64Float64Float641phi_1-0.3830190.1046950.00361324836.615762.4440.99958572.16552phi_20.1586610.1011960.00351463838.301749.6721.0002172.3113sigma0.1083420.007886220.000291067755.403743.3821.0014565.1603\rPredictions Making Predictions To make predictions, the following steps are taken:\nSet the number of time steps into the future, $t_f$ Initialize an empty matrix for the forecasted $X$ values - This will be a matrix because it will be a collection of vectors. Each vector will represent one sample forecast Initialize two steps of each of the sample vectors to be generated - In practical terms, initialize the first number of each column; each column will represent a forecast time series Keep in mind that what will be done here is to create samples of the future behavior of the signal $t_f$ number of time steps into the future. To do this, we will generate signals that use the posterior distributions of the parameters $\\theta$ by calling the function rand(chain[:,Z,Z]) which will randomly pick a number out of the sample pool, effectively \u0026ldquo;sampling\u0026rdquo; from that posterior distribution (sample pool).\ntime_future = 15 X_future = Matrix{Float64}(undef, time_future+2, samples) # Initialize the first two time steps for every forecast X_future[1, :] .= X[time-1] X_future[2, :] .= X[time] # populate the forecast vectors by sampling from the posterior sample pool of the parameters θ for col in 1:samples phi_1_future = rand(chain[:,1,1]) phi_2_future = rand(chain[:,2,1]) error_future = rand(chain[:,3,1]) noise_future = rand(Normal(0, error_future)) for row in 3:(time_future+2) X_future[row, col] = phi_1_future * X_future[row-1, col] + phi_2_future * X_future[row-2, col] + noise_future end end Visualize the forecast Now that we propagated the uncertainty of in the posterior distribution of the parameters $\\theta$, we can plot the posterior predictive distribution of $X$, $P(X^*|\\theta)$.\ntime_predict = time:(time + time_future) for i in 1:samples plot!(p_data, time_predict, X_future[2:end, i], legend = false, # predictions linewidth = 1, color = :green, alpha = 0.1 ) end p_data # visualize mean values for predictions X_future_mean = [mean(X_future[i, 1:samples]) for i in 2:(time_future+2)] plot!(p_data, time_predict, X_future_mean, legend = false, linewidth = 2, color = :red, linestyle = :dot ) ","permalink":"http://localhost:1313/posts/20240222_bayesian_time_series_analysis/20240222_bayesian_time_series_analysis/","summary":"This tutorial covers the fundamentals of Bayesian approaches to time series, model construction, and practical implementation, using real-world data for hands-on learning.","title":"Bayesian Time Series Analysis with Julia and Turing.jl"},{"content":" In this example, I am following the tutorials found in:\nTuring.jl - Bayesian Poisson Regression PyMC - GLM: Poisson Regression Both examples show the interaction between some variables and a discrete outcome. In this case, the outcome is the number of sneezes per day (i.e. a discrete outcome) in some study subjects, and whether or not they take antihistamine medicine and whether or not they drink alcohol.\nThis example explores how these factors, and more specifically, the combination of these factors, affect the number of times a person sneezes.\nusing CSV, DataFrames, Turing, StatsPlots, Plots, Random Random.seed!(42) TaskLocalRNG()\rCollect (generate) the data In this example, we will generate the data in the same way as in the tutorials:\nNo Alcohol Alcohol No Meds 6 36 Meds 1 3 Those values will be used to create the artificial data by generating Poisson-distributed random samples.\ntheta_noalc_nomed = 6 theta_noalc_med = 1 theta_alc_nomed = 36 theta_alc_med = 3 ns = 500 # number of samples # create a data frame data = DataFrame( hcat( vcat( rand(Poisson(theta_noalc_med), ns), rand(Poisson(theta_alc_med), ns), rand(Poisson(theta_noalc_nomed), ns), rand(Poisson(theta_alc_nomed), ns) ), vcat( falses(ns), trues(ns), falses(ns), trues(ns) ), vcat( falses(ns), falses(ns), trues(ns), trues(ns) ) ), :auto ) # assign names to headers head_names = [:n_sneezes, :alcohol, :nomeds] sneeze_data = DataFrame(data, head_names) first(sneeze_data, 10) 10×3 DataFrameRown_sneezesalcoholnomedsInt64Int64Int6410002100300040005100610071008100920010200\rVisualize the data Now that we have \u0026ldquo;collected\u0026rdquo; some data on the number of sneezes per day from a number of people, we visualize the data.\nThe way we are collecting and plotting these data sub-sets is as follows:\nCall the histogram function Create a histogram of the dataframe \u0026ldquo;sneeze_data\u0026rdquo; we \u0026ldquo;collected\u0026rdquo; previously Select a subset of that dataframe All the rows of the columns where alcohol is false i.e. 0 AND all the rows where no medicine was taken is also false All the rows of the columns where alcohol is false AND all the rows of where medicine is true \u0026hellip; and so on # create separate histograms for each case p1 = histogram(sneeze_data[(sneeze_data[:,:alcohol] .== 0) .\u0026amp; (sneeze_data[:,:nomeds] .== 0), :n_sneezes]; title = \u0026#34;No alcohol + No Meds\u0026#34;, ylabel=\u0026#34;People Count\u0026#34;) p2 = histogram(sneeze_data[(sneeze_data[:,:alcohol] .== 1) .\u0026amp; (sneeze_data[:,:nomeds] .== 0), :n_sneezes]; title = \u0026#34;No alcohol + Meds\u0026#34;) p3 = histogram(sneeze_data[(sneeze_data[:,:alcohol] .== 0) .\u0026amp; (sneeze_data[:,:nomeds] .== 1), :n_sneezes]; title = \u0026#34;Alcohol + No Meds\u0026#34;, xlabel = \u0026#34;Sneezes/Day\u0026#34;, ylabel=\u0026#34;People Count\u0026#34;) p4 = histogram(sneeze_data[(sneeze_data[:,:alcohol] .== 1) .\u0026amp; (sneeze_data[:,:nomeds] .== 1), :n_sneezes]; title = \u0026#34;Alcohol + Meds\u0026#34;, xlabel = \u0026#34;Sneezes/Day\u0026#34;) plot(p1, p2, p3, p4; layout=(2,2), legend = false) Interpreting the data The histograms show that the data from the \u0026ldquo;study\u0026rdquo; resembles a Poisson distribution (as mentioned in the PyMC tutorial, this is obvious, because that\u0026rsquo;s how the data is generated!). Furthermore, the data is telling us something:\nLooking at the plot for \u0026ldquo;no alcohol and medicine\u0026rdquo; it is clear that most people reported very few sneezes; notice how the histogram skews towards large counts (of people) for very few sneezes On the other hand, notice how the \u0026ldquo;alcohol and no medicine\u0026rdquo; seems to tell us that many reported somewhere around 35 sneezes per day Again, we can start thinking of a pattern just by looking at the data, and it seems like the data is telling us that if you don\u0026rsquo;t drink alcohol and take antihistamines, you are less likely to be sneezing around than if you drink alcohol and don\u0026rsquo;t take any allergy meds. Makes sense, right?\nModel We established that the data looks like it could be modelled as a Poisson distribution. Thus, we can define our probabilistic model as follows:\n$$Y_{obs} \\sim Poisson(\\lambda)$$\n$$\\log(\\lambda) = \\theta\u0026rsquo;\\mathbf{x} = \\alpha + \\beta\u0026rsquo; \\mathbf{x}$$\nWhat the above means is that we assume that the observed data outcomes, i.e., the number of sneezes per day, follow a Poisson distribution, which is a discrete probability distribution that models the number of events that occur in a fixed interval of time or space. The rate or intensity of the events, $\\lambda$, depends on the predictor variables (the input data) $\\mathcal{x}$, such as the season, the temperature, or, in our case, whether a person ingested alcohol and whether the person took antihistamines.\nThe linear predictor $\\theta\u0026rsquo; \\mathcal{x}$ is the function that links the predictor variables to the rate parameter, where $\\theta = {\\alpha, \\beta\u0026rsquo;}$ are the parameters of the model.\nLooking at the structure of the linear relationship between the paramters of the model, and the predictors:\n$$\\log(\\lambda) = \\alpha + \\beta\u0026rsquo; \\mathcal{x}$$\nwe can understand that the parameter $\\alpha$ is the intercept, which is the expected number of sneezes when all the predictor variables are zero. The parameter $\\beta\u0026rsquo;$ is a vector of coefficients, which measure the effect of each predictor variable $\\mathcal{x}$ on the number of sneezes. The log link function ensures that the rate parameter $\\lambda$ is always positive and allows for multiplicative effects of the predictor variables on the response variable.\nDefine the model with Turing.jl Now that we know how we are modeling our data, we use the package Turing.jl to define the model. Turing.jl is a tool that helps us write models in Julia and find the best parameters for them.\nThe model has two parts: the prior and the likelihood. The prior is what we think or guess about the parameters before we see the data. The likelihood is how likely the data is under the parameters. The parameters are the numbers that control the model, such as the rate of sneezes.\nWe use the Poisson distribution for the likelihood, because it is good for counting things, like sneezes. The Poisson distribution has one parameter, the rate of sneezes. The higher the rate, the more sneezes we expect.\nWe use any distribution for the prior, depending on how much we know about the parameters. If we know nothing, we use a flat prior, which does not favor any value. The prior affects the final answer, because it is our starting point.\nWe use Bayes’ theorem to combine the prior and the likelihood and get the final answer. The final answer is the posterior, which is what we believe about the parameters after we see the data. The posterior is the best fit for the model and the data.\nLet\u0026rsquo;s crank up the Bayes!\n@model function poisson(x, y) # define the priors alpha ~ Normal(0,1) alcohol ~ Normal(0, 1) nomeds ~ Normal(0, 1) # alc_med ~ Normal(0,1) # define the likelihood for i in 1:length(y) log_lambda = alpha + alcohol * x[i, 1] + nomeds * x[i, 2] lambda = exp(log_lambda) y[i] ~ Poisson(lambda) end end poisson (generic function with 2 methods)\r# pass the data to the model function # pass the predictor data as a Matrix for efficiency model = poisson(Matrix(sneeze_data[!,[:alcohol, :nomeds] ]), sneeze_data[!, :n_sneezes]) # select the sampler sampler = NUTS() # define the number of sampler samples = 1000 # set number of chains num_chains = 8 # crank up the Bayes! chain = sample(model, sampler, MCMCThreads(), samples, num_chains) \u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.00625\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.0125\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.00625\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.0125\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.0125\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.00625\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.00625\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.0125\r\u001b[32mSampling (8 threads): 100%|█████████████████████████████| Time: 0:00:00\u001b[39m\rChains MCMC chain (1000×15×8 Array{Float64, 3}):\rIterations = 501:1:1500\rNumber of chains = 8\rSamples per chain = 1000\rWall duration = 13.66 seconds\rCompute duration = 100.67 seconds\rparameters = alpha, alcohol, nomeds\rinternals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\rSummary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m\u001b[0m ⋯\ralpha -0.5025 0.0277 0.0005 2943.5608 2841.2874 1.0030 ⋯\ralcohol 1.7333 0.0186 0.0003 3801.1996 3652.2403 1.0022 ⋯\rnomeds 2.3348 0.0236 0.0004 2901.3750 3410.6453 1.0020 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rQuantiles\r\u001b[1m parameters \u001b[0m \u001b[1m 2.5% \u001b[0m \u001b[1m 25.0% \u001b[0m \u001b[1m 50.0% \u001b[0m \u001b[1m 75.0% \u001b[0m \u001b[1m 97.5% \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\ralpha -0.5568 -0.5212 -0.5023 -0.4839 -0.4486\ralcohol 1.6974 1.7205 1.7331 1.7458 1.7698\rnomeds 2.2891 2.3189 2.3346 2.3506 2.3824\rNOTE: The above routine employs the MCMCThreads method to sample multiple chains. However, in order to implement this, one needs to change the environment variables for the number of threads Julia can use. These two threads might shed some light as to how to achieve this:\nhttps://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794 Of course, if you don\u0026rsquo;t want to bother, then just change the last two functional lines in the cell above so that they read:\n# set number of chains - comment this out: # num_chains = 8 # crank up the Bayes! - delete MCMCThreads() and num_chains chain = sample(model, sampler, samples) Visualize the results We can see above that we have obtained a sample pool of the posterior distribution of the parameters. This is what we were looking for. What this means is that now we have a posterior distribution (in the form of a sample pool), which we can also summarize with summary statistics.\nLet\u0026rsquo;s look at the diagnostics plots and the summary statistics.\nplot(chain) DataFrame(summarystats(chain)) 3×8 DataFrameRowparametersmeanstdmcseess_bulkess_tailrhatess_per_secSymbolFloat64Float64Float64Float64Float64Float64Float641alpha-0.5025190.02765530.0005110692943.562841.291.0029829.23972alcohol1.73330.01860970.0003016113801.23652.241.0022437.7593nomeds2.33480.02362690.0004363852901.383410.651.0019728.8207\r# taking the first chain c1 = chain[:, :, 1] # Calculating the exponentiated means b0_exp = exp(mean(c1[:alpha])) b1_exp = exp(mean(c1[:alcohol])) b2_exp = exp(mean(c1[:nomeds])) println(\u0026#34;The exponent of the mean of the weights (or coefficients) are: \\n\u0026#34;) println(\u0026#34;b0: \u0026#34;, b0_exp) println(\u0026#34;b1: \u0026#34;, b1_exp) println(\u0026#34;b2: \u0026#34;, b2_exp) The exponent of the mean of the weights (or coefficients) are: b0: 0.604415461752317\rb1: 5.658573583760772\rb2: 10.342642711232362\rNotice how we are not recovering the original $\\lambda$ values that were used to create this data set, i.e.:\ntheta_noalc_nomed = 6 theta_noalc_med = 1 theta_alc_nomed = 36 theta_alc_med = 3 Instead, we are recovering the parameters of the linear function, in other words, $\\theta = {\\alpha, \\beta\u0026rsquo;}$ in the linear relation:\n$$\\log(\\lambda) = \\alpha + \\beta_1 x_{alc} + \\beta_2 x_{meds}$$\nwhere $x_{(\\cdot)}$ represents the binary variable of whether the subject took alcohol/medicine or not.\nConclusion This tutorial shows how to perform Bayesian inference on discrete data, e.g. the record of how many sneezes per day a group of people had, and classified according to their alcohol and medication consumption.\nIn real-world scenarios, we would obviously not know the parameter values, since this is precisely what we want to find out by incorporating whatever we knew about them into what we observed.\n","permalink":"http://localhost:1313/posts/20240217_bayesian_poisson_regression/20240217_bayesian_poisson_regression/","summary":"Explore Bayesian Poisson regression for modeling count data with Julia and Turing.jl. This tutorial includes model setup, implementation, and performance assessment with a practical example.","title":"Bayesian Poisson Regression with Julia and Turing.jl"},{"content":" Problem Statement You are interested in studying the factors that influence the likelihood of heart disease among patients.\nYou have a dataset of 303 patients, each with 14 variables: age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise induced angina, oldpeak, slope, number of major vessels, thalassemia, and diagnosis of heart disease.\nYou want to use Bayesian logistic regression to model the probability of heart disease (the outcome variable) as a function of some or all of the other variables (the predictor variables).\nYou also want to compare different models and assess their fit and predictive performance.\nBayesian Workflow For this project, I will try to follow this workflow:\nData exploration: Explore the data using descriptive statistics and visualizations to get a sense of the distribution, range, and correlation of the variables. Identify any outliers, missing values, or potential errors in the data. Transform or standardize the variables if needed.\nModel specification: Specify a probabilistic model that relates the outcome variable to the predictor variables using a logistic regression equation. Choose appropriate priors for the model parameters, such as normal, student-t, or Cauchy distributions. You can use the brms package in Julia to define and fit Bayesian models using a formula syntax similar to lme4. However, try to use Turing.jl\nModel fitting: Fit the model using a sampling algorithm such as Hamiltonian Monte Carlo (HMC) or No-U-Turn Sampler (NUTS). You can use the DynamicHMC or Turing.jl package in Julia to implement these algorithms. Check the convergence and mixing of the chains using diagnostics such as trace plots, autocorrelation plots, effective sample size, and potential scale reduction factor. You can use the MCMCDiagnostics or the included diagnostics features in Turing.jl package in Julia to compute these diagnostics.\nModel checking: Check the fit and validity of the model using posterior predictive checks, residual analysis, and sensitivity analysis. You can use the PPCheck package in Julia to perform posterior predictive checks, which compare the observed data to data simulated from the posterior predictive distribution. You can use the BayesianRidgeRegression package in Julia to perform residual analysis, which plots the residuals against the fitted values and the predictor variables. You can use the Sensitivity package in Julia to perform sensitivity analysis, which measures how the posterior distribution changes with respect to the prior distribution or the likelihood function.\n# import packages using CSV, Turing, DataFrames, StatsPlots, LaTeXStrings, Distributions using Images, ImageIO using Random: seed! seed!(42) Random.TaskLocalRNG()\rData Exploration After \u0026ldquo;collecting\u0026rdquo; the data, we may import it and arrange it so we can use it further.\nThe data set can be found in this Kaggle link.\ndf = CSV.read(\u0026#34;data/processed_cleveland.csv\u0026#34;, DataFrame) map!(x -\u0026gt; x != 0 ? 1 : 0, df.num, df.num); # make the outcome binary df 303×14 DataFrame278 rows omittedRowagesexcptrestbpscholfbsrestecgthalachexangoldpeakslopecathalnumInt64Int64Int64Int64Int64Int64Int64Int64Int64Float64Int64String1String1Int64163111452331215002.33060267141602860210811.52331367141202290212912.62271437131302500018703.53030541021302040217201.41030656121202360017800.81030762041402680216003.63231857041203540016310.61030963141302540214701.421711053141402031215513.130711157141401920014800.420601256021402940215301.320301356131302561214210.62161\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;\u0026vellip;29255021323420016601.2103029344141201690014412.8306129463141401870214414.0127129563041241970013610.0203129641121201570018200.010302975914164176129001.0226129857041402410012310.2207129945111102640013201.2207130068141441931014103.4227130157141301310011511.2217130257021302360217400.0213130338131381750017300.01?30\rIn the above data frame, the attributes are as follows:\nVariable Name Role Type Demographic Description Units Missing Values age Feature Integer Age years no sex Feature Categorical Sex no cp Feature Categorical no trestbps Feature Integer resting blood pressure (on admission to the hospital) mm Hg no chol Feature Integer serum cholestoral mg/dl no fbs Feature Categorical fasting blood sugar \u0026gt; 120 mg/dl no restecg Feature Categorical no thalach Feature Integer maximum heart rate achieved no exang Feature Categorical exercise induced angina no oldpeak Feature Integer ST depression induced by exercise relative to rest no Complete attribute documentation:\n1. age: age in years\r2. sex: sex (1 = male; 0 = female)\r3. cp: chest pain type\r- Value 1: typical angina\r- Value 2: atypical angina\r- Value 3: non-anginal pain\r- Value 4: asymptomatic\r4. trestbps: resting blood pressure (in mm Hg on admission to the\rhospital)\r5. chol: serum cholestoral in mg/dl\r6.fbs: fasting blood sugar \u0026gt; 120 mg/dl (1 = true; 0 = false)\r7. restecg: resting electrocardiographic results\r- Value 0: normal\r- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of \u0026gt; 0.05 mV)\r- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\r8. thalach: maximum heart rate achieved\r9. exang: exercise induced angina (1 = yes; 0 = no)\r10. oldpeak: ST depression induced by exercise relative to rest\r11. slope: the slope of the peak exercise ST segment\r- Value 1: upsloping\r- Value 2: flat\r- Value 3: downsloping\r12. ca: number of major vessels (0-3) colored by flourosopy (for calcification of vessels)\r13. thal: results of nuclear stress test (3 = normal; 6 = fixed defect; 7 = reversable defect)\r14. num: target variable representing diagnosis of heart disease (angiographic disease status) in any major vessel\r- Value 0: \u0026lt; 50% diameter narrowing\r- Value 1: \u0026gt; 50% diameter narrowing\rData Interpretation After collecting the data, it has been imported as a Data Frame. Now, to understand what we will do with this exercise, we need to analyze the data by means of Bayesian Logistic Regression.\nWith this type of analysis, we can make predictions on (typically) binary outcomes, based on a set of parameters. In this particular case, we are interested in predicting whether a patient will have heart disease based on a set of parameters such as age, chest pain, blood pressure, etc.\nIn terms of the data available, we have a set of 303 observations (303 patients) whose symptoms and circumstances have been recorded, and the outcome is the heart disease diagnosis. To simplify things, this data set has a binary outcome, i.e. heart disease present/not present.\nAdditionally, this study is divided in two parts: first, I will set up the logistic regression model to include only one predictor, i.e., age. Afterwards, an analysis will be performed including two or more predictors.\n# find the range for the age, to set the plot limits below # min_age = minimum(df.age) min_age = 15 max_age = 85 # visualize data p_data = scatter(df.age, df.num, legend = false, xlims = (min_age, max_age), color = :red, markersize = 5, title = \u0026#34;Probability of Heart Disease\u0026#34;, xlabel = \u0026#34;Age (years)\u0026#34;, ylabel = \u0026#34;Probability of Heart Disease\u0026#34;, widen = true, dpi = 150 ) Model Specification In this stage of the workflow, we will specify the Bayesian model and then use Turing.jl to program it in Julia.\nThe model I will use for this analysis is a Bayesian Logistic Regression model, which relates the probability of heart disease to a linear combination of the predictor variables, using a logistic function. The model can be written as:\n$$\\begin{aligned} y_i \u0026amp;\\sim Bernoulli(p_i) \\\\ p_i \u0026amp;= \\frac{1}{1+e^{-\\eta_i}} \\\\ \\eta_i \u0026amp;= \\alpha + {\\beta_1 x_{i,1}} + {\\beta_2 x_{i,2}} + \\ldots + {\\beta_{13} x_{i,13}} \\\\ \\alpha \u0026amp;\\sim \\mathcal{N}(\\mu_\\alpha,\\sigma_\\sigma) \\\\ \\beta_j \u0026amp;\\sim \\mathcal{N}(\\mu_{\\beta},\\sigma_{\\beta}) \\\\ \\end{aligned}$$\nwhere $y_i$ is the outcome for the i-th patient, $p_i$ is the probability of heart disease for the i-th patient, $\\eta_i$ is the linear predictor for the i-th patient, $\\alpha$ and $\\beta_j$ are the intercept and coefficient for the j-th predictor variable, respectively, and $x_{ij}$ is the value of the j-th predictor variable for the i-th patient.\nThe assumptions that I am making are:\nThe outcome variable follows a Bernoulli distribution, i.e. $y_i \\sim Bernoulli(p_i)$, which is appropriate for binary outcomes The predictor variables are linearly related to the log-odds of the outcome variable, i.e. $\\log(\\frac{p}{1-p})$ which is a common assumption for logistic regression models The prior distributions for the model parameters are uniform, which are weakly informative and reflect my prior beliefs about the plausible range of the parameters Regarding point (2):\nThat statement means that the log-odds of the outcome variable (the log of the odds ratio) can be expressed as a linear function of the predictor variables. Mathematically, this can be written as:\n$$\\log(\\frac{p}{1-p}) = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k$$\nwhere $p$ is the probability of the outcome variable being 1, $x_1, x_2, \\ldots, x_k$ are the predictor variables, and $\\alpha, \\beta_1, \\beta_2, \\ldots, \\beta_k$ are the coefficients (parameters).\nThis assumption implies that the effect of each predictor variable on the log-odds of the outcome variable is contant, regardless of the values of the other predictor variables. It also implies that the relationship between the predictor variables and the probability of the outcome variable is non-linear, as the probability is obtained by applying the inverse of the log-odds function, which is the logistic function:\n$$p = \\frac{1}{1+e^{-(\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k)}}$$\nThe logistic function is an S-shaped curve that maps any real number to a value between 0 and 1. It has the property that as the linear predictor increases, the probability approaches 1, and as the linear predictor decreases, the probability approaches 0.\nModel Specification Using Turing.jl # define the Bayesian model @model function logit_model(predictors, disease) # priors α ~ Normal(0.0,10.0) β ~ Normal(0.0,10.0) # likelihood η = α .+ β.*predictors p = 1 ./ (1 .+ exp.(-η)) # remember to include the \u0026#34;.\u0026#34;! for i in eachindex(p) disease[i] ~ Bernoulli(p[i]) end end logit_model (generic function with 2 methods)\rCrank up the Bayes! Run the model using sample(model, sampler, opt_argument, samples, chains)\n# infer posterior probability model = logit_model(df.age, df.num) sampler = NUTS() samples = 1_000 num_chains = 8 # set the number of chains chain = sample(model, sampler, MCMCThreads(), samples, num_chains) \u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.025\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.0125\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.025\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.0125\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.025\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.05\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.025\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.025\r\u001b[32mSampling (8 threads): 100%|█████████████████████████████| Time: 0:00:01\u001b[39m\rChains MCMC chain (1000×14×8 Array{Float64, 3}):\rIterations = 501:1:1500\rNumber of chains = 8\rSamples per chain = 1000\rWall duration = 13.18 seconds\rCompute duration = 100.1 seconds\rparameters = α, β\rinternals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\rSummary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m\u001b[0m ⋯\rα -3.0326 0.7453 0.0210 1242.6057 1246.3034 1.0043 ⋯\rβ 0.0524 0.0134 0.0004 1224.4182 1259.7727 1.0037 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rQuantiles\r\u001b[1m parameters \u001b[0m \u001b[1m 2.5% \u001b[0m \u001b[1m 25.0% \u001b[0m \u001b[1m 50.0% \u001b[0m \u001b[1m 75.0% \u001b[0m \u001b[1m 97.5% \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\rα -4.4814 -3.5432 -3.0127 -2.5183 -1.5973\rβ 0.0264 0.0432 0.0521 0.0617 0.0789\rNOTE: The above routine employs the MCMCThreads() method to sample multiple chains. However, to implement this, one needs to change the environment variables for the number of threads Julia can use. These two discussions might shed some light as to how to achieve this:\nhttps://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794 Of course, if you don\u0026rsquo;t want to bother, then just change the last two functional lines in the cell above so that they read:\n# set number of chains - comment this out:\r# num_chains = 8\r# crank up the Bayes! - delete MCMCThreads() and num_chains\rchain = sample(model, sampler, samples)\rPlot the MCMC Diagnostics plot(chain, dpi = 150) Get the Summary Statistics summarystats(chain) Summary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m\u001b[0m ⋯\rα -3.0326 0.7453 0.0210 1242.6057 1246.3034 1.0043 ⋯\rβ 0.0524 0.0134 0.0004 1224.4182 1259.7727 1.0037 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rPlot and Interpret the Results Ok, how do we interpret the results from a Bayesian approach? Let\u0026rsquo;s start by plotting the results. This will help us understand not only the results, but really grasp the power of a Bayesian model in action.\nFrom a frequentist or a machine learning approach, we would expect to find a function that models the data the best possible way, i.e. fit a model. If we were to visualize it, we would see one single sigmoid curve trying its best to explain the data.\nHow about this chart here, though? This chart is a collection of possible outcomes given that the parameters $\\alpha$ and $\\beta$ in this case, are modeled as random variables with some probability distribution. Therefore, there is an uncertainty associated with them. This uncertainty is naturally propagated onto the sigmoid function. Therefore, there is also an uncertainty associated with that sigmoid curve that we are trying to model.\nAgain, below we can see a collection of possible outcomes given the parameter sample space. There is a darker region where most sigmoid functions turned out, and these tend to be the most probable sigmoid functions, or, in other words, these sigmoid functions are the most probable functions that could fit the data, considering the distributions of the parameters too!\nInt(samples/10) 100\rx_line = 15:1:max_age for i in 1:samples b = chain[i, 1, 1] m = chain[i, 2, 1] line(x) = m*x +b p(x) = 1 / (1 + exp(-line(x)) ) plot!(p_data, x_line, p, legend = false, linewidth = 2, color= :blue, alpha = 0.02, dpi = 150 ) end p_data Making Predictions So why go through all this trouble, you might be asking. Well, one of the reasons we want to use probabilistic models is, first, to make predictions. But I would go further than that: these models are useful when making informed decisions. Let\u0026rsquo;s try this out.\nLet\u0026rsquo;s make predictions for different arbitrary ages (50, 60, 70, 80, 20):\nnew_Age = [50, 60, 70, 80, 20] p_disease = fill(missing, length(new_Age)) predictions = predict(logit_model(new_Age, p_disease), chain) summarystats(predictions) Summary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m \u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m \u001b[0m ⋯\rdisease[1] 0.3855 0.4867 0.0055 7711.3762 NaN 0.9998 ⋯\rdisease[2] 0.5258 0.4994 0.0055 8284.1301 NaN 0.9998 ⋯\rdisease[3] 0.6432 0.4791 0.0056 7441.4457 NaN 1.0002 ⋯\rdisease[4] 0.7555 0.4298 0.0050 7352.4368 NaN 0.9998 ⋯\rdisease[5] 0.1224 0.3277 0.0039 7016.6404 NaN 1.0004 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rInterpreting the predictions The last operations make predictions of heart diseased based on age only. What the predictions mean is that, given the data, the probability distribution of an individual of age 50 to have heart disease has a mean of 0.379, and a standard deviation of 0.485 (this is highly uncertain, by the way).\nSimilarly, a 20-year-old individual has a probability with a mean of 0.13 and standard deviation of 0.336 of having heart disease.\nThese statistics are extremely powerful when you are trying to make decisions, such as when diagnosing Heart Disease. It stands to reason that, if you were a physician, you want to know what your model says might be wrong (or not) with your patient, but you also want to know how much you can trust that prediction.\nIf your model classifies Patient X as having heart disease, you would probably want to know how sure you are of this. And this certainty comes partially from\u0026hellip; you guessed it: your priors and the data.\nIn the plot below, we can see the where the predictions lie. Note that these probabilities are on a continuum given by the sigmoid function. But we want our final decision to be a yes or a no. To do that, we need to set a decision threshold.\nWe will do that at the end of the next section.\nfor i in 1:length(new_Age) pred_mean = mean(predictions[:, i, 1]) pred_plot = scatter!(p_data, (new_Age[i], pred_mean), dpi=150) end p_data Model Specification Using Multiple Predictors Some Data Cleaning In this part, I am using the Turing.jl documentation tutorial found in https://turinglang.org/dev/tutorials/02-logistic-regression/.\nIn the tutorial, they quite rightly incorporate a train/test split, and data normalization, which is the recommended practice. I didn\u0026rsquo;t do it in the first part of this tutorial to keep things simple!\nHere is how they handle the split and the data normalization using MLUtils.\nfunction split_data(df, target; at=0.70)\rshuffled = shuffleobs(df)\rreturn trainset, testset = stratifiedobs(row -\u0026gt; row[target], shuffled; p=at)\rend\rfeatures = [:StudentNum, :Balance, :Income]\rnumerics = [:Balance, :Income]\rtarget = :DefaultNum\rtrainset, testset = split_data(data, target; at=0.05)\rfor feature in numerics\rμ, σ = rescale!(trainset[!, feature]; obsdim=1)\rrescale!(testset[!, feature], μ, σ; obsdim=1)\rend\r# Turing requires data in matrix form, not dataframe\rtrain = Matrix(trainset[:, features])\rtest = Matrix(testset[:, features])\rtrain_label = trainset[:, target]\rtest_label = testset[:, target];\rusing MLDataUtils: shuffleobs, stratifiedobs, rescale! using StatsFuns # we introduce this package so we can later call the # logistic function directly instead of defining it manually as before function split_data(df, target; at=0.70) shuffled = shuffleobs(df) return trainset, testset = stratifiedobs(row -\u0026gt; row[target], shuffled; p=at) end features = [:age, :cp, :chol] target = :num trainset, testset = split_data(df, target;) # convert the feature columns to float64 to ensure compatibility with rescale! for feature in features df[!, feature] = float.(df[!, feature]) end for feature in features μ, σ = rescale!(trainset[!, feature]; obsdim=1) rescale!(testset[!, feature], μ, σ; obsdim=1) end # Turing requires data in matrix form, not dataframe train = Matrix(trainset[!, features]) test = Matrix(testset[!, features]) train_label = trainset[!, target] test_label = testset[!, target]; Inference Now that our data is formatted, we can perform our Bayesian logistic regression with multiple predictors: using chest pain (cp), age (age), resting bloodpressure (tresttbps) and cholesterol (chol) levels.\n@model function logreg_multi(X, y) # priors intercept ~ Normal(0.0, 10.0) age ~ Normal(0.0, 10.0) cp ~ Normal(0.0, 10.0) chol ~ Normal(0.0, 10.0) n, _ = size(X) for i in 1:n # call the logistic function directly, instead of manually v = logistic(intercept + age*X[i,1] + cp*X[i,2] + chol*X[i,3]) y[i] ~ Bernoulli(v) end end logreg_multi (generic function with 2 methods)\rX = train y = train_label println(size(train), size(test)) (212, 3)(91, 3)\rNow we build the model and create the chain:\nmodel_multi = logreg_multi(X, y) chain_multi = sample(model_multi, NUTS(), MCMCThreads(), 2_000, 8) # select 2000 samples directly \u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 1.6\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.8\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.8\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.8\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.8\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 1.6\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 0.8\r\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 1.6\rChains MCMC chain (2000×16×8 Array{Float64, 3}):\rIterations = 1001:1:3000\rNumber of chains = 8\rSamples per chain = 2000\rWall duration = 11.32 seconds\rCompute duration = 87.27 seconds\rparameters = intercept, age, cp, chol\rinternals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\rSummary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64\u001b[0m ⋯\rintercept -0.2821 0.1647 0.0012 20113.9419 13042.8456 1.0003 ⋯\rage 0.6003 0.1760 0.0013 18327.5449 12926.7418 1.0001 ⋯\rcp 1.0699 0.1922 0.0014 19583.1899 13534.2405 0.9999 ⋯\rchol -0.0073 0.1641 0.0012 18280.4944 12242.8280 1.0004 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rQuantiles\r\u001b[1m parameters \u001b[0m \u001b[1m 2.5% \u001b[0m \u001b[1m 25.0% \u001b[0m \u001b[1m 50.0% \u001b[0m \u001b[1m 75.0% \u001b[0m \u001b[1m 97.5% \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\rintercept -0.6118 -0.3923 -0.2817 -0.1711 0.0388\rage 0.2645 0.4792 0.5964 0.7178 0.9575\rcp 0.7106 0.9372 1.0636 1.1963 1.4603\rchol -0.3283 -0.1177 -0.0080 0.1025 0.3151\rPlot the MCMC Diagnostics plot(chain_multi, dpi=150) Summary Statistics summarystats(chain_multi) Summary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64\u001b[0m ⋯\rintercept -0.2821 0.1647 0.0012 20113.9419 13042.8456 1.0003 ⋯\rage 0.6003 0.1760 0.0013 18327.5449 12926.7418 1.0001 ⋯\rcp 1.0699 0.1922 0.0014 19583.1899 13534.2405 0.9999 ⋯\rchol -0.0073 0.1641 0.0012 18280.4944 12242.8280 1.0004 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rThank you! And that concludes this little tutorial showcasing the power of a Bayesian model and the fun of using Julia. Thank you for stopping by!\nVictor Flores\n","permalink":"http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/","summary":"Applying Turing.jl package in Julia for a probabilistic approach to a classification problem on a real-world dataset.","title":"Bayesian Logistic Regression with Julia and Turing.jl"},{"content":" Finding a Linear Relationship Between Height and Weight Using Bayesian Methods Problem Statement You have some data on the relationship between the height and weight of some people, and you want to fit a linear model of the form:\n$$y = \\alpha + \\beta x + \\varepsilon$$\nwhere $y$ is the weight, $x$ is the height, $\\alpha$ is the intercept, $\\beta$ is the slope, and $\\varepsilon$ is the error term. You want to use Bayesian inference to estimate the posterior distributions of $\\alpha$ and $\\beta$ given the data and some prior assumptions. You also want to use probabilistic programming to implement the Bayesian model and perform inference using a package like Turing.jl.\nYour task is to write the code in Julia that can generate some synthetic data (or use an existing data set), define the Bayesian linear regression model, and sample from the posterior distributions using Hamiltonian Monte Carlo (HMC).\nCredit This exercise is heavily inspired, and mostly taken from, the doggo\u0026rsquo;s tutorial. Please visit his Youtube channel here, it\u0026rsquo;s an amazing starting point for Julia programming!\nImport the Necessary Packages using LinearAlgebra, Turing, CSV, DataFrames, Plots, StatsPlots, LaTeXStrings Bayesian Workflow For this exercise, I will implement the following workflow:\nCollect data: this will be implemented by downloading the relevant data Build a Bayesian model: will use Turing.jl to build the model Infer the posterior distributions of the parameters $\\alpha$ and $\\beta$ Evaluate the fit of the model Collecting the data The data to be analyzed will be the height vs. weight data from: https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset.\nSince the dataset is too large, we will select only the first 1000 entries.\n# collect data # this data set was downloaded from kaggle: # https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset df = CSV.read(joinpath(\u0026#34;data\u0026#34;, \u0026#34;SOCR-HeightWeight.csv\u0026#34;), DataFrame) # select only 100 entries df = df[1:1000, :] first(df, 5) 5×3 DataFrameRowIndexHeight(Inches)Weight(Pounds)Int64Float64Float641165.7833112.9932271.5152136.4873369.3987153.0274468.2166142.3355567.7878144.297\r# change the column headers for easier access colnames = [\u0026#34;index\u0026#34;,\u0026#34;height\u0026#34;,\u0026#34;weight\u0026#34;]; rename!(df, Symbol.(colnames)) first(df, 5) 5×3 DataFrameRowindexheightweightInt64Float64Float641165.7833112.9932271.5152136.4873369.3987153.0274468.2166142.3355567.7878144.297\rVisualizing the Data plot_data = scatter(df.height, df.weight, legend = false, title = \u0026#34;Height vs. Weight\u0026#34;, xlabel = \u0026#34;Height (in)\u0026#34;, ylabel = \u0026#34;Weight (lb)\u0026#34; ) Building a Bayesian model with Turing.jl. First, we assume that the weight is a variable dependent on the height. Thus, we can express the Bayesian model as:\n$$y\\sim N(\\alpha + \\beta^{T}\\mathbf{X}, \\sigma^2)$$\nThe above means that we assume that the data follows a normal distribution (in this case, a multivariate normal distribution), whose standard deviation is σ and its mean is the linear relationship $\\alpha + \\beta^{T}\\mathbf{X}$.\nNext, we need to assign priors to the variables $\\alpha$, $\\beta$ and $\\sigma^2$. The latter is a measure of the uncertainty in the model.\nSo, the priors will be assigned as follows:\n$$\\alpha \\sim N(0,10)$$ $$\\beta \\sim U(0,50)$$ $$\\sigma^{2} \\sim TN(0,100;0,\\infty)$$\nThe last distribution is a truncated normal distribution bounded from 0 to $\\infty$.\n@model function blr(height, weight) # priors: α ~ Normal(0,10) # intercept β ~ Uniform(0,50) σ ~ truncated(Normal(0, 100); lower=0) # variance standard distribution # likelihood # the likelihood in this case means that I assume that the data follows a # multivariate normal distribution, whose uncertainty is σ, and its mean is the linear relationship: avg_weight = α .+ (β.*height) # build the model weight ~ MvNormal(avg_weight, σ) end blr (generic function with 2 methods)\rThe next step is to perform Bayesian inference. Crank up the Bayes!\n# crank up the bayes! model = blr(df.height, df.weight) samples = 1000 chain = sample(model, NUTS(), samples) \u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFound initial step size\r\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m ϵ = 9.765625e-5\r\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:11\u001b[39m9m\rChains MCMC chain (1000×15×1 Array{Float64, 3}):\rIterations = 501:1:1500\rNumber of chains = 1\rSamples per chain = 1000\rWall duration = 31.4 seconds\rCompute duration = 31.4 seconds\rparameters = α, β, σ\rinternals = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\rSummary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m \u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m \u001b[0m ⋯\rα -34.8414 7.6414 0.4117 344.5155 365.1189 1.0038 ⋯\rβ 2.3859 0.1124 0.0060 345.5269 345.0618 1.0039 ⋯\rσ 10.3030 0.2239 0.0100 509.4680 389.9078 1.0016 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rQuantiles\r\u001b[1m parameters \u001b[0m \u001b[1m 2.5% \u001b[0m \u001b[1m 25.0% \u001b[0m \u001b[1m 50.0% \u001b[0m \u001b[1m 75.0% \u001b[0m \u001b[1m 97.5% \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\rα -49.8948 -39.7950 -34.9188 -29.8116 -19.8403\rβ 2.1673 2.3108 2.3872 2.4580 2.6100\rσ 9.8649 10.1550 10.3018 10.4554 10.7449\rVisualizing the MCMC Diagnostics and Summarizing the Results Now that we have performed Bayesian inference using the NUTS() algorithm, we can visualize the results. Addisionally, call for a summary of the statistics of the inferred posterior distributions of $\\theta$.\nsummarize(chain) \u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m \u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m \u001b[0m ⋯\rα -34.8414 7.6414 0.4117 344.5155 365.1189 1.0038 ⋯\rβ 2.3859 0.1124 0.0060 345.5269 345.0618 1.0039 ⋯\rσ 10.3030 0.2239 0.0100 509.4680 389.9078 1.0016 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rplot(chain) Visualizing the results It is worth noting that the results from a Bayesian Linear Regression is not one single regression line, but many. From PyMC\u0026rsquo;s Generalized Linear Regression tutorial:\nIn GLMs, we do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. We can manually generate these regression lines using the posterior samples directly.\nWhat this means is that if we want to visualize all the lines that are generated by the parameter posterior distribution sample pool, we need to generate one line per sample set, and then we can plot them all. This procedure is executed next.\n# plot all the sample regressions # this method was taken from: https://www.youtube.com/watch?v=EgrrtZEVOv0\u0026amp;t=1113s for i in 1:samples α = chain[i,1,1] #chain[row, column, chain_ID] β = chain[i,2,1] σ² = chain[i,3,1] plot!(plot_data, x -\u0026gt; α + β*x, legend = false, # samples linewidth = 2, color = :orange, alpha = 0.02, # error ribbon = σ², fillalpha = 0.002 ) end\tplot_data Using the Regression Model to Make Predictions Select the heights for which we want to predict the weights and then run the prediction command from Turing.\npred_height = [62, 84, 75, 70, 71, 67] predictions = predict(blr(pred_height, missing), chain) Chains MCMC chain (1000×6×1 Array{Float64, 3}):\rIterations = 1:1:1000\rNumber of chains = 1\rSamples per chain = 1000\rparameters = weight[1], weight[2], weight[3], weight[4], weight[5], weight[6]\rinternals = Summary Statistics\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m \u001b[1m std \u001b[0m \u001b[1m mcse \u001b[0m \u001b[1m ess_bulk \u001b[0m \u001b[1m ess_tail \u001b[0m \u001b[1m rhat \u001b[0m \u001b[1m\u001b[0m ⋯\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m\u001b[0m ⋯\rweight[1] 113.6815 10.3344 0.3270 997.5393 947.2109 0.9993 ⋯\rweight[2] 165.3164 10.8352 0.3744 832.5405 818.6640 1.0008 ⋯\rweight[3] 143.8911 10.5355 0.3461 929.5467 874.2977 0.9993 ⋯\rweight[4] 132.3417 10.4836 0.3448 921.6347 943.0320 1.0007 ⋯\rweight[5] 134.7606 10.7046 0.3350 1023.8876 977.6814 1.0025 ⋯\rweight[6] 124.9423 10.2245 0.3247 993.9282 867.7391 0.9991 ⋯\r\u001b[36m 1 column omitted\u001b[0m\rQuantiles\r\u001b[1m parameters \u001b[0m \u001b[1m 2.5% \u001b[0m \u001b[1m 25.0% \u001b[0m \u001b[1m 50.0% \u001b[0m \u001b[1m 75.0% \u001b[0m \u001b[1m 97.5% \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\rweight[1] 93.9378 106.3972 113.6943 120.8093 134.9264\rweight[2] 142.4871 158.4933 165.5406 172.7313 184.7437\rweight[3] 122.8292 137.0108 144.0339 151.1920 164.2645\rweight[4] 111.8872 125.3733 132.1726 139.2690 153.7222\rweight[5] 113.9147 127.4356 135.0149 142.1375 154.5537\rweight[6] 105.3221 118.0098 125.1640 131.6011 145.2976\rVisualize the Distributions of the Predicted Weights plot(predictions) Finally, to obtain a point estimate, compute the mean weight prediction for each height.\nmean_predictions = mean(predictions) Mean\r\u001b[1m parameters \u001b[0m \u001b[1m mean \u001b[0m\r\u001b[90m Symbol \u001b[0m \u001b[90m Float64 \u001b[0m\rweight[1] 113.6815\rweight[2] 165.3164\rweight[3] 143.8911\rweight[4] 132.3417\rweight[5] 134.7606\rweight[6] 124.9423\r","permalink":"http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/","summary":"Learn the basics of Bayesian linear regression using Julia and Turing.jl. This tutorial covers model formulation, implementation, and interpretation through a practical example.","title":"Bayesian Linear Regression with Julia and Turing.jl"},{"content":"Hi! I love learning about new stuff and diving into the world of data science and machine learning. I have been exploring Bayesian modeling, Gaussian processes, and deep learning. I\u0026rsquo;m comfortable working with Julia and Python, and in a past life, MATLAB too. I enjoy creating tutorials and sharing what I learn with others. When I\u0026rsquo;m not learning about Bayesian stuff, you\u0026rsquo;ll find me swimming, running, or experimenting with new recipes in the kitchen, probably while listening to some flavor of metal 🤘.\nI\u0026rsquo;m always looking for hands-on collaborations on data science and machine learning projects. Reach out to discuss what we can build together!\n","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eHi! I love learning about new stuff and diving into the world of data science and machine learning. I have been exploring Bayesian modeling, Gaussian processes, and deep learning. I\u0026rsquo;m comfortable working with Julia and Python, and in a past life, MATLAB too. I enjoy creating tutorials and sharing what I learn with others. When I\u0026rsquo;m not learning about Bayesian stuff, you\u0026rsquo;ll find me swimming, running, or experimenting with new recipes in the kitchen, probably while listening to some flavor of metal 🤘.\u003c/p\u003e","title":"About me"}]