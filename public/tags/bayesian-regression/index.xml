<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Bayesian Regression on Victor Flores, PhD</title>
    <link>http://localhost:1313/tags/bayesian-regression/</link>
    <description>Recent content in Bayesian Regression on Victor Flores, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 24 Sep 2024 14:12:51 +0800</lastBuildDate><atom:link href="http://localhost:1313/tags/bayesian-regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bayesian Classification and Survival Analysis with NumPyro</title>
      <link>http://localhost:1313/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/</link>
      <pubDate>Tue, 24 Sep 2024 14:12:51 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/</guid>
      <description>Applying Bayesian Classification and Survival Analysis to the Cirrhosis Patient Dataset.</description>
      <content:encoded><![CDATA[<p><a href="https://colab.research.google.com/github/vflores-io/Portfolio/blob/main/Bayesian%20Methods%20Tutorials/Python/NumPyro/NP01_LogReg_Surv_Analysis/NP01_LogReg_Surv_Analysis.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<hr>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/intro.png" type="" alt="image"  /></p>
<h1 id="1-introduction">1. Introduction</h1>
<p>Cirrhosis is a progressive liver disease characterized by the replacement of healthy liver tissue with scar tissue, leading to impaired liver function. Early prediction of patient survival can significantly impact treatment decisions and improve outcomes. In this project, we employ Bayesian statistical methods to analyze and predict the survival of patients with cirrhosis using a publicly available dataset from the UCI ML Dataset Repository.</p>
<ol>
<li>
<p><strong>Bayesian Classification (Logistic Regression):</strong> We develop a Bayesian logistic regression model to predict the survival status of patients based on various clinical features. This probabilistic approach allows us to incorporate prior knowledge and quantify uncertainty in our predictions.</p>
</li>
<li>
<p><strong>Bayesian Survival Analysis:</strong> We perform a comprehensive survival analysis using Bayesian methods. We start with a basic Weibull model without covariates to understand the baseline survival function. We then introduce covariates to the Weibull model, and despite encountering challenges with this approach, we proceed to implement a log-normal model with covariates, which demonstrates improved performance. Finally, we refine the Weibull model by including selected covariates and accounting for censored data to enhance the model&rsquo;s applicability to real-world scenarios.</p>
</li>
</ol>
<p>Throughout this project, we emphasize the iterative nature of model development in Bayesian statistics and showcase how to handle practical issues that may arise during analysis.</p>
<h2 id="11-importing-packages">1.1 Importing Packages</h2>
<p>To begin our analysis, we first import the necessary Python libraries for data manipulation, visualization, and Bayesian modeling.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">sys</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s1">&#39;numpyro&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
</span></span><span class="line"><span class="cl">        <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Running on Google Colab. NumPyro will be installed in this environment.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">numpyro</span><span class="nd">@git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pyro</span><span class="o">-</span><span class="n">ppl</span><span class="o">/</span><span class="n">numpyro</span> <span class="n">arviz</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;NumPyro is already installed. Skipping installation.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Running locally. Make sure NumPyro and dependencies are installed in the environment.&#34;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Running locally. Make sure NumPyro and dependencies are installed in the environment.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpyro</span>
</span></span><span class="line"><span class="cl"><span class="n">numpyro</span><span class="o">.</span><span class="n">set_host_device_count</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">numpyro.infer</span> <span class="kn">import</span> <span class="n">MCMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">,</span> <span class="n">Predictive</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">scale</span>
</span></span></code></pre></div><h2 id="12-loading-the-dataset">1.2 Loading the Dataset</h2>
<p>We load the cirrhosis dataset, adjusting the file path depending on whether we&rsquo;re running the code on Google Colab or locally. After reading the data into a pandas DataFrame, we display the first few rows to preview the dataset.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">  <span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;https://archive.ics.uci.edu/static/public/878/data.csv&#39;</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;data/cirrhosis.csv&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>N_Days</th>
      <th>Status</th>
      <th>Drug</th>
      <th>Age</th>
      <th>Sex</th>
      <th>Ascites</th>
      <th>Hepatomegaly</th>
      <th>Spiders</th>
      <th>Edema</th>
      <th>Bilirubin</th>
      <th>Cholesterol</th>
      <th>Albumin</th>
      <th>Copper</th>
      <th>Alk_Phos</th>
      <th>SGOT</th>
      <th>Tryglicerides</th>
      <th>Platelets</th>
      <th>Prothrombin</th>
      <th>Stage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>400</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>21464</td>
      <td>F</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>14.5</td>
      <td>261.0</td>
      <td>2.60</td>
      <td>156.0</td>
      <td>1718.0</td>
      <td>137.95</td>
      <td>172.0</td>
      <td>190.0</td>
      <td>12.2</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4500</td>
      <td>C</td>
      <td>D-penicillamine</td>
      <td>20617</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>1.1</td>
      <td>302.0</td>
      <td>4.14</td>
      <td>54.0</td>
      <td>7394.8</td>
      <td>113.52</td>
      <td>88.0</td>
      <td>221.0</td>
      <td>10.6</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1012</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>25594</td>
      <td>M</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
      <td>S</td>
      <td>1.4</td>
      <td>176.0</td>
      <td>3.48</td>
      <td>210.0</td>
      <td>516.0</td>
      <td>96.10</td>
      <td>55.0</td>
      <td>151.0</td>
      <td>12.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1925</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>19994</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>S</td>
      <td>1.8</td>
      <td>244.0</td>
      <td>2.54</td>
      <td>64.0</td>
      <td>6121.8</td>
      <td>60.63</td>
      <td>92.0</td>
      <td>183.0</td>
      <td>10.3</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1504</td>
      <td>CL</td>
      <td>Placebo</td>
      <td>13918</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>3.4</td>
      <td>279.0</td>
      <td>3.53</td>
      <td>143.0</td>
      <td>671.0</td>
      <td>113.15</td>
      <td>72.0</td>
      <td>136.0</td>
      <td>10.9</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="2-preparing-and-cleaning-the-data">2. Preparing and Cleaning the Data</h1>
<p>We examine the dataset&rsquo;s structure and data types using the <code>data.info()</code> method. This provides an overview of the dataset, including the number of entries, columns, non-null counts, data types, and memory usage. This step is crucial for identifying missing values and planning how to handle them in our analysis.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 20 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   ID             418 non-null    int64  
 1   N_Days         418 non-null    int64  
 2   Status         418 non-null    object 
 3   Drug           312 non-null    object 
 4   Age            418 non-null    int64  
 5   Sex            418 non-null    object 
 6   Ascites        312 non-null    object 
 7   Hepatomegaly   312 non-null    object 
 8   Spiders        312 non-null    object 
 9   Edema          418 non-null    object 
 10  Bilirubin      418 non-null    float64
 11  Cholesterol    284 non-null    float64
 12  Albumin        418 non-null    float64
 13  Copper         310 non-null    float64
 14  Alk_Phos       312 non-null    float64
 15  SGOT           312 non-null    float64
 16  Tryglicerides  282 non-null    float64
 17  Platelets      407 non-null    float64
 18  Prothrombin    416 non-null    float64
 19  Stage          412 non-null    float64
dtypes: float64(10), int64(3), object(7)
memory usage: 65.4+ KB
</code></pre>
<p>To assess the extent of missing data in our dataset, we calculate the total number of NaN (missing) values.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># check for nan</span>
</span></span><span class="line"><span class="cl"><span class="n">nan_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of NaN values:&#39;</span><span class="p">,</span> <span class="n">nan_values</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>Number of NaN values: 1033
</code></pre>
<p>This reveals that there are <strong>1,033 missing values</strong> across various columns. Understanding the amount and distribution of missing data is crucial for deciding how to handle it in our analysis, whether through imputation, removal, or other methods.</p>
<p>To pinpoint which columns contain missing values, we identify and list all columns with NaN entries. Recognizing these columns is essential for data cleaning and preprocessing steps. To handle the missing data, we choose to fill all NaN values with zeros using the <code>fillna(0)</code> method. This approach ensures that our dataset is complete and ready for analysis, without excluding any records due to missing values. After performing this operation, we display the updated dataset to confirm that all missing values have been addressed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># find columns with NaN values</span>
</span></span><span class="line"><span class="cl"><span class="n">nan_columns</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Columns with NaN values:</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">nan_columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># fill NaN values with 0</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span>
</span></span></code></pre></div><pre><code>Columns with NaN values:
 ['Drug', 'Ascites', 'Hepatomegaly', 'Spiders', 'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>N_Days</th>
      <th>Status</th>
      <th>Drug</th>
      <th>Age</th>
      <th>Sex</th>
      <th>Ascites</th>
      <th>Hepatomegaly</th>
      <th>Spiders</th>
      <th>Edema</th>
      <th>Bilirubin</th>
      <th>Cholesterol</th>
      <th>Albumin</th>
      <th>Copper</th>
      <th>Alk_Phos</th>
      <th>SGOT</th>
      <th>Tryglicerides</th>
      <th>Platelets</th>
      <th>Prothrombin</th>
      <th>Stage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>400</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>21464</td>
      <td>F</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>14.5</td>
      <td>261.0</td>
      <td>2.60</td>
      <td>156.0</td>
      <td>1718.0</td>
      <td>137.95</td>
      <td>172.0</td>
      <td>190.0</td>
      <td>12.2</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4500</td>
      <td>C</td>
      <td>D-penicillamine</td>
      <td>20617</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>1.1</td>
      <td>302.0</td>
      <td>4.14</td>
      <td>54.0</td>
      <td>7394.8</td>
      <td>113.52</td>
      <td>88.0</td>
      <td>221.0</td>
      <td>10.6</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1012</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>25594</td>
      <td>M</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
      <td>S</td>
      <td>1.4</td>
      <td>176.0</td>
      <td>3.48</td>
      <td>210.0</td>
      <td>516.0</td>
      <td>96.10</td>
      <td>55.0</td>
      <td>151.0</td>
      <td>12.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1925</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>19994</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>S</td>
      <td>1.8</td>
      <td>244.0</td>
      <td>2.54</td>
      <td>64.0</td>
      <td>6121.8</td>
      <td>60.63</td>
      <td>92.0</td>
      <td>183.0</td>
      <td>10.3</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1504</td>
      <td>CL</td>
      <td>Placebo</td>
      <td>13918</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>3.4</td>
      <td>279.0</td>
      <td>3.53</td>
      <td>143.0</td>
      <td>671.0</td>
      <td>113.15</td>
      <td>72.0</td>
      <td>136.0</td>
      <td>10.9</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>413</th>
      <td>414</td>
      <td>681</td>
      <td>D</td>
      <td>0</td>
      <td>24472</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>1.2</td>
      <td>0.0</td>
      <td>2.96</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>174.0</td>
      <td>10.9</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>414</th>
      <td>415</td>
      <td>1103</td>
      <td>C</td>
      <td>0</td>
      <td>14245</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>0.9</td>
      <td>0.0</td>
      <td>3.83</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>180.0</td>
      <td>11.2</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>415</th>
      <td>416</td>
      <td>1055</td>
      <td>C</td>
      <td>0</td>
      <td>20819</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>1.6</td>
      <td>0.0</td>
      <td>3.42</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>143.0</td>
      <td>9.9</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>416</th>
      <td>417</td>
      <td>691</td>
      <td>C</td>
      <td>0</td>
      <td>21185</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>0.8</td>
      <td>0.0</td>
      <td>3.75</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>269.0</td>
      <td>10.4</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>417</th>
      <td>418</td>
      <td>976</td>
      <td>C</td>
      <td>0</td>
      <td>19358</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>0.7</td>
      <td>0.0</td>
      <td>3.29</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>350.0</td>
      <td>10.6</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
<p>418 rows × 20 columns</p>
</div>
<h3 id="21-data-preprocessing">2.1 Data Preprocessing</h3>
<p>To prepare the dataset for modeling, we perform several preprocessing steps, including handling categorical variables, scaling numerical features, and encoding target variables.</p>
<p><strong>1. Extracting and Processing Categorical Data:</strong></p>
<ul>
<li>
<p><strong>Selecting Categorical Columns:</strong> We extract all columns with data type <code>'object'</code>, which represent categorical variables in the dataset.</p>
</li>
<li>
<p><strong>Including <code>'Stage'</code> as Categorical:</strong> Although the <code>'Stage'</code> column is numeric, it represents categorical stages of cirrhosis, so we include it in the categorical data.</p>
</li>
<li>
<p><strong>Mapping <code>'Status'</code> to Binary Values:</strong> The <code>'Status'</code> column indicates the patient&rsquo;s survival status with values <code>'C'</code> (Censored), <code>'CL'</code> (Censored Liver), and <code>'D'</code> (Deceased). We map these to binary values for modeling, where <code>'C'</code> and <code>'CL'</code> are mapped to 0 (survived), and <code>'D'</code> is mapped to 1 (did not survive).</p>
</li>
</ul>
<p><strong>2. Extracting and Scaling Numerical Data:</strong></p>
<ul>
<li>
<p><strong>Selecting Numerical Features:</strong> We select all numerical columns from the dataset, excluding <code>'ID'</code>, <code>'N_Days'</code>, and <code>'Stage'</code>.</p>
<ul>
<li><strong><code>'ID'</code>:</strong> A unique identifier for each patient, which does not contribute to the model and can be excluded.</li>
<li><strong><code>'N_Days'</code>:</strong> Represents the number of days of follow-up and will be used as the target variable in survival analysis.</li>
<li><strong><code>'Stage'</code>:</strong> Already included as a categorical variable.</li>
</ul>
</li>
<li>
<p><strong>Scaling Numerical Features:</strong> We scale the numerical features using standard scaling (mean = 0, variance = 1) to normalize the data, which can improve the performance of many machine learning models.</p>
</li>
</ul>
<p><strong>3. Encoding Categorical Variables:</strong></p>
<ul>
<li><strong>Label Encoding:</strong> We encode the categorical variables into numerical format using label encoding, which assigns unique integer values to each category in a column.</li>
</ul>
<p><strong>4. Combining Processed Data:</strong></p>
<ul>
<li><strong>Concatenating DataFrames:</strong> We concatenate the encoded categorical data, the scaled numerical data, and the <code>'ID'</code>, <code>'N_Days'</code>, and <code>'Stage'</code> columns into a single DataFrame. This consolidated dataset is now ready for modeling.</li>
</ul>
<p>Finally, we display the first few rows of the processed dataset to verify that the preprocessing steps have been applied correctly.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># separate categorical and numerical features</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span> <span class="o">=</span> <span class="s1">&#39;object&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># include &#39;Stage&#39; in categorical data</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span><span class="p">[</span><span class="s1">&#39;Stage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Stage&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># map status to binary vlaues</span>
</span></span><span class="line"><span class="cl"><span class="n">status_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;CL&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">categorical_data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">status_mapping</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># select numerical features, excluding &#39;Stage&#39;, &#39;ID&#39; and &#39;N_Days&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;Stage&#39; is actually a categorical feature</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;ID&#39; has no relevance</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;N_Days&#39; is the target value</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span> <span class="o">=</span> <span class="s1">&#39;number&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Days&#39;</span><span class="p">,</span> <span class="s1">&#39;Stage&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># scale numerical features</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_scaled</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">numerical_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_scaled_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">numerical_scaled</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">numerical_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># label encoder</span>
</span></span><span class="line"><span class="cl"><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span> <span class="o">=</span> <span class="n">categorical_data</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># concatenate encoded categorical and scaled numerical features</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">categorical_data</span><span class="p">,</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Days&#39;</span><span class="p">,</span> <span class="s1">&#39;Stage&#39;</span><span class="p">]],</span> <span class="n">numerical_scaled_df</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Status</th>
      <th>Drug</th>
      <th>Sex</th>
      <th>Ascites</th>
      <th>Hepatomegaly</th>
      <th>Spiders</th>
      <th>Edema</th>
      <th>Stage</th>
      <th>ID</th>
      <th>N_Days</th>
      <th>...</th>
      <th>Age</th>
      <th>Bilirubin</th>
      <th>Cholesterol</th>
      <th>Albumin</th>
      <th>Copper</th>
      <th>Alk_Phos</th>
      <th>SGOT</th>
      <th>Tryglicerides</th>
      <th>Platelets</th>
      <th>Prothrombin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>1</td>
      <td>400</td>
      <td>...</td>
      <td>0.768941</td>
      <td>2.562152</td>
      <td>0.038663</td>
      <td>-2.114296</td>
      <td>0.981918</td>
      <td>0.116853</td>
      <td>0.642305</td>
      <td>1.110012</td>
      <td>-0.572406</td>
      <td>1.20688</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>2</td>
      <td>4500</td>
      <td>...</td>
      <td>0.546706</td>
      <td>-0.481759</td>
      <td>0.198060</td>
      <td>1.513818</td>
      <td>-0.216383</td>
      <td>2.902613</td>
      <td>0.304654</td>
      <td>0.048897</td>
      <td>-0.277943</td>
      <td>-0.06384</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>1012</td>
      <td>...</td>
      <td>1.852567</td>
      <td>-0.413611</td>
      <td>-0.291793</td>
      <td>-0.041088</td>
      <td>1.616312</td>
      <td>-0.473001</td>
      <td>0.063889</td>
      <td>-0.367969</td>
      <td>-0.942860</td>
      <td>1.04804</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>1925</td>
      <td>...</td>
      <td>0.383244</td>
      <td>-0.322748</td>
      <td>-0.027428</td>
      <td>-2.255651</td>
      <td>-0.098903</td>
      <td>2.277917</td>
      <td>-0.426348</td>
      <td>0.099427</td>
      <td>-0.638898</td>
      <td>-0.30210</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>5</td>
      <td>1504</td>
      <td>...</td>
      <td>-1.210972</td>
      <td>0.040704</td>
      <td>0.108642</td>
      <td>0.076708</td>
      <td>0.829193</td>
      <td>-0.396938</td>
      <td>0.299540</td>
      <td>-0.153220</td>
      <td>-1.085342</td>
      <td>0.17442</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>
<h3 id="22-exploratory-data-analysis-eda">2.2 Exploratory Data Analysis (EDA)</h3>
<p>With the data preprocessed, we now perform Exploratory Data Analysis to gain insights into the dataset. This involves visualizing the distributions of key variables and exploring patterns that might inform our modeling approach.</p>
<h4 id="visualizing-survival-status">Visualizing Survival Status</h4>
<p>We begin by examining the distribution of the &lsquo;Status&rsquo; variable, which indicates whether a patient survived (0) or did not survive (1). Understanding the class balance is crucial for classification tasks and can impact the performance of our predictive models.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># count plot</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;Status&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;lightblue&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Status&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_16_0.png" type="" alt="png"  /></p>
<p>The bar plot above shows the distribution of patient survival status in the dataset. The <code>'Status'</code> variable is binary, with 0 representing patients who survived and 1 representing those who did not survive. The plot reveals that there are more patients who survived (labeled as 0) compared to those who did not survive (labeled as 1), indicating a slight class imbalance in the dataset.</p>
<h4 id="correlation-heatmap">Correlation Heatmap</h4>
<p>Next, we examine the relationships between the numerical features by visualizing their pairwise correlations using a heatmap. A correlation heatmap helps us understand how different variables are related to one another, which is particularly useful for identifying multicollinearity or discovering variables that might be strong predictors of survival.</p>
<p>In this plot, the correlation coefficients range from -1 to 1:</p>
<ul>
<li><strong>Positive correlation (values closer to 1)</strong> indicates that as one variable increases, the other also increases.</li>
<li><strong>Negative correlation (values closer to -1)</strong> indicates that as one variable increases, the other decreases.</li>
<li><strong>Values near 0</strong> suggest little to no linear relationship between the variables.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># correlation heatmap</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">numerical_data</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Correlation Heatmap&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_19_0.png" type="" alt="png"  /></p>
<p><strong>Interpretation of the Correlation Heatmap</strong></p>
<p>The heatmap reveals several interesting relationships between the features. For example:</p>
<ul>
<li><strong>Cholesterol and SGOT</strong> have a relatively high positive correlation (0.61), indicating that as cholesterol levels increase, SGOT tends to increase as well.</li>
<li><strong>Cholesterol and Triglycerides</strong> also show a strong positive correlation (0.63), which suggests that these two features might capture similar information.</li>
<li><strong>Bilirubin and Copper</strong> exhibit a moderate positive correlation (0.36), which could indicate some physiological link between these features.</li>
</ul>
<p>By identifying correlated features, we can make informed decisions on feature selection for modeling to avoid multicollinearity or redundancy in the predictors.</p>
<h4 id="correlation-of-features-with-the-target">Correlation of Features with the Target</h4>
<p>To further investigate the predictive power of each feature, we calculate the absolute correlation between each feature and the target variable, <code>Status</code>, which represents patient survival. Correlation with the target helps us identify which features are most strongly associated with survival and might be valuable predictors in our classification and survival models.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># calculate correlations with the target variable</span>
</span></span><span class="line"><span class="cl"><span class="n">correlations</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Days&#39;</span><span class="p">,</span> <span class="s1">&#39;Stage&#39;</span><span class="p">,</span> <span class="s1">&#39;Status&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">corrwith</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate colors</span>
</span></span><span class="line"><span class="cl"><span class="n">n_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">correlations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot horizontal bar plot for feature correlations with the target</span>
</span></span><span class="line"><span class="cl"><span class="n">correlations</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Feature Correlations with the Target&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Correlation&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_22_0.png" type="" alt="png"  /></p>
<p><strong>Plotting Feature Correlations</strong></p>
<p>In the horizontal bar plot above, the features are sorted by their correlation with the target variable. Features with higher correlations are likely to be more informative in predicting the patient&rsquo;s survival status.</p>
<p><strong>Interpretation of the Plot</strong></p>
<ul>
<li><strong>Bilirubin</strong>, <strong>Edema</strong>, and <strong>Copper</strong> are among the features with the strongest correlation to the target, suggesting they may be highly influential in determining survival outcomes.</li>
<li><strong>Sex</strong> and <strong>Drug</strong> show relatively weak correlations, indicating they might have less impact on survival predictions.</li>
<li>This plot will help guide the selection of features in our model, focusing on those with higher correlations for better predictive performance.</li>
</ul>
<h4 id="pair-plot-of-selected-features">Pair Plot of Selected Features</h4>
<p>To explore the relationships between the most significant features and the target variable, we create a pair plot. This type of plot is particularly useful for visualizing the pairwise relationships between features, along with their distributions, while also distinguishing between different classes of the target variable.</p>
<p><strong>Selecting Top Features</strong></p>
<p>We focus on the top features identified from the previous correlation analysis to avoid overloading the plot with too many variables. The selected features have the strongest correlation with the target variable, <code>Status</code>, and are likely to be the most informative for our analysis.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># pairplot</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># first, take care of the warning:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># FutureWarning: use_inf_as_na option is deprecated and will be removed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># in a future version. Convert inf values to NaN before operating instead.</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># there are too many features, so we only plot some of them</span>
</span></span><span class="line"><span class="cl"><span class="c1"># we have previously assigned the correlations and sorted them</span>
</span></span><span class="line"><span class="cl"><span class="c1"># so now we select the top features</span>
</span></span><span class="line"><span class="cl"><span class="n">top_features</span> <span class="o">=</span> <span class="n">correlations</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">7</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">selected_features</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">selected_features</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s1">&#39;Status&#39;</span><span class="p">);</span>
</span></span></code></pre></div><pre><code>['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos']
</code></pre>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_25_1.png" type="" alt="png"  /></p>
<p><strong>Interpretation of the Pair Plot</strong></p>
<p>In the pair plot above, we observe scatter plots for each pair of selected features, with data points colored by the patient&rsquo;s survival status (<code>Status</code>):</p>
<ul>
<li><strong>Blue</strong> represents patients who survived (<code>Status = 0</code>).</li>
<li><strong>Orange</strong> represents patients who did not survive (<code>Status = 1</code>).</li>
</ul>
<p>Diagonal plots show the distribution of individual features, while off-diagonal plots reveal the relationships between different features. This helps us identify potential clusters, outliers, and separations between survival classes based on feature combinations. For example, we can observe how <strong>Bilirubin</strong> and <strong>Edema</strong> may differ between the two groups, providing insights into their predictive power.</p>
<h1 id="3-bayesian-logistic-regression-for-survival-classification">3. Bayesian Logistic Regression for Survival Classification</h1>
<p>In this section, we implement a Bayesian logistic regression model to predict patient survival based on clinical features. The logistic regression model is a widely used method for binary classification tasks. In a Bayesian framework, the model allows us to incorporate prior knowledge and quantify uncertainty in the predictions, making it especially suitable for medical applications where uncertainty plays a crucial role.</p>
<h2 id="31-defining-the-bayesian-logistic-regression-model">3.1 Defining the Bayesian Logistic Regression Model</h2>
<p>The logistic regression model predicts the probability of an event (in this case, patient survival) occurring. The model&rsquo;s structure can be described as:</p>
<p>\begin{aligned}
\text{logit}(P(y = 1 \mid X)) = \alpha + X\beta
\end{aligned}</p>
<p>Where:</p>
<ul>
<li>$ y $ is the binary outcome (survival: 0 or 1).</li>
<li>$ X $ is the matrix of features (predictors).</li>
<li>$ \alpha $ is the intercept term (a scalar).</li>
<li>$ \beta $ is the vector of coefficients corresponding to the features in $ X $.</li>
<li>$ \text{logit}(p) $ is the log-odds transformation: $ \text{logit}(p) = \log\left(\frac{p}{1 - p}\right) $.</li>
</ul>
<p>The posterior distribution for the model parameters $ \alpha $ and $ \beta $ is obtained using Bayesian inference, which combines prior distributions with the likelihood of the data.</p>
<h3 id="code-explanation">Code Explanation</h3>
<p>In the code below, we define the Bayesian logistic regression model using <a href="https://num.pyro.ai/en/stable/index.html"><strong>NumPyro</strong></a>:</p>
<ul>
<li>
<p><strong>Priors:</strong> We assume normal priors for both the intercept term $ \alpha $ and the coefficients $ \beta $. These priors reflect our belief about the parameters before observing the data:</p>
<ul>
<li>$ \alpha \sim \mathcal{N}(0, 1) $</li>
<li>$ \beta_j \sim \mathcal{N}(0, 1) $ for each feature $ j $.</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong> The likelihood function specifies how the data (observed outcomes) are generated given the parameters. In this case, the likelihood follows a Bernoulli distribution, where the probability of success (survival) is given by the logistic function applied to the linear combination of the features.</p>
</li>
</ul>
<p>\begin{aligned}
P(y_i = 1 \mid X_i, \alpha, \beta) = \frac{1}{1 + \exp^{-(\alpha + X_i\beta)}}
\end{aligned}</p>
<p>The code block defines the model structure, including priors and likelihood, and will be used in the next steps for inference using MCMC (Markov Chain Monte Carlo).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># define the bayesian model</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">cirrhosis_classification_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">alpha</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                              <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]),</span>
</span></span><span class="line"><span class="cl">                              <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
</span></span><span class="line"><span class="cl">                          <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># define likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="preparing-the-data-for-modeling">Preparing the Data for Modeling</h3>
<p>Before fitting the Bayesian logistic regression model, we need to prepare the input data. We use the top features identified from the correlation analysis as our predictors, and the <code>Status</code> column as the binary outcome (survival).</p>
<ul>
<li><strong><code>X</code>:</strong> The matrix of predictor variables (features), consisting of the top correlated features selected during EDA.</li>
<li><strong><code>y</code>:</strong> The target variable, representing the patient&rsquo;s survival status (0 for survived, 1 for not survived).</li>
</ul>
<p>We then check the shape of <code>X</code> to confirm that it has the correct dimensions for modeling (rows corresponding to patients and columns corresponding to features).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prepare the data</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">()]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span></code></pre></div><pre><code>(418, 7)
</code></pre>
<h3 id="visualizing-the-model-structure">Visualizing the Model Structure</h3>
<p>To better understand the structure of our Bayesian logistic regression model, we generate a graphical representation using <code>numpyro.render_model()</code>. This function provides a clear visualization of the model&rsquo;s components, including the priors, likelihood, and data dependencies.</p>
<p>In the rendered graph, we can see:</p>
<ul>
<li><strong>Priors:</strong> The intercept <code>alpha</code> and coefficients <code>beta</code> are sampled from normal distributions.</li>
<li><strong>Likelihood:</strong> The binary target variable <code>y</code> follows a Bernoulli distribution with probabilities defined by the logistic transformation of the linear combination of features (<code>logits</code>).</li>
</ul>
<p>This graphical model representation helps ensure that the model is correctly specified and provides insights into how the data interacts with the parameters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">numpyro</span><span class="o">.</span><span class="n">render_model</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">,</span> <span class="n">model_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_32_0.svg" type="" alt="svg"  /></p>
<h3 id="32-running-mcmc-inference">3.2 Running MCMC Inference</h3>
<p>We use <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong> to perform Bayesian inference on our logistic regression model. MCMC is a powerful method for sampling from the posterior distribution of our model&rsquo;s parameters, especially in cases where analytical solutions are intractable.</p>
<ul>
<li><strong><code>NUTS</code>:</strong> A variant of Hamiltonian Monte Carlo (HMC) that automatically tunes the step size and trajectory length, improving the efficiency of the sampling process.</li>
<li><strong><code>num_warmup</code>:</strong> The number of warm-up steps (burn-in period) where the sampler adapts to the posterior distribution.</li>
<li><strong><code>num_samples</code>:</strong> The number of samples to draw from the posterior after the warm-up phase.</li>
<li><strong><code>num_chains</code>:</strong> The number of independent MCMC chains to run in parallel, allowing us to evaluate the convergence of the model.</li>
</ul>
<p>We then execute the sampling procedure using the <code>run()</code> method, which generates samples from the posterior distribution of the model parameters. After sampling, we print the summary of the results, which includes posterior means, standard deviations, and diagnostics such as effective sample size (ESS) and R-hat (a measure of convergence).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
     alpha     -0.54      0.14     -0.54     -0.76     -0.32   6398.50      1.00
   beta[0]      1.21      0.21      1.21      0.85      1.55   6336.86      1.00
   beta[1]      0.65      0.30      0.65      0.11      1.09   4819.01      1.00
   beta[2]      0.32      0.15      0.32      0.06      0.55   6200.94      1.00
   beta[3]      0.44      0.15      0.44      0.21      0.69   6358.73      1.00
   beta[4]     -0.14      0.13     -0.15     -0.36      0.07   6798.94      1.00
   beta[5]      0.64      0.14      0.64      0.41      0.86   7289.60      1.00
   beta[6]      0.41      0.13      0.40      0.19      0.62   6308.57      1.00

Number of divergences: 0
</code></pre>
<h4 id="mcmc-inference-summary">MCMC Inference Summary</h4>
<p>The output of the MCMC inference provides a summary of the posterior distributions for each parameter in the model, including the intercept (<code>alpha</code>) and the coefficients for the selected features (<code>beta</code> values). Each row represents a parameter, and the summary includes the following key statistics:</p>
<ul>
<li><strong>mean:</strong> The mean of the posterior distribution, which gives us a point estimate of the parameter.</li>
<li><strong>std:</strong> The standard deviation of the posterior, indicating the uncertainty in the parameter estimate.</li>
<li><strong>median:</strong> The median of the posterior distribution, often used as a robust point estimate.</li>
<li><strong>5.0% and 95.0% percentiles:</strong> The lower and upper bounds of the 90% credible interval, giving us a range within which the true value of the parameter is likely to lie with 90% probability.</li>
<li><strong>n_eff:</strong> The effective sample size, which quantifies the amount of independent information in the MCMC samples. A higher value suggests that the chain is well-mixed and the samples are less correlated.</li>
<li><strong>r_hat:</strong> The R-hat statistic, which measures the convergence of the MCMC chains. Values close to 1 indicate good convergence, meaning the chains have converged to the same distribution.</li>
</ul>
<h4 id="interpretation">Interpretation</h4>
<ul>
<li>
<p><strong><code>alpha</code>:</strong> The intercept has a posterior mean of -0.54 with a standard deviation of 0.14, indicating moderate uncertainty. The 90% credible interval ranges from -0.76 to -0.32.</p>
</li>
<li>
<p><strong><code>beta[0]</code>:</strong> This coefficient has the highest posterior mean (1.21) and the smallest credible interval, suggesting that it has a strong positive effect on the probability of survival.</p>
</li>
<li>
<p><strong><code>beta[1]</code> to <code>beta[6]</code>:</strong> These coefficients represent the effects of the top features on survival. The values range between -0.14 and 0.65, indicating varying levels of influence. For example, <code>beta[5]</code> (0.64) and <code>beta[6]</code> (0.41) show relatively strong positive associations with the target variable.</p>
</li>
<li>
<p><strong>Effective Sample Size (n_eff):</strong> All parameters have high effective sample sizes (in the thousands), indicating that the MCMC sampler performed efficiently and produced a large number of independent samples.</p>
</li>
<li>
<p><strong>R-hat:</strong> The R-hat values are all 1.00, confirming that the MCMC chains have converged successfully for each parameter.</p>
</li>
</ul>
<p>The inference results show that the model has converged well, and we have meaningful posterior estimates for the model parameters. The posterior means and credible intervals provide insights into how the features are associated with the probability of survival, with several coefficients (e.g., <code>beta[0]</code>, <code>beta[5]</code>) standing out as strong predictors.</p>
<h3 id="33-trace-and-pair-plots-for-mcmc-diagnostics">3.3 Trace and Pair Plots for MCMC Diagnostics</h3>
<p>To assess the performance and convergence of the MCMC sampler, we generate trace plots and pair plots of the sampled posterior distributions. These plots allow us to visually inspect how well the sampler explored the parameter space and whether there were any issues, such as divergences.</p>
<ol>
<li>
<p><strong>Trace Plot:</strong>
The trace plot shows the sampled values of each parameter across the MCMC iterations. Ideally, we expect to see &ldquo;well-mixed&rdquo; chains, where the samples quickly explore the parameter space without getting stuck, indicating good convergence.</p>
</li>
<li>
<p><strong>Pair Plot:</strong>
The pair plot visualizes the relationships between the parameters by plotting their joint posterior distributions. We also include information about divergences, which can indicate potential problems with the sampling process. Divergences suggest that the sampler struggled to explore certain regions of the posterior, often due to ill-specified priors or likelihoods.</p>
</li>
</ol>
<p>These plots provide valuable diagnostics for ensuring that the MCMC sampling was successful and that the posterior estimates are reliable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">trace</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_37_0.png" type="" alt="png"  /></p>
<h4 id="interpreting-the-trace-plot">Interpreting the Trace Plot</h4>
<p>The <strong>trace plot</strong> shows the sampled values of each parameter (both <code>alpha</code> and <code>beta</code> coefficients) across the iterations of the MCMC chains. The left-hand side of each panel displays the posterior distribution (density plot), while the right-hand side shows the trace of the samples.</p>
<ul>
<li>
<p><strong>Posterior Distributions:</strong> Each parameter&rsquo;s posterior distribution appears well-behaved and roughly Gaussian, with all MCMC chains overlapping significantly. This indicates good convergence across the different chains.</p>
</li>
<li>
<p><strong>Trace of Samples:</strong> The trace plots show that the MCMC samples fluctuate around a stable mean, with no obvious trends or drifts. This behavior suggests that the chains have mixed well and are sampling from the stationary distribution, which is essential for reliable posterior estimates.</p>
</li>
</ul>
<p>Overall, the trace plot confirms that the model parameters have converged and the posterior distributions are stable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">divergences</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_39_0.png" type="" alt="png"  /></p>
<h4 id="interpreting-the-pair-plot">Interpreting the Pair Plot</h4>
<p>The <strong>pair plot</strong> shows pairwise relationships between the parameters (<code>alpha</code> and <code>beta</code> coefficients), with each dot representing a sample from the posterior distribution. The diagonal plots display the marginal distributions for each parameter, while the off-diagonal scatter plots show the joint distributions between pairs of parameters.</p>
<ul>
<li>
<p><strong>Joint Distributions:</strong> The scatter plots between parameter pairs show no strong patterns, such as high correlations or dependencies. This suggests that the parameters are relatively independent of each other, which is a positive sign for the model&rsquo;s stability and interpretability.</p>
</li>
<li>
<p><strong>Divergences:</strong> No major divergences or problematic regions are visible in the joint distributions, indicating that the sampler did not struggle with any particular combination of parameters.</p>
</li>
</ul>
<p>The pair plot helps us visually confirm that the posterior samples form well-behaved distributions, with no obvious issues like strong correlations or divergences. Together, the trace and pair plots provide strong evidence that the MCMC sampling has performed effectively and that the posterior estimates are trustworthy.</p>
<h3 id="34-prior-predictive-checks">3.4 Prior Predictive Checks</h3>
<p>Before analyzing the posterior predictive distribution, it is important to first check the prior predictive distribution to ensure that our prior assumptions make sense. A <strong>prior predictive check</strong> involves generating data from the model using only the priors, without any influence from the observed data. This allows us to see if the priors we have chosen are reasonable and if they align with plausible outcomes.</p>
<ul>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to sample from the prior distribution of our Bayesian model.</li>
<li><strong><code>y_prior</code>:</strong> This contains samples generated from the prior distribution. These are drawn without considering the observed data, giving us a sense of the expected outcomes under the prior assumptions.</li>
<li><strong><code>prior_data</code>:</strong> The prior samples are converted into an <code>InferenceData</code> object, which allows us to use <strong>ArviZ</strong>&rsquo;s plotting utilities.</li>
<li><strong>Plotting Prior Predictive Samples:</strong> The prior predictive plot shows the range of outcomes generated purely from the prior distributions.</li>
</ul>
<p>This check ensures that our prior assumptions are not unrealistic and that the prior distributions provide a reasonable range of expected outcomes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># get the prior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">y_prior</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to InferenceData</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">prior</span> <span class="o">=</span> <span class="n">y_prior</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the prior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">prior_data</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;prior&#39;</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_42_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-prior-predictive-check">Interpretation of the Prior Predictive Check</h4>
<p>The prior predictive check plot shows the range of possible outcomes (in blue) generated from the model based purely on the prior distributions. The dashed orange line represents the mean of the prior predictive distribution, while the solid black line indicates the observed data.</p>
<ul>
<li>
<p><strong>Prior Predictive Range:</strong> The blue lines show a wide spread of potential outcomes, indicating that the priors we defined for the model parameters allow for a broad range of values for the binary target variable (<code>y</code>), which corresponds to patient survival. This ensures that the priors are not overly restrictive.</p>
</li>
<li>
<p><strong>Comparison to Observed Data:</strong> The observed data (black lines) mostly align within the range of the prior predictive distribution, suggesting that the priors are reasonable and that the model is well-calibrated to allow for plausible outcomes before even fitting to the actual data.</p>
</li>
</ul>
<p>Overall, this plot suggests that the prior distributions are neither too narrow nor too unrealistic, providing a good foundation for the next step: posterior inference based on observed data.</p>
<h3 id="35-posterior-predictive-check">3.5 Posterior Predictive Check</h3>
<p>After fitting the model to the observed data, we perform a <strong>posterior predictive check</strong>. This step involves generating new data from the posterior distribution of the model, which incorporates both the prior information and the observed data. By comparing the posterior predictive distribution to the actual observed outcomes, we can assess how well the model fits the data.</p>
<ul>
<li><strong><code>posterior_samples</code>:</strong> These are the samples drawn from the posterior distribution after fitting the model to the data using MCMC.</li>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to generate predictions based on the posterior samples.</li>
<li><strong>Posterior Predictive Distribution:</strong> The plot visualizes the range of predicted values from the posterior distribution, along with the observed data for comparison.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get samples from the posterior</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define a Predictive class</span>
</span></span><span class="line"><span class="cl"><span class="n">predictive</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># get the samples</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">predictive</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to inference data</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">posterior_data</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;posterior&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_45_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-posterior-predictive-plot">Interpretation of the Posterior Predictive Plot</h4>
<p>In the resulting plot:</p>
<ul>
<li>The <strong>blue lines</strong> represent the range of possible outcomes generated from the posterior predictive distribution.</li>
<li>The <strong>solid black line</strong> shows the observed data.</li>
<li>The <strong>dashed orange line</strong> indicates the posterior predictive mean.</li>
</ul>
<p>In contrast to the prior predictive plot, the posterior predictive distribution is now informed by the data:</p>
<ul>
<li>The posterior predictive outcomes are much more closely aligned with the observed data, indicating that the model has successfully learned from the data.</li>
<li>The <strong>posterior predictive mean</strong> closely matches the observed values, showing that the model is well-calibrated and provides accurate predictions for the binary survival outcomes.</li>
</ul>
<p>This posterior predictive check confirms that the model is capable of producing predictions that are consistent with the observed data, providing confidence in its predictive power.</p>
<h3 id="36-visualizing-the-logistic-regression-sigmoid-curves">3.6 Visualizing the Logistic Regression Sigmoid Curves</h3>
<p>To better understand the relationship between <strong>Bilirubin</strong> (one of the top predictive features) and the <strong>survival status</strong> of patients, we plot the sigmoid function for each of the posterior samples. The sigmoid curve represents the probability of survival as a function of the Bilirubin levels.</p>
<ul>
<li>
<p><strong>Sigmoid Function:</strong> The logistic regression model uses the sigmoid function to transform the linear combination of features into a probability:</p>
<p>\begin{aligned}
\text{sigmoid}(x) = \frac{1}{1 + \exp{-(\alpha + x \cdot \beta)}}
\end{aligned}</p>
<p>where $ \alpha $ is the intercept and $ \beta $ is the coefficient for Bilirubin in this case.</p>
</li>
<li>
<p><strong>Posterior Samples:</strong> We draw several samples from the posterior distribution of the model parameters (<code>alpha</code> and <code>beta</code>), and for each sample, we plot the corresponding sigmoid curve. This gives us a range of possible outcomes, representing the uncertainty in the model.</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># define the sigmoid function</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate a range of X values for plotting the sigmoid functions</span>
</span></span><span class="line"><span class="cl"><span class="n">x_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the data</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">edgecolors</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>  <span class="c1"># bilirubin vs status</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot sigmoid functions from the posterior samples</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_range</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">    <span class="n">alpha_sample</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta_sample</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_sample</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">alpha_sample</span><span class="p">,</span> <span class="n">beta_sample</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">y_sample</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">top_features</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Status&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cirrhosis Classification Model&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_48_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-plot">Interpretation of the Plot</h4>
<ul>
<li>
<p><strong>Red Points:</strong> These represent the actual data, with the x-axis showing Bilirubin levels and the y-axis showing the observed survival status (0 or 1).</p>
</li>
<li>
<p><strong>Blue Sigmoid Curves:</strong> The faint blue lines represent the range of sigmoid functions sampled from the posterior distribution. These curves show how the model&rsquo;s predictions vary depending on different samples from the posterior. The spread of the curves reflects the uncertainty in the model&rsquo;s predictions.</p>
</li>
<li>
<p><strong>General Trend:</strong> As Bilirubin levels increase, the probability of survival decreases, as shown by the steep rise of the sigmoid curves from 0 to 1. This suggests that higher Bilirubin levels are associated with a higher likelihood of not surviving.</p>
</li>
</ul>
<p>This plot provides a clear visualization of how the model is using Bilirubin to predict patient survival and the uncertainty in those predictions.</p>
<h3 id="37-model-evaluation-confusion-matrix">3.7 Model Evaluation: Confusion Matrix</h3>
<p>After generating predictions using the posterior samples, we evaluate the model&rsquo;s performance by comparing the predicted classes to the actual survival outcomes. This is done using a <strong>confusion matrix</strong>, which summarizes the classification results in terms of:</p>
<ul>
<li><strong>True Positives (TP):</strong> Correct predictions of non-survival.</li>
<li><strong>True Negatives (TN):</strong> Correct predictions of survival.</li>
<li><strong>False Positives (FP):</strong> Incorrect predictions of non-survival.</li>
<li><strong>False Negatives (FN):</strong> Incorrect predictions of survival.</li>
</ul>
<p>In the code:</p>
<ul>
<li>We use the sigmoid function to convert the posterior samples into probabilities.</li>
<li>We then average the probabilities across all posterior samples and apply a threshold of 0.5 to classify each patient as either survived (0) or not survived (1).</li>
<li>Finally, we generate a confusion matrix to visually assess the performance of the classifier.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate predictions</span>
</span></span><span class="line"><span class="cl"><span class="c1"># use sigmoid function defined previously</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate predictions using the posterior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">n_samples</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># calculate the mean probability across posterior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">predicted_probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">             <span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">             <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert probabilities to binary predictions</span>
</span></span><span class="line"><span class="cl"><span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">predicted_classes</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted_probabilities</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># compare predicted classes with true classes</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># actual labels</span>
</span></span><span class="line"><span class="cl"><span class="n">y_true</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">predicted_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_51_0.png" type="" alt="png"  /></p>
<h4 id="confusion-matrix-interpretation">Confusion Matrix Interpretation</h4>
<p>In the plot:</p>
<ul>
<li><strong>233 true negatives</strong>: The model correctly predicted 233 patients as survivors.</li>
<li><strong>98 true positives</strong>: The model correctly predicted 98 patients as non-survivors.</li>
<li><strong>24 false positives</strong>: The model incorrectly predicted 24 patients as non-survivors when they actually survived.</li>
<li><strong>63 false negatives</strong>: The model incorrectly predicted 63 patients as survivors when they did not survive.</li>
</ul>
<h4 id="overall-performance">Overall Performance</h4>
<p>The confusion matrix shows that the model performs well on survival predictions, with more correct predictions than incorrect ones. The balance between true positives and true negatives suggests that the model handles both classes reasonably well, but there are more false negatives than false positives, indicating that the model might underpredict non-survival cases slightly.</p>
<h1 id="4-bayesian-survival-analysis">4. Bayesian Survival Analysis</h1>
<p>In this section, we shift our focus from classification to <strong>Bayesian survival analysis</strong>. Survival analysis is a statistical method used to estimate the time until an event of interest occurs, often referred to as &ldquo;time-to-event&rdquo; data. In our case, we are interested in modeling the time until either a patient succumbs to cirrhosis or is censored (i.e., the event does not occur during the observation period). This form of analysis allows us to incorporate censored data and provides insights into the distribution of survival times.</p>
<h2 id="41-basics-of-survival-analysis">4.1 Basics of Survival Analysis</h2>
<p>In classical survival analysis, we model the <strong>survival function</strong> $S(t)$, which represents the probability of survival beyond time $t$:</p>
<p>\begin{aligned}
S(t) = P(T &gt; t)
\end{aligned}</p>
<p>Where:</p>
<ul>
<li>$T$ is the random variable representing the time to the event (e.g., death or censoring).</li>
<li>$t$ is the specific time of interest.</li>
</ul>
<p>The complementary function, called the <strong>hazard function</strong>, $h(t)$, represents the instantaneous rate at which events occur at time $t$, given that the individual has survived up to that time. The hazard function is defined as:</p>
<p>\begin{aligned}
h(t) = \frac{f(t)}{S(t)}
\end{aligned}</p>
<p>Where $f(t)$ is the probability density function of survival times.</p>
<h3 id="bayesian-survival-analysis">Bayesian Survival Analysis</h3>
<p>In a Bayesian framework, we treat the parameters governing the survival function (or hazard function) as random variables, and we estimate their posterior distributions using observed data. By incorporating prior beliefs about these parameters, Bayesian methods offer a natural way to handle uncertainty, particularly in the presence of censored data.</p>
<p>The general approach involves:</p>
<ol>
<li><strong>Defining a Parametric Survival Model:</strong> A common choice for the survival model is the Weibull distribution, which provides flexibility in modeling both increasing and decreasing hazard rates.</li>
<li><strong>Priors:</strong> Assign prior distributions to the model parameters (e.g., shape and scale parameters of the Weibull distribution).</li>
<li><strong>Likelihood:</strong> The likelihood is defined based on the observed survival times and whether the event was censored.</li>
<li><strong>Posterior Inference:</strong> Use Markov Chain Monte Carlo (MCMC) or similar methods to draw samples from the posterior distribution of the parameters.</li>
</ol>
<p>For example, the <strong>Weibull distribution</strong> is a common parametric model for survival times. Its survival function is given by:</p>
<p>\begin{aligned}
S(t) = \exp \left( - \left( \frac{t}{\lambda} \right)^\kappa \right)
\end{aligned}</p>
<p>Where:</p>
<ul>
<li>$\lambda$ is the scale parameter.</li>
<li>$\kappa$ is the shape parameter.</li>
</ul>
<p>In the Bayesian context, we assign priors to $\lambda$ and $\kappa$, then use the observed data to update these priors to form the posterior distributions.</p>
<h3 id="censored-data">Censored Data</h3>
<p>In survival analysis, it&rsquo;s common to have censored data, where we know that the event of interest did not occur before a certain time, but we do not know the exact time of the event. Bayesian methods handle censored data naturally within the likelihood function by accounting for both observed events and censored observations.</p>
<p>The likelihood for a censored observation is:</p>
<p>\begin{aligned}
P(T &gt; t \mid \text{censored}) = S(t)
\end{aligned}</p>
<p>Thus, in a Bayesian survival model, the posterior distribution is based on both observed survival times and the fact that certain observations were censored.</p>
<p>This framework allows for a flexible and powerful analysis of time-to-event data while handling uncertainties in parameter estimates.</p>
<h2 id="42-weibull-survival-model-without-covariates">4.2 Weibull Survival Model Without Covariates</h2>
<p>We begin by defining a <strong>Weibull survival model</strong> for time-to-event data, with no covariates. The <strong>Weibull distribution</strong> is a flexible parametric model commonly used in survival analysis due to its ability to model both increasing and decreasing hazard rates over time, depending on the value of its shape parameter $k$.</p>
<p>The model assumes that survival times follow a Weibull distribution parameterized by:</p>
<ul>
<li>$k$ (shape parameter): This determines the form of the hazard function. A value of $k &gt; 1$ suggests an increasing hazard rate, while $k &lt; 1$ implies a decreasing hazard rate.</li>
<li>$\lambda$ (scale parameter): This controls the scale of the distribution and shifts the time-to-event distribution horizontally.</li>
</ul>
<h4 id="code-explanation-1">Code Explanation</h4>
<ol>
<li>
<p><strong>Priors:</strong></p>
<ul>
<li>
<p>We place <strong>Exponential(1.0)</strong> priors on both the shape parameter $k$ and the scale parameter $\lambda$. The exponential distribution is a standard choice when we do not have strong prior information and expect non-negative values for these parameters.</p>
</li>
<li>
<p>$k \sim \text{Exponential}(1.0)$</p>
</li>
<li>
<p>$\lambda \sim \text{Exponential}(1.0)$</p>
</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong></p>
<ul>
<li>
<p>The likelihood assumes that the observed survival times follow a <strong>Weibull distribution</strong>. We model the observed data (<code>obs</code>) as being drawn from a Weibull distribution, parameterized by the sampled values of $k$ and $\lambda$.</p>
</li>
<li>
<p>The survival times in <code>data</code> are modeled using this likelihood, and if no data is provided, a placeholder dimension (1000) is used to define the plate.</p>
</li>
</ul>
</li>
</ol>
<p>The Weibull model is ideal for analyzing baseline survival without introducing any covariates to the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">survival_weibull_model</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">lam</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;lam&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Weibull</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lam</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="preparing-the-data-for-survival-analysis">Preparing the Data for Survival Analysis</h3>
<p>Before fitting the survival model, we need to preprocess the data by selecting only the <strong>uncensored</strong> survival times. In survival analysis, censored data refers to cases where the event of interest (e.g., death) did not occur during the observation period, and we do not know the exact time of the event. For this initial analysis, we focus on observed events.</p>
<ul>
<li><strong><code>survival_times_observed</code>:</strong> We extract the survival times for patients where the event (death) was observed, indicated by <code>Status == 1</code>. This ensures that only complete (uncensored) data is included in the model.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prepare the data</span>
</span></span><span class="line"><span class="cl"><span class="c1"># select only the uncensored data</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times_observed</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span></code></pre></div><h3 id="fitting-the-weibull-survival-model">Fitting the Weibull Survival Model</h3>
<p>We now proceed to fit the <strong>Weibull survival model</strong> to the uncensored survival times using <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong>. This step involves estimating the posterior distributions of the model parameters ($k$ and $\lambda$) based on the observed data.</p>
<ul>
<li><strong><code>NUTS</code>:</strong> The No-U-Turn Sampler is a variant of Hamiltonian Monte Carlo (HMC), which efficiently explores the posterior distribution by dynamically tuning the step size and trajectory length.</li>
<li><strong><code>num_warmup</code>:</strong> The number of warm-up (burn-in) iterations, where the sampler adapts to the posterior distribution before drawing final samples.</li>
<li><strong><code>num_samples</code>:</strong> The number of posterior samples to draw after the warm-up phase.</li>
<li><strong><code>num_chains</code>:</strong> The number of independent MCMC chains to run in parallel, which helps assess convergence and reduce bias.</li>
</ul>
<p>After running the MCMC sampler, we print a summary of the estimated posterior distributions for the model parameters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># fit the model</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">survival_weibull_model</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
         k     48.25      7.99     47.86     35.15     61.08   1442.43      1.00
       lam      0.25      0.02      0.25      0.22      0.28   1419.78      1.00

Number of divergences: 0
</code></pre>
<h4 id="interpretation-of-weibull-model-posterior-estimates">Interpretation of Weibull Model Posterior Estimates</h4>
<p>The output from the MCMC run provides the posterior distributions for the shape parameter $k$ and the scale parameter $\lambda$ of the Weibull survival model. Here’s a brief interpretation of the results:</p>
<ul>
<li>
<p><strong>$k$ (shape parameter):</strong> The posterior mean of $k$ is <strong>48.25</strong>, with a standard deviation of <strong>7.99</strong>. The 90% credible interval (from 5% to 95%) ranges from <strong>35.15</strong> to <strong>61.08</strong>. A high value of $k$ suggests that the hazard rate increases rapidly over time, indicating that the risk of the event (death) increases as time progresses.</p>
</li>
<li>
<p><strong>$\lambda$ (scale parameter):</strong> The posterior mean of $\lambda$ is <strong>0.25</strong>, with a standard deviation of <strong>0.02</strong>. The 90% credible interval ranges from <strong>0.22</strong> to <strong>0.28</strong>. The scale parameter affects the distribution&rsquo;s spread; smaller values of $\lambda$ indicate that the survival times are more concentrated around lower values, meaning shorter survival times are more likely.</p>
</li>
<li>
<p><strong>Effective Sample Size (n_eff):</strong> Both $k$ and $\lambda$ have high effective sample sizes, indicating that the MCMC chains were well-mixed and independent, producing a sufficient number of effective samples.</p>
</li>
<li>
<p><strong>$\hat{R}$ values:</strong> The $\hat{R}$ values for both parameters are exactly <strong>1.00</strong>, indicating that the MCMC chains have converged well, and the posterior estimates are reliable.</p>
</li>
<li>
<p><strong>Number of Divergences:</strong> There were <strong>0 divergences</strong>, meaning that the NUTS sampler encountered no issues while exploring the posterior distribution.</p>
</li>
</ul>
<p>The posterior estimates suggest that the risk of the event (death) increases as time passes, with the hazard rate rising rapidly due to the large shape parameter $k$. The small scale parameter $\lambda$ indicates that shorter survival times are common in the dataset.</p>
<h3 id="prior-predictive-check-for-the-weibull-survival-model">Prior Predictive Check for the Weibull Survival Model</h3>
<p>Before we assess the model&rsquo;s posterior predictions, we perform a <strong>prior predictive check</strong> to ensure that the prior distributions we&rsquo;ve chosen are reasonable. This step involves generating data based purely on the priors, without any observed data influencing the predictions. By comparing the prior predictive distribution to the observed data, we can determine if our prior assumptions lead to plausible outcomes.</p>
<h4 id="code-explanation-2">Code Explanation</h4>
<ul>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to generate survival times from the prior distribution by drawing 1,000 samples.</li>
<li><strong>Prior Predictive Plot:</strong> The plot visualizes the prior predictive distribution, represented by the dashed orange line (the mean of the prior predictions), and the solid blue line shows the individual prior predictive samples.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># perform prior predictive check</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_predictive_survival_weibull</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">survival_weibull_model</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_samples_survival_weibull</span> <span class="o">=</span> <span class="n">prior_predictive_survival_weibull</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to InferenceData</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_predictive_data_survival_weibull</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_weibull</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                                         <span class="n">prior</span> <span class="o">=</span> <span class="n">prior_samples_survival_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># prior_samples_survival_weibull</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_predictive_data_survival_weibull</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the prior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">prior_predictive_data_survival_weibull</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;prior&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_62_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-plot-1">Interpretation of the Plot</h4>
<ul>
<li>The <strong>solid blue line</strong> represents the individual prior predictive samples, while the <strong>dashed orange line</strong> represents the mean of these prior predictions.</li>
<li>The plot shows how the model&rsquo;s prior assumptions generate survival times, and we can compare this to the range of observed data to evaluate the plausibility of the priors.</li>
<li>In this case, the prior predictive distribution seems to cover a reasonable range of potential survival times, suggesting that the prior assumptions about the shape and scale of the Weibull distribution are not overly restrictive or implausible.</li>
</ul>
<p>This prior predictive check gives us confidence that the model&rsquo;s priors are sensible, providing a good foundation for further inference when we include the observed data.</p>
<h3 id="posterior-trace-plot-for-weibull-survival-model">Posterior Trace Plot for Weibull Survival Model</h3>
<p>After fitting the Weibull survival model, we generate a <strong>trace plot</strong> to assess the MCMC sampling process and the posterior distributions of the model parameters. The trace plot shows both the posterior distributions and the sampling traces for the shape parameter $k$ and the scale parameter $\lambda$.</p>
<h4 id="code-explanation-3">Code Explanation</h4>
<ul>
<li><strong><code>survival_posterior_samples_weibull</code>:</strong> We extract the posterior samples for the model parameters.</li>
<li><strong><code>az.plot_trace</code>:</strong> This function generates the trace plot, showing the posterior distribution and the corresponding MCMC trace for each parameter.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">survival_posterior_samples_weibull</span> <span class="o">=</span> <span class="n">mcmc_survival_weibull</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az_survival_weibull</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">az_survival_weibull</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_65_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-trace-plot">Interpretation of the Trace Plot</h4>
<ol>
<li>
<p><strong>Posterior Distributions (Left Panels):</strong></p>
<ul>
<li>The left panels show the posterior distributions for $k$ and $\lambda$. Both parameters exhibit well-defined, unimodal distributions, which indicate stable and meaningful posterior estimates.</li>
<li>For $k$, the distribution has a peak around <strong>48</strong>, while for $\lambda$, the peak is around <strong>0.25</strong>.</li>
</ul>
</li>
<li>
<p><strong>Sampling Trace (Right Panels):</strong></p>
<ul>
<li>The right panels show the sampling trace for each parameter across the MCMC iterations. Both $k$ and $\lambda$ exhibit well-mixed traces, with no visible trends or drifts over time. This indicates that the sampler explored the parameter space effectively, without getting stuck in any particular region.</li>
<li>The trace plots confirm that the MCMC chains have converged well, as there is good overlap between the chains and no sign of poor mixing.</li>
</ul>
</li>
</ol>
<p>Overall, the trace plot shows that the MCMC sampling worked efficiently, with well-behaved posterior distributions and convergence for both $k$ and $\lambda$. This gives us confidence in the reliability of the posterior estimates for the survival model.</p>
<h3 id="posterior-predictive-check-for-weibull-survival-model">Posterior Predictive Check for Weibull Survival Model</h3>
<p>We now perform a <strong>posterior predictive check</strong> to evaluate how well the Weibull survival model, after being fitted to the data, predicts survival times. Posterior predictive checks allow us to compare the model&rsquo;s predicted values to the observed data, providing insight into how well the model captures the true underlying patterns.</p>
<h4 id="code-explanation-4">Code Explanation</h4>
<ul>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to generate survival times based on the posterior samples of the model parameters ($k$ and $\lambda$).</li>
<li><strong>Posterior Predictive Data:</strong> We create an <code>InferenceData</code> object using the posterior predictive samples, which includes both the predicted survival times and the observed data.</li>
<li><strong>Posterior Predictive Plot:</strong> The plot visualizes the posterior predictive distribution (blue lines), the observed data (solid black line), and the posterior predictive mean (dashed orange line).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">survival_posterior_predictive_weibull</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">survival_weibull_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">survival_posterior_samples_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_times_predicted_weibull</span> <span class="o">=</span> <span class="n">survival_posterior_predictive_weibull</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">survival_times_observed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_posterior_data_weibull</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_weibull</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                          <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">survival_times_predicted_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_68_0.png" type="" alt="png"  /></p>
<h3 id="plot-interpretation-for-weibull-posterior-predictive-vs-observed-data-uncensored">Plot Interpretation for Weibull Posterior Predictive vs. Observed Data (Uncensored)</h3>
<p>This plot shows a comparison between the <strong>Weibull posterior predictive distribution</strong> and the <strong>observed uncensored survival data</strong>, without any covariates involved. The purpose of this plot is to evaluate how well the Weibull model, fit to the uncensored data, can replicate the distribution of survival times.</p>
<h4 id="explanation-of-the-plot">Explanation of the Plot:</h4>
<ul>
<li>
<p><strong>X-axis (Survival Times):</strong></p>
<ul>
<li>The x-axis represents the survival times (in days), ranging from 0 to about 4500 days.</li>
</ul>
</li>
<li>
<p><strong>Y-axis (Density):</strong></p>
<ul>
<li>The y-axis shows the <strong>density</strong> of survival times, or the likelihood of individuals surviving for certain durations, based on both the observed data and the model&rsquo;s posterior predictions.</li>
</ul>
</li>
</ul>
<h4 id="lines-in-the-plot">Lines in the Plot:</h4>
<ul>
<li>
<p><strong>Blue Line (Weibull Posterior Predictive):</strong></p>
<ul>
<li>This line represents the predicted survival times from the Weibull model. It shows the expected distribution of survival times based on the model&rsquo;s posterior distribution.</li>
</ul>
</li>
<li>
<p><strong>Green Line (Weibull Observed Data):</strong></p>
<ul>
<li>This line represents the true observed distribution of uncensored survival times. It shows the actual survival time density for individuals who experienced the event (e.g., death).</li>
</ul>
</li>
</ul>
<h4 id="interpretation-1">Interpretation:</h4>
<ul>
<li>
<p><strong>Fit Between Lines:</strong></p>
<ul>
<li>The <strong>green line</strong> (observed data) and the <strong>blue line</strong> (posterior predictive) show a relatively good alignment in terms of overall trend, particularly in the early time periods (0 to 2000 days). The posterior predictive distribution follows the general trend of the observed data, which suggests that the Weibull model does an acceptable job at capturing the distribution of survival times in this range.</li>
</ul>
</li>
<li>
<p><strong>Discrepancies:</strong></p>
<ul>
<li>Similar to the lognormal model, we observe that the posterior predictive line fluctuates more in the earlier survival times, indicating higher uncertainty or variability in the predictions.</li>
<li>The fit becomes more diverging in the later periods (beyond 2500 days), where the posterior predictive distribution starts to exhibit higher peaks and troughs compared to the observed data, suggesting that the Weibull model may struggle to accurately capture extreme or late survival times.</li>
</ul>
</li>
<li>
<p><strong>Conclusion:</strong></p>
<ul>
<li>The Weibull model generally captures the trend of survival times, but as seen, there are some inconsistencies in the later survival times, where the model tends to overestimate or underestimate specific time periods. These fluctuations indicate that while the model is useful for capturing general trends, it might not fully account for more extreme cases of survival.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">[</span><span class="s1">&#39;posterior_predictive&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span> <span class="p">:</span> <span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Weibull Posterior Predictive&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">[</span><span class="s1">&#39;observed_data&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span> <span class="p">:</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Weibull Observed Data&#39;</span><span class="p">})</span>
</span></span></code></pre></div><pre><code>&lt;Axes: &gt;
</code></pre>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_70_1.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-posterior-predictive-check-plot">Interpretation of the Posterior Predictive Check Plot</h4>
<ul>
<li>
<p><strong>Blue Lines (Posterior Predictive Samples):</strong> These represent the survival times generated from the posterior distribution. The spread of the lines shows the model&rsquo;s uncertainty in predicting survival times based on the posterior samples.</p>
</li>
<li>
<p><strong>Solid Black Line (Observed Data):</strong> This represents the actual observed survival times.</p>
</li>
<li>
<p><strong>Dashed Orange Line (Posterior Predictive Mean):</strong> This shows the mean of the posterior predictive samples.</p>
</li>
</ul>
<p>In this case, the posterior predictive samples align well with the observed survival data, suggesting that the Weibull survival model provides a good fit to the data. The predicted survival times fall within a reasonable range around the observed data, and the posterior predictive mean closely follows the observed survival trend. This check confirms that the model has successfully learned from the data and is capable of generating realistic survival time predictions.</p>
<h3 id="posterior-summary-for-weibull-survival-model">Posterior Summary for Weibull Survival Model</h3>
<p>The summary table provides key statistics for the posterior distributions of the shape parameter ($k$) and scale parameter ($\lambda$) of the Weibull survival model. These values give us insights into the uncertainty and characteristics of the estimated parameters after fitting the model to the data.</p>
<h4 id="key-metrics">Key Metrics:</h4>
<ul>
<li>
<p><strong>Mean:</strong> The posterior mean of each parameter, representing the expected value based on the posterior distribution.</p>
<ul>
<li>$k$ (shape): <strong>48.25</strong> — This value indicates that the hazard rate increases rapidly over time.</li>
<li>$\lambda$ (scale): <strong>0.25</strong> — A small scale parameter, which suggests that the model predicts shorter survival times on average.</li>
</ul>
</li>
<li>
<p><strong>Standard Deviation (sd):</strong> The standard deviation of the posterior distribution, reflecting the uncertainty in the parameter estimates.</p>
<ul>
<li>$k$: <strong>7.99</strong></li>
<li>$\lambda$: <strong>0.017</strong></li>
</ul>
</li>
<li>
<p><strong>HDI 3% - 97%:</strong> The highest density interval (HDI) represents the 94% credible interval for the parameters, showing the range of likely values.</p>
<ul>
<li>$k$: <strong>34.08</strong> to <strong>63.65</strong></li>
<li>$\lambda$: <strong>0.219</strong> to <strong>0.283</strong></li>
</ul>
</li>
<li>
<p><strong>ESS (Effective Sample Size):</strong> Indicates the number of effectively independent samples in the posterior. High values of ESS mean that the MCMC chains mixed well, and the posterior is reliable.</p>
<ul>
<li>$k$: <strong>1451.0</strong></li>
<li>$\lambda$: <strong>1423.0</strong></li>
</ul>
</li>
<li>
<p><strong>$\hat{R}$ (r_hat):</strong> A diagnostic measure for MCMC convergence. Values close to <strong>1.0</strong> indicate good convergence, meaning the chains have reached the same distribution.</p>
<ul>
<li>Both $k$ and $\lambda$ have $\hat{R} = 1.0$, confirming that the model has converged.</li>
</ul>
</li>
</ul>
<p>The summary shows that the model has successfully converged, with high effective sample sizes and credible intervals that provide reasonable bounds on the parameter estimates. The increasing hazard rate (high $k$ value) suggests that the risk of death increases over time, while the small $\lambda$ value indicates relatively short survival times. This analysis gives us a good understanding of the survival patterns in the data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">)</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>k</th>
      <td>48.249</td>
      <td>7.987</td>
      <td>34.077</td>
      <td>63.653</td>
      <td>0.209</td>
      <td>0.148</td>
      <td>1451.0</td>
      <td>1795.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>lam</th>
      <td>0.251</td>
      <td>0.017</td>
      <td>0.219</td>
      <td>0.283</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1423.0</td>
      <td>1762.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="43-weibull-survival-model-with-covariates">4.3 Weibull Survival Model With Covariates</h2>
<p>In this section, we will extend the Weibull survival model to account for individual-specific covariates. by introducing covariates, we allow the model to make &ldquo;personalized&rdquo; predictions for survival times, based on characteristics like <strong>age, bilirubin levels</strong> and <strong>albumin</strong>.</p>
<p>The scale parameter $\lambda_i$, which influences the expected survival time, will now depend on a <strong>combination of covariates (characteristics)</strong> for each individual. This makes the model more flexible and capable of capturing how different factors impact survival.</p>
<p>The scale parameter $\lambda_i$ for individual $i$ is modeled as:</p>
<p>$$\lambda_i = \exp \left( \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \dots + \beta_k \cdot x_{ki} \right)$$</p>
<p>Where:</p>
<ul>
<li>$ x_{1i}, x_{2i}, \dots, x_{ki} $ are the covariates for individual  i ,</li>
<li>$ \beta_0, \beta_1, \dots, \beta_k $ are the regression coefficients for each covariate.</li>
</ul>
<p>The likelihood function for the observed survival times $t_i$ is given by the Weibull distribution:</p>
<p>$$ f(t_i | k, \lambda_i) = \frac{k}{\lambda_i} \left( \frac{t_i}{\lambda_i}  \right)^{k-1} \exp \left(- \left(\frac{t_i}{\lambda_i}\right)^k\right) $$</p>
<p>Where:</p>
<ul>
<li>$k$ is the shape parameter,</li>
<li>$\lambda_i$ is the scale parameter that varies with covariates.</li>
</ul>
<p>By estimating the coefficients $\beta$, we can assess how each covariate influences survival time. This approach personalizes the model to each individual&rsquo;s characteristics, leading to more tailored predictions.</p>
<p>We will implement this in the following steps:</p>
<ol>
<li>Define the model where $\lambda_i$ is a function of covariates.</li>
<li>Prepare the data (covariates and survival times)</li>
<li>Perform MCMC sampling to fit the model</li>
<li>Analyze the results.</li>
</ol>
<h3 id="define-the-weibull-survival-model-with-covariates">Define the Weibull Survival Model with Covariates</h3>
<p>We will now define the model with priors for both the <strong>shape parameter</strong> $k$ and the <strong>regression coefficients</strong> $\beta$. The scale parameter $\lambda_i$ will be modeled as the exponential of a linear combination of the covariates.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">survival_model_weibull_with_covariates</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span><span class="n">covariates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>  <span class="c1"># coefficients (beta)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># linear model for log(lambda)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_lambda</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>      <span class="c1"># linear combination of covariates</span>
</span></span><span class="line"><span class="cl">    <span class="n">lambda_i</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_lambda</span><span class="p">)</span>              <span class="c1"># ensure scale parameter lambda is positive</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Weibull</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lambda_i</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="selecting-covariates-for-the-weibull-model">Selecting Covariates for the Weibull Model</h3>
<p>Before fitting the Weibull model with covariates, we need to ensure that the relevant covariates are selected from the dataset. These covariates will be used to personalize the scale parameter $\lambda_i$ for each individual, allowing the model to capture how various features influence survival times.</p>
<p>In this step, we simply print the list of top features, which were previously selected based on their correlation with the survival outcome. These features will serve as the covariates in the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos']
</code></pre>
<p>To extend the Weibull model with covariates, we need to prepare two key components:</p>
<ol>
<li><strong>Covariates:</strong> The selected covariates (features) that will be used to predict survival times for each individual.</li>
<li><strong>Survival Times:</strong> The observed survival times for the uncensored data (i.e., where the event of interest has occurred).</li>
</ol>
<h4 id="code-explanation-5">Code Explanation</h4>
<ul>
<li><strong><code>covariate_selection</code>:</strong> We explicitly select a subset of covariates that are expected to have an influence on survival, including <strong>Bilirubin</strong>, <strong>Edema</strong>, <strong>Prothrombin</strong>, <strong>Albumin</strong>, and <strong>Age</strong>.</li>
<li><strong><code>covariates_survival</code>:</strong> We filter the dataset to include only individuals with observed survival times (where <code>Status == 1</code>) and extract the covariate values.</li>
<li><strong><code>survival_times</code>:</strong> We extract the corresponding survival times for these individuals.</li>
</ul>
<p>Finally, we print the top features and the selected covariates to verify that the correct data is being used for the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">covariate_selection</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;Edema&#39;</span><span class="p">,</span> <span class="s1">&#39;Prothrombin&#39;</span><span class="p">,</span> <span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">covariates_survival</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="n">covariate_selection</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">covariate_selection</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos']
['Bilirubin', 'Edema', 'Prothrombin', 'Albumin', 'Age']
</code></pre>
<h3 id="fitting-the-weibull-survival-model-with-covariates">Fitting the Weibull Survival Model with Covariates</h3>
<p>We now fit the Weibull survival model with covariates using <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong>. This model accounts for the effects of the selected covariates on the survival times, making the predictions more personalized for each individual.</p>
<h4 id="code-explanation-6">Code Explanation</h4>
<ul>
<li><strong>NUTS:</strong> We use the NUTS kernel to efficiently explore the posterior distribution of the parameters.</li>
<li><strong>MCMC Setup:</strong> The model is run with 1,000 warm-up iterations, followed by 2,000 sampling iterations, and using 4 chains to assess convergence.</li>
<li><strong>Covariates:</strong> The scale parameter $\lambda_i$ is modeled as a function of the selected covariates, allowing the model to account for individual characteristics when predicting survival times.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">nuts_kernel_survival_weibull_with_covariates</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">survival_model_weibull_with_covariates</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull_with_covariates</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">nuts_kernel_survival_weibull_with_covariates</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull_with_covariates</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                  <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="n">covariates</span> <span class="o">=</span> <span class="n">covariates_survival</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull_with_covariates</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
   beta[0]      0.07      0.04      0.07      0.03      0.12      2.96      1.70
   beta[1]      0.03      0.54     -0.24     -0.36      0.96      2.02      8.85
   beta[2]     -0.05      0.05     -0.06     -0.11      0.04      2.42      2.29
   beta[3]      0.12      0.39     -0.10     -0.14      0.80      2.00     20.11
   beta[4]     -0.19      0.30     -0.03     -0.72      0.01      2.01     14.35
         k    284.18    163.08    372.41      2.42    392.76       nan     15.33

Number of divergences: 2000
</code></pre>
<h3 id="results-interpretation">Results Interpretation:</h3>
<p>The output provides the posterior estimates for the regression coefficients ($\beta$) and the shape parameter ($k$), along with diagnostics.</p>
<ul>
<li>
<p><strong>$\beta$ coefficients:</strong></p>
<ul>
<li><strong>beta[0] (Bilirubin):</strong> Mean of <strong>0.07</strong>, indicating a small positive effect on survival time, though there is substantial uncertainty as indicated by the standard deviation (std).</li>
<li><strong>beta[1] to beta[4] (Other Covariates):</strong> The estimates for the remaining covariates (Edema, Prothrombin, Albumin, Age) show wide credible intervals and large $\hat{R}$ values, suggesting poor convergence and high uncertainty in the estimates.</li>
</ul>
</li>
<li>
<p><strong>Shape Parameter ($k$):</strong> The mean estimate for $k$ is <strong>284.18</strong>, with a high standard deviation, indicating extreme variability in the estimates. Additionally, the model encountered <strong>2,000 divergences</strong>, signaling serious issues with the MCMC sampling process.</p>
</li>
<li>
<p><strong>Convergence Issues:</strong></p>
<ul>
<li>The <strong>$\hat{R}$</strong> values for most of the parameters are far from <strong>1.0</strong>, indicating that the chains did not converge.</li>
<li><strong>Effective Sample Size (<code>n_eff</code>):</strong> Extremely low values for the effective sample size show that the sampler struggled to explore the posterior efficiently, producing highly correlated samples.</li>
<li><strong>Divergences:</strong> The model reported <strong>2,000 divergences</strong>, suggesting that the NUTS sampler encountered significant problems in exploring the posterior, which likely stems from poor model specification or highly informative priors.</li>
</ul>
</li>
</ul>
<p>The results indicate that the model failed to converge, likely due to the high number of divergences and poor effective sample sizes. These issues point to potential problems with the model&rsquo;s specification, such as overly complex priors or insufficient data to support the model&rsquo;s structure. In subsequent steps, it may be necessary to simplify the model, adjust the priors, or address potential multicollinearity in the covariates.</p>
<h2 id="44-introducing-the-log-normal-survival-model">4.4 Introducing the Log-Normal Survival Model</h2>
<p>After encountering issues with the Weibull model with covariates, we modify the model structure and introduce a <strong>log-normal survival model</strong> with covariates. This change in model specification can sometimes lead to better performance or convergence when the original model struggles with the data.</p>
<p>The <strong>log-normal distribution</strong> assumes that the logarithm of the survival times follows a normal distribution, which is another common choice for modeling time-to-event data. Unlike the Weibull distribution, the log-normal distribution may provide better flexibility when the hazard rate does not follow the typical monotonic increasing or decreasing pattern.</p>
<h4 id="code-explanation-7">Code Explanation</h4>
<ul>
<li>
<p><strong>Priors:</strong></p>
<ul>
<li>We place <strong>Normal(0, 1)</strong> priors on the regression coefficients $\beta$, which influence how each covariate impacts the survival time.</li>
<li>An <strong>Exponential(1.0)</strong> prior is assigned to $\sigma$, the standard deviation, to ensure positivity.</li>
</ul>
</li>
<li>
<p><strong>Log-Mu (Mean of Log-Survival Times):</strong></p>
<ul>
<li>The linear combination of covariates is modeled as $ \log(\mu_i) = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \dots + \beta_k \cdot x_{ki} $, where $x_{1i}, x_{2i}, \dots, x_{ki}$ are the covariates for individual $i$.</li>
<li>This represents the log of the expected survival time for each individual.</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong></p>
<ul>
<li>The observed survival times are modeled as being drawn from a <strong>LogNormal distribution</strong>, where the mean is $\log(\mu)$ and the standard deviation is $\sigma$.</li>
<li>This distribution models the uncertainty in the survival times, allowing the log-transformed times to follow a normal distribution, while ensuring that the predicted survival times themselves are positive.</li>
</ul>
</li>
</ul>
<p>This adjustment in the model may lead to improved performance, especially if the underlying survival times fit better with a log-normal assumption rather than a Weibull distribution.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lognormal_survival_model_with_covariates</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span><span class="n">covariates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>  <span class="c1"># coefficients (beta)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># linear model for log(mu)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_mu</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>      <span class="c1"># linear combination of covariates</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="n">log_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="fitting-the-log-normal-survival-model-with-covariates">Fitting the Log-Normal Survival Model with Covariates</h3>
<p>We now fit the <strong>log-normal survival model</strong> with covariates using the <strong>No-U-Turn Sampler (NUTS)</strong>. This model assumes that the logarithm of the survival times follows a normal distribution, and the scale parameter (or mean of the log-survival times) depends on a linear combination of the selected covariates.</p>
<h4 id="code-explanation-8">Code Explanation</h4>
<ul>
<li><strong>Feature Selection:</strong> We choose a new set of covariates based on their relevance to survival: <strong>Bilirubin, Edema, Copper, Prothrombin, Albumin, and Age</strong>.
<ul>
<li>These features are expected to influence survival times based on prior domain knowledge or their relationship with survival.</li>
</ul>
</li>
<li><strong>MCMC with NUTS:</strong> The log-normal model is fitted using MCMC with 1,000 warm-up iterations and 1,000 sampling iterations.
<ul>
<li>The NUTS sampler explores the posterior distribution of the model parameters, accounting for the covariates in predicting survival times.</li>
</ul>
</li>
</ul>
<p>By running this code, we estimate the posterior distributions of the regression coefficients ($\beta$) and the standard deviation ($\sigma$) for the log-normal survival model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">selection_features_lognormal</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;Edema&#39;</span><span class="p">,</span> <span class="s1">&#39;Copper&#39;</span><span class="p">,</span> <span class="s1">&#39;Prothrombin&#39;</span><span class="p">,</span> <span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">covariates_survival_lognormal</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="n">selection_features_lognormal</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_with_covariates_lognormal</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">lognormal_survival_model_with_covariates</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                  <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">covariates</span> <span class="o">=</span> <span class="n">covariates_survival_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
   beta[0]      0.79      0.34      0.80      0.21      1.34   5897.04      1.00
   beta[1]      1.57      0.60      1.56      0.64      2.56   5230.58      1.00
   beta[2]      0.56      0.38      0.56     -0.07      1.18   6122.26      1.00
   beta[3]      0.60      0.41      0.60     -0.04      1.30   6428.30      1.00
   beta[4]      0.03      0.42      0.03     -0.70      0.68   5850.73      1.00
   beta[5]      1.23      0.46      1.23      0.45      1.95   5866.68      1.00
     sigma      5.96      0.33      5.94      5.39      6.47   6314.24      1.00

Number of divergences: 0
</code></pre>
<h4 id="results-interpretation-for-the-log-normal-survival-model-with-covariates">Results Interpretation for the Log-Normal Survival Model with Covariates</h4>
<p>The output provides the posterior estimates for the regression coefficients ($\beta$) and the standard deviation ($\sigma$) in the log-normal survival model. The model successfully converged with no divergences, and the effective sample sizes ($n_{\text{eff}}$) and $\hat{R}$ values indicate good convergence.</p>
<ul>
<li>
<p><strong>$\beta$ coefficients:</strong></p>
<ul>
<li><strong>beta[0] (Bilirubin):</strong> Mean of <strong>0.79</strong>, suggesting that higher Bilirubin levels are associated with a higher log-survival time, meaning longer survival on average.</li>
<li><strong>beta[1] (Edema):</strong> Mean of <strong>1.57</strong>, showing a relatively strong positive effect on survival. This indicates that patients with Edema have longer predicted survival times.</li>
<li><strong>beta[2] (Copper):</strong> Mean of <strong>0.56</strong>, suggesting that higher copper levels are also associated with longer survival times, though the effect size is smaller than for Bilirubin and Edema.</li>
<li><strong>beta[3] (Prothrombin):</strong> Mean of <strong>0.60</strong>, indicating a positive relationship between Prothrombin and survival time.</li>
<li><strong>beta[4] (Albumin):</strong> Mean of <strong>0.03</strong>, suggesting that the effect of Albumin on survival time is weak and uncertain, as evidenced by the wide credible interval.</li>
<li><strong>beta[5] (Age):</strong> Mean of <strong>1.23</strong>, suggesting that older age is associated with longer survival times, which might be counterintuitive but could reflect a complex interaction with other covariates in this dataset.</li>
</ul>
</li>
<li>
<p><strong>Standard Deviation ($\sigma$):</strong></p>
<ul>
<li>The mean estimate for $\sigma$ is <strong>5.96</strong>, indicating that there is considerable variability in the log-survival times that the model is capturing. The narrow credible interval (5.39 to 6.47) suggests that the uncertainty around this parameter is low.</li>
</ul>
</li>
<li>
<p><strong>Effective Sample Size ($n_{\text{eff}}$) and $\hat{R}$ values:</strong></p>
<ul>
<li>All parameters have high effective sample sizes, indicating good exploration of the posterior distribution during sampling.</li>
<li>The $\hat{R}$ values are all <strong>1.00</strong>, indicating that the MCMC chains have converged well.</li>
</ul>
</li>
</ul>
<p>The results indicate that the log-normal model has fit the data well, with all parameters showing good convergence and meaningful posterior estimates. The positive $\beta$ coefficients for most covariates suggest that these factors are positively correlated with survival times. However, further analysis might be needed to interpret some of the results, such as the positive association between age and survival.</p>
<p>Using a <strong>log-normal model</strong> may be more suitable than the Weibull model in this case because the log-normal distribution can capture more flexible hazard patterns, including non-monotonic hazard rates, which the Weibull model may struggle with. If the survival data exhibits more complex or non-linear relationships, the log-normal model can provide a better fit by allowing the log of survival times to follow a normal distribution, capturing a wider range of survival behaviors.</p>
<h3 id="posterior-trace-and-pair-plots-for-the-log-normal-survival-model">Posterior Trace and Pair Plots for the Log-Normal Survival Model</h3>
<p>After fitting the log-normal survival model with covariates, we use trace and pair plots to assess the quality of the MCMC sampling and the relationships between the parameters. These visualizations help verify that the posterior distributions are well-behaved and that the sampler has explored the parameter space efficiently.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">trace_survival_with_covariates_lognormal</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_survival_with_covariates_lognormal</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace_survival_with_covariates_lognormal</span><span class="p">,</span> <span class="n">divergences</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_90_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_90_1.png" type="" alt="png"  /></p>
<h4 id="trace-plot-interpretation">Trace Plot Interpretation</h4>
<ul>
<li>
<p><strong>Posterior Distributions (Left Panels):</strong></p>
<ul>
<li>The left panels show the posterior distributions for the regression coefficients ($\beta$) and the standard deviation ($\sigma$). All the distributions appear unimodal and well-defined, with little to no skewness, indicating that the model has produced meaningful estimates for each parameter.</li>
<li>Each coefficient corresponds to one of the covariates selected for the model, and $\sigma$ represents the standard deviation of the log-normal distribution.</li>
</ul>
</li>
<li>
<p><strong>MCMC Trace (Right Panels):</strong></p>
<ul>
<li>The right panels show the MCMC sampling traces. All traces exhibit good mixing and stability, meaning that the chains are not stuck and have thoroughly explored the parameter space. The lack of any visible trends suggests that the MCMC chains have converged well.</li>
</ul>
</li>
</ul>
<h4 id="pair-plot-interpretation">Pair Plot Interpretation</h4>
<ul>
<li>The pair plots show the joint distributions and scatter plots of the posterior samples for each pair of parameters.
<ul>
<li>The plots indicate minimal correlation between the parameters, as the scatter plots are roughly circular, suggesting that each parameter is being estimated independently of the others.</li>
<li>No significant divergences are visible in the scatter plots, confirming that the NUTS sampler did not encounter serious issues during sampling.</li>
</ul>
</li>
</ul>
<p>The trace and pair plots demonstrate that the MCMC sampling worked efficiently, with good mixing and convergence. The posterior distributions are well-defined, and the parameter estimates are reliable. Additionally, the pair plots show that there are no strong correlations between the covariates, supporting the validity of the model structure and the sampling process.</p>
<h3 id="posterior-predictive-check-for-log-normal-survival-model-with-covariates">Posterior Predictive Check for Log-Normal Survival Model with Covariates</h3>
<p>We now perform a <strong>posterior predictive check</strong> to evaluate how well the log-normal survival model with covariates predicts the observed survival times. This helps assess the quality of the model’s fit by comparing the predicted and observed data distributions.</p>
<h4 id="code-explanation-9">Code Explanation</h4>
<ul>
<li><strong>Posterior Predictive Samples:</strong> We generate posterior predictive survival times using the posterior samples of the regression coefficients ($\beta$) and standard deviation ($\sigma$). This allows us to simulate the model&rsquo;s predictions based on the observed data and the learned parameters.</li>
<li><strong>Posterior Predictive Plot:</strong> The plot shows the posterior predictive distribution (blue lines), the observed data (black line), and the posterior predictive mean (dashed orange line).</li>
</ul>
<h3 id="posterior-predictive-check-for-log-normal-survival-model-with-covariates-1">Posterior Predictive Check for Log-Normal Survival Model with Covariates</h3>
<p>We now perform a <strong>posterior predictive check</strong> to evaluate how well the log-normal survival model with covariates predicts the observed survival times. This helps assess the quality of the model’s fit by comparing the predicted and observed data distributions.</p>
<h4 id="code-explanation-10">Code Explanation</h4>
<ul>
<li><strong>Posterior Predictive Samples:</strong> We generate posterior predictive survival times using the posterior samples of the regression coefficients ($\beta$) and standard deviation ($\sigma$). This allows us to simulate the model&rsquo;s predictions based on the observed data and the learned parameters.</li>
<li><strong>Posterior Predictive Plot:</strong> The plot shows the posterior predictive distribution (blue lines), the observed data (black line), and the posterior predictive mean (dashed orange line).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">survival_posterior_samples_lognormal</span> <span class="o">=</span> <span class="n">mcmc_survival_with_covariates_lognormal</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_posterior_predictive_lognormal_with_covariates</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lognormal_survival_model_with_covariates</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">survival_posterior_samples_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_times_predicted_lognormal_with_covariates</span> <span class="o">=</span> <span class="n">survival_posterior_predictive_lognormal_with_covariates</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                                                                                                    <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                                                    <span class="n">covariates</span> <span class="o">=</span> <span class="n">covariates_survival_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_posterior_data_lognormal_with_covariates</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                          <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">survival_times_predicted_lognormal_with_covariates</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">survival_posterior_data_lognormal_with_covariates</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_94_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-posterior-predictive-plot-1">Interpretation of the Posterior Predictive Plot</h4>
<ul>
<li>
<p><strong>Blue Lines (Posterior Predictive Samples):</strong> These represent the model’s predictions for survival times based on the posterior distribution. The spread of the lines illustrates the uncertainty in the predictions.</p>
</li>
<li>
<p><strong>Solid Black Line (Observed Data):</strong> This represents the actual observed survival times.</p>
</li>
<li>
<p><strong>Dashed Orange Line (Posterior Predictive Mean):</strong> The dashed line shows the mean of the posterior predictive samples.</p>
</li>
</ul>
<p>The posterior predictive samples align well with the observed data, suggesting that the log-normal survival model with covariates captures the underlying survival patterns in the data. The predicted survival times fall within a reasonable range around the observed data, and the posterior predictive mean closely follows the trend of the observed survival times. This result indicates that the model has fit the data well and is capable of generating realistic survival time predictions.</p>
<h3 id="plot-interpretation-for-lognormal-posterior-predictive-vs-observed-data">Plot Interpretation for Lognormal Posterior Predictive vs. Observed Data</h3>
<p>In this step, we visualize the posterior predictive check for the <strong>lognormal model with covariates</strong> using kernel density estimation (KDE) plots. These plots help us understand how well the model captures the distribution of the survival data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_lognormal_with_covariates</span><span class="p">[</span><span class="s1">&#39;posterior_predictive&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span> <span class="p">:</span> <span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Lognormal Posterior Predictive&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_lognormal_with_covariates</span><span class="p">[</span><span class="s1">&#39;observed_data&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Observed Data&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_97_0.png" type="" alt="png"  /></p>
<h4 id="explanation-of-the-plot-1">Explanation of the Plot:</h4>
<ul>
<li>
<p><strong>X-axis (Survival Times):</strong></p>
<ul>
<li>The x-axis represents the <strong>survival times</strong> (in days), ranging from 0 to about 4500 days.</li>
</ul>
</li>
<li>
<p><strong>Y-axis (Density):</strong></p>
<ul>
<li>The y-axis represents the <strong>density</strong> of survival times, or how frequently individuals survive for certain durations based on the observed data and model predictions.</li>
</ul>
</li>
</ul>
<h4 id="lines-in-the-plot-1">Lines in the Plot:</h4>
<ul>
<li>
<p><strong>Blue Line (Lognormal Posterior Predictive Uncensored):</strong></p>
<ul>
<li>This line represents the model&rsquo;s prediction of uncensored survival times using the <strong>lognormal distribution</strong>. It shows how the model expects individuals to survive over time, based on the posterior samples.</li>
</ul>
</li>
<li>
<p><strong>Green Line (Observed Data):</strong></p>
<ul>
<li>This line represents the <strong>actual observed data</strong> of uncensored survival times. It shows the true distribution of survival times for individuals where the event (e.g., death) has occurred.</li>
</ul>
</li>
</ul>
<h4 id="interpretation-2">Interpretation:</h4>
<ul>
<li>
<p><strong>Alignment of the Lines:</strong></p>
<ul>
<li>The blue and green lines show some overlap, especially in the range from <strong>500 to 2000 days</strong>. This indicates that the model has captured the trend of survival times relatively well in this region.</li>
</ul>
</li>
<li>
<p><strong>Discrepancies at the Extremes:</strong></p>
<ul>
<li>The blue line fluctuates more at the start of the timeline (before 1000 days) and at the far right (after 3000 days). This suggests that the model has higher variability in predicting shorter survival times and potentially struggles to match the observed data precisely in those ranges.</li>
</ul>
</li>
<li>
<p><strong>Overall Fit:</strong></p>
<ul>
<li>Despite some variability, the general trend of the posterior predictive distribution (blue) aligns with the observed data (green). This indicates that the <strong>lognormal model with covariates</strong> is reasonably accurate in predicting uncensored survival times but could be improved in certain time ranges, especially for early and late survival times.</li>
</ul>
</li>
</ul>
<p>By including this plot, we visually validate that the <strong>lognormal model</strong> captures key aspects of the survival data, though some refinement may be needed for more extreme cases.</p>
<h2 id="45-log-normal-survival-model-with-covariates-and-censored-data">4.5 Log-Normal Survival Model with Covariates and Censored Data</h2>
<p>In this final part, we extend the log-normal survival model to account for <strong>censored data</strong>. In survival analysis, <strong>censoring</strong> occurs when we do not observe the exact event time for some individuals. For example, if a patient is still alive at the end of the study or drops out before experiencing the event (death), their survival time is <strong>right-censored</strong>—we only know that the event did not occur before a certain time.</p>
<h3 id="modeling-censored-data">Modeling Censored Data</h3>
<p>To account for censored data, we modify the likelihood function to handle both <strong>observed</strong> and <strong>censored</strong> survival times. Specifically, for censored data, the likelihood is based on the <strong>survival function</strong> rather than the probability density function (PDF).</p>
<h3 id="likelihood-for-censored-and-observed-data">Likelihood for Censored and Observed Data</h3>
<p>For observed survival times $t_i$ (i.e., the event occurred):
$$ f(t_i | \mu_i, \sigma) = \frac{1}{t_i \sigma \sqrt{2\pi}} \exp\left(-\frac{(\log t_i - \mu_i)^2}{2\sigma^2}\right) $$</p>
<p>For censored survival times $t_i$ (i.e., the event did not occur by time $t_i$):
$$ S(t_i | \mu_i, \sigma) = 1 - F(t_i | \mu_i, \sigma) $$</p>
<p>Where:</p>
<ul>
<li>$f(t_i | \mu_i, \sigma)$ is the <strong>log-normal probability density function</strong> (PDF) for observed data,</li>
<li>$S(t_i | \mu_i, \sigma)$ is the <strong>survival function</strong>, representing the probability of survival beyond time $t_i$ for censored data,</li>
<li>$F(t_i | \mu_i, \sigma)$ is the <strong>cumulative distribution function</strong> (CDF) of the log-normal distribution,</li>
<li>$\mu_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki}$ is the mean of the log-survival times for individual $i$, based on their covariates,</li>
<li>$\sigma$ is the standard deviation of the log-survival times.</li>
</ul>
<h3 id="handling-censoring">Handling Censoring</h3>
<p>In the likelihood, we condition on whether the data is censored or not:</p>
<ul>
<li>For <strong>observed data</strong>, we use the PDF of the log-normal distribution.</li>
<li>For <strong>censored data</strong>, we use the survival function (1 minus the CDF).</li>
</ul>
<p>By incorporating both observed and censored survival times, this model can better capture the full picture of the dataset, including individuals who did not experience the event during the study period.</p>
<h3 id="steps-for-the-model">Steps for the Model:</h3>
<ol>
<li>Define the log-normal survival model with covariates, including censored data.</li>
<li>Prepare the dataset, distinguishing between censored and uncensored observations.</li>
<li>Fit the model using MCMC and analyze the results.</li>
</ol>
<h3 id="defining-the-log-normal-survival-model-with-censored-data">Defining the Log-Normal Survival Model with Censored Data</h3>
<p>In this model, we incorporate <strong>censored data</strong> into the log-normal survival model with covariates. Censoring allows us to handle cases where the exact survival time is unknown because the event (e.g., death) has not yet occurred by the end of the observation period.</p>
<h4 id="code-explanation-11">Code Explanation</h4>
<ul>
<li>
<p><strong>Priors:</strong></p>
<ul>
<li><strong>$\sigma$ (standard deviation parameter):</strong> We place an <strong>Exponential(1.0)</strong> prior on $\sigma$. This parameter controls the spread of the log-normal distribution for survival times, representing the variability in the log-transformed survival times.</li>
<li><strong>$\beta$ (covariate coefficients):</strong> If covariates are provided, we define a normal prior on $\beta$, which adjusts the mean of the log-survival times $\mu_i$ for each individual based on their characteristics.</li>
</ul>
</li>
<li>
<p><strong>Handling Censored Data:</strong></p>
<ul>
<li>We use a <strong>mask</strong> to differentiate between uncensored (event occurred) and censored (event did not occur) data.</li>
<li>For <strong>uncensored data</strong>, the likelihood is based on the <strong>log-normal distribution</strong>: $f(t_i | \mu_i, \sigma)$, where $\mu_i$ is the mean log-survival time (a linear combination of the covariates).</li>
<li>For <strong>censored data</strong>, the likelihood is based on the <strong>survival function</strong> of the log-normal distribution: $S(t_i | \mu_i, \sigma) = 1 - F(t_i | \mu_i, \sigma)$, where $F(t_i | \mu_i, \sigma)$ is the cumulative distribution function (CDF) of the log-normal distribution.</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong></p>
<ul>
<li>We define the likelihood separately for uncensored and censored data:
<ul>
<li>For <strong>uncensored data</strong>, we use the <strong>log-normal distribution</strong> to model the observed survival times.</li>
<li>For <strong>censored data</strong>, we use a Bernoulli likelihood to model the survival probabilities, representing the probability that the event has not yet occurred by the observed time. This is based on the <strong>survival function</strong> of the log-normal distribution.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>By integrating both the <strong>observed events</strong> and <strong>censored data</strong>, this model more accurately reflects the reality of survival analysis, where not all individuals experience the event within the study period. The log-normal model offers flexibility, particularly for data where the hazard rate may change over time, making it a more suitable choice for complex survival data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">survival_model_lognormal_with_censored</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">survival_times</span><span class="p">,</span> <span class="n">event_occurred</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>  <span class="c1"># standard deviation for the log-normal distribution</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">covariates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span><span class="n">covariates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>  <span class="c1"># regression coefficients</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_mu</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>  <span class="c1"># linear model for log(mu)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_mu</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># if no covariates, set log(mu) to 0 (default)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood - handle based on censoring status</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">survival_times</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># uncensored data (event occurred, i.e. event_occurred == 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">uncensored_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">event_occurred</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs_uncensored&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="n">log_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">uncensored_mask</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">survival_times</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># censored data (event did not occur, i.e. event_occurred == 0)</span>
</span></span><span class="line"><span class="cl">        <span class="n">censored_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">event_occurred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">survival_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="n">log_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">survival_times</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs_censored&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">survival_prob</span><span class="p">)</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">censored_mask</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">event_occurred</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="preparing-the-data-for-the-log-normal-survival-model-with-censored-data">Preparing the Data for the Log-Normal Survival Model with Censored Data</h3>
<p>To fit the log-normal survival model with censored data, we need to prepare the following components:</p>
<ul>
<li>
<p><strong>Survival Times (<code>survival_times</code>):</strong> This represents the number of days each patient survived. These are the time-to-event data, which include both censored and uncensored observations.</p>
</li>
<li>
<p><strong>Event Occurred (<code>event_occurred</code>):</strong> This variable indicates whether the event (death) occurred or not. We create a binary indicator where <strong>1</strong> represents that the event occurred (i.e., death), and <strong>0</strong> represents that the data is censored (i.e., the patient was alive at the end of the study).</p>
</li>
<li>
<p><strong>Covariates (<code>covariates_survival</code>):</strong> We select a set of covariates that will be used to model the individual-specific survival times. In this case, we choose <code>'Bilirubin'</code>, <code>'Edema'</code>, <code>'Copper'</code>, <code>'Prothrombin'</code>, <code>'Albumin'</code>, and <code>'Age'</code> as the covariates, which are expected to influence the survival times, as we had already observed in EDA.</p>
</li>
</ul>
<h4 id="code-explanation-12">Code Explanation:</h4>
<ul>
<li><strong><code>survival_times</code>:</strong> Extracts the survival times (in days) from the dataset.</li>
<li><strong><code>event_occurred</code>:</strong> Converts the status variable into a binary format where 1 means the event occurred, and 0 means the data is censored.</li>
<li><strong><code>covariates_survival</code>:</strong> Selects the covariates of interest, which are used to predict survival times.</li>
</ul>
<p>This step ensures that we have the necessary data formatted correctly to fit the survival model, accounting for both censored and uncensored data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prepare the data</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># the survival times (days)</span>
</span></span><span class="line"><span class="cl"><span class="n">event_occurred</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># 1 if event (death), 0 if censored</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># select your covariates</span>
</span></span><span class="line"><span class="cl"><span class="n">covariates_survival</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;Edema&#39;</span><span class="p">,</span> <span class="s1">&#39;Copper&#39;</span><span class="p">,</span> <span class="s1">&#39;Prothrombin&#39;</span><span class="p">,</span> <span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>  
</span></span></code></pre></div><h3 id="fitting-the-weibull-survival-model-with-censored-data">Fitting the Weibull Survival Model with Censored Data</h3>
<p>We now fit the <strong>Weibull survival model</strong> with covariates and censored data using <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong>. The model accounts for both censored and uncensored survival times, making it suitable for real-world survival analysis where not all events occur within the observation window.</p>
<h4 id="code-explanation-13">Code Explanation</h4>
<ul>
<li><strong>NUTS Sampler:</strong> We use the NUTS kernel to efficiently explore the posterior distribution of the parameters.</li>
<li><strong>MCMC Setup:</strong> The model is run with 1,000 warm-up iterations and 1,000 sampling iterations, with 4 independent chains to ensure proper convergence and mixing.</li>
<li><strong>Input Data:</strong>
<ul>
<li><strong>Covariates:</strong> The selected covariates are passed to the model.</li>
<li><strong>Survival Times:</strong> The observed or censored survival times are provided to the model.</li>
<li><strong>Event Occurred:</strong> This binary indicator differentiates between censored (0) and uncensored (1) data.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># fit the Weibull model with censored data</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_lognormal_censored</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">survival_model_lognormal_with_censored</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                             <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_lognormal_censored</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">covariates</span><span class="o">=</span><span class="n">covariates_survival</span><span class="p">,</span> <span class="n">survival_times</span><span class="o">=</span><span class="n">survival_times</span><span class="p">,</span> <span class="n">event_occurred</span><span class="o">=</span><span class="n">event_occurred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_lognormal_censored</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
   beta[0]      0.91      0.30      0.91      0.44      1.42   5068.22      1.00
   beta[1]      1.32      0.52      1.31      0.50      2.20   5117.46      1.00
   beta[2]      0.61      0.32      0.61      0.11      1.14   6312.73      1.00
   beta[3]      0.68      0.34      0.68      0.09      1.20   6077.50      1.00
   beta[4]     -0.02      0.35     -0.02     -0.62      0.52   6067.08      1.00
   beta[5]      1.16      0.35      1.16      0.59      1.74   6004.50      1.00
     sigma      5.25      0.24      5.24      4.84      5.62   6263.06      1.00

Number of divergences: 0
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"></code></pre></div><h3 id="results-interpretation-for-the-log-normal-survival-model-with-censored-data">Results Interpretation for the Log-Normal Survival Model with Censored Data</h3>
<p>The output provides the posterior estimates for the <strong>regression coefficients</strong> ($\beta$) and the <strong>standard deviation ($\sigma$)</strong> for the log-normal survival model with censored data. The model has converged well, as indicated by the <strong>$\hat{R}$ values</strong> and <strong>effective sample sizes ($n_{\text{eff}}$)</strong>, with no divergences reported during sampling.</p>
<ul>
<li>
<p><strong>$\beta$ Coefficients (Covariates):</strong></p>
<ul>
<li><strong>beta[0] (Bilirubin):</strong> Mean of <strong>0.91</strong>, indicating a positive association between Bilirubin levels and survival time. Higher Bilirubin levels are associated with increased survival times, and the 90% credible interval does not cross zero, suggesting a strong effect.</li>
<li><strong>beta[1] (Albumin):</strong> Mean of <strong>1.32</strong>, showing a significant positive effect of Albumin on survival time. Higher Albumin levels appear to correlate with longer survival times, and the effect is pronounced.</li>
<li><strong>beta[2] (Stage):</strong> Mean of <strong>0.61</strong>, suggesting a positive effect of Stage on survival time. The 90% credible interval remains above zero, implying that individuals at higher stages have slightly increased survival times, though the effect size is moderate.</li>
<li><strong>beta[3] and beta[4]:</strong> These coefficients represent additional covariates in the model. Both show some positive effects, with <strong>beta[3]</strong> having a mean of <strong>0.68</strong> and <strong>beta[4]</strong> showing a very slight negative effect with a mean of <strong>-0.02</strong>. However, the credible interval for <strong>beta[4]</strong> crosses zero, indicating less certainty about this effect.</li>
<li><strong>beta[5]:</strong> A mean of <strong>1.16</strong>, suggesting another strong positive association with survival time, with a credible interval that does not cross zero.</li>
</ul>
</li>
<li>
<p><strong>Standard Deviation ($\sigma$):</strong></p>
<ul>
<li>The parameter <strong>$\sigma$</strong> has a mean of <strong>5.25</strong>, indicating the variability in the log-transformed survival times. This suggests that there is a moderate amount of variance in the log-survival times across individuals, likely due to the influence of covariates.</li>
</ul>
</li>
<li>
<p><strong>Effective Sample Size ($n_{\text{eff}}$) and $\hat{R}$ Values:</strong></p>
<ul>
<li>The effective sample sizes are large, suggesting good mixing of the chains.</li>
<li>All $\hat{R}$ values are equal to <strong>1.00</strong>, confirming that the model has converged correctly.</li>
</ul>
</li>
</ul>
<p>The log-normal survival model with censored data has fit the data well, with meaningful estimates for the regression coefficients and the standard deviation. The positive effects of <strong>Bilirubin</strong> and <strong>Albumin</strong> are particularly strong, suggesting these covariates play a key role in predicting survival times. Additionally, the inclusion of censored data has improved the model&rsquo;s robustness by accounting for individuals whose survival times are unknown by the end of the study.</p>
<h3 id="posterior-predictive-check-for-the-log-normal-survival-model-with-censored-data">Posterior Predictive Check for the Log-Normal Survival Model with Censored Data</h3>
<p>The final step involves performing a <strong>posterior predictive check (PPC)</strong> to evaluate how well the log-normal survival model with censored data fits the observed data. This process involves generating predictions from the model based on the posterior samples and comparing them to the actual observed data.</p>
<h4 id="code-explanation-14">Code Explanation</h4>
<ul>
<li>
<p><strong>Posterior Predictive Samples:</strong></p>
<ul>
<li>We use the <code>Predictive</code> class to sample from the posterior distribution of the model parameters (obtained from the MCMC) and generate new predicted survival times.</li>
</ul>
</li>
<li>
<p><strong>Inference Data:</strong></p>
<ul>
<li>The generated posterior predictive samples are converted into <strong>InferenceData</strong> format using <code>ArviZ</code> to facilitate easy analysis and visualization.</li>
</ul>
</li>
<li>
<p><strong>Trace Plot:</strong></p>
<ul>
<li>The trace plot visualizes the posterior distributions of the model parameters and the convergence of the MCMC chains.</li>
</ul>
</li>
<li>
<p><strong>Posterior Predictive Check (PPC):</strong></p>
<ul>
<li>The PPC compares the predicted survival times (both censored and uncensored) with the observed survival times. This allows us to assess the model&rsquo;s fit by evaluating how closely the predicted values match the actual data.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># posterior predictive check for the Weibull model</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_samples_lognormal_censored</span> <span class="o">=</span> <span class="n">mcmc_lognormal_censored</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_predictive_lognormal_censored</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">survival_model_lognormal_with_censored</span><span class="p">,</span> <span class="n">posterior_samples_lognormal_censored</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_predictive_samples_lognormal_censored</span> <span class="o">=</span> <span class="n">posterior_predictive_lognormal_censored</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                                                                                      <span class="n">covariates</span><span class="o">=</span><span class="n">covariates_survival</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                                      <span class="n">survival_times</span><span class="o">=</span><span class="n">survival_times</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                                                                      <span class="n">event_occurred</span><span class="o">=</span><span class="n">event_occurred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to InferenceData and plot PPC</span>
</span></span><span class="line"><span class="cl"><span class="n">az_data_lognormal_censored</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_lognormal_censored</span><span class="p">,</span> <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive_samples_lognormal_censored</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_survival_with_covariates_lognormal</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">az_data_lognormal_censored</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&#34;posterior&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_110_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_110_1.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-results">Interpretation of the Results</h4>
<ol>
<li>
<p><strong>Trace Plot:</strong></p>
<ul>
<li>The trace plot shows well-mixed chains for each parameter (the $\beta$ coefficients and $\sigma$). The chains appear to have converged properly, with no visible signs of autocorrelation or poor mixing, further confirmed by the high effective sample sizes and $\hat{R}$ values from the summary.</li>
</ul>
</li>
<li>
<p><strong>Posterior Predictive Plot:</strong></p>
<ul>
<li>The PPC plot shows the predicted survival times (for both censored and uncensored data) overlaid with the actual observed data. The <strong>uncensored data</strong> (right plot) shows that the model does a reasonable job of capturing the distribution of the observed survival times, with the <strong>posterior predictive mean</strong> aligning well with the data.</li>
<li>For the <strong>censored data</strong> (left plot), the model also appears to fit well, indicating that the log-normal model appropriately captures the uncertainty introduced by censoring.</li>
</ul>
</li>
</ol>
<p>The posterior predictive check indicates that the log-normal model with censored data provides a good fit to the data. The model successfully handles both censored and uncensored survival times, making accurate predictions for the observed survival times. This confirms the model&rsquo;s robustness in dealing with time-to-event data in the presence of censoring.</p>
<p>The two plots presented below show the posterior predictive checks (PPC) for both <strong>uncensored</strong> and <strong>censored</strong> data from our log-normal survival model with censored observations.</p>
<h3 id="uncensored-data-plot">Uncensored Data Plot:</h3>
<p>The first plot compares the <strong>observed uncensored data</strong> (green line) with the <strong>posterior predictive distribution</strong> (blue line).</p>
<ul>
<li><strong>Observed Uncensored Data</strong>: These are individuals for whom the event (e.g., death) has occurred within the study period, and the exact survival times are known.</li>
<li><strong>Posterior Predictive Uncensored</strong>: The blue line represents the model&rsquo;s predicted distribution for these uncensored individuals, given the posterior distribution of parameters.</li>
</ul>
<p>In this plot, we observe a reasonable fit between the observed and predicted distributions. The general shape of both lines aligns, with some differences in fluctuation, which could be attributed to noise or model limitations. The model captures the overall distribution of survival times for individuals who experienced the event.</p>
<h3 id="censored-data-plot">Censored Data Plot:</h3>
<p>The second plot compares the <strong>observed censored data</strong> (green line) with the <strong>posterior predictive distribution</strong> (blue line).</p>
<ul>
<li><strong>Observed Censored Data</strong>: This represents individuals whose event did not occur by the end of the study period, so we only know they survived at least until a certain time. The x-axis shows the proportion of time for which the event had not occurred (between 0 and 1).</li>
<li><strong>Posterior Predictive Censored</strong>: The blue line shows the predicted distribution for censored individuals based on the posterior samples from the model.</li>
</ul>
<p>The fit is close, with both lines following similar trends. The model correctly predicts that most censored individuals have survival probabilities close to either 0 or 1, reflecting the individuals who were not likely to experience the event by the end of the study. This shows that the model is appropriately accounting for the censored data.</p>
<p>Overall, both PPC plots demonstrate that the model is capable of predicting survival times effectively, accounting for both censored and uncensored data, and matching the general distribution patterns in the dataset.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;observed_data&#34;</span><span class="p">][</span><span class="s2">&#34;obs_uncensored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C2&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Observed Data Uncensored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;posterior_predictive&#34;</span><span class="p">][</span><span class="s2">&#34;obs_uncensored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C0&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Posterior Predictive Uncensored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;observed_data&#34;</span><span class="p">][</span><span class="s2">&#34;obs_censored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C2&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Observed Data Censored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;posterior_predictive&#34;</span><span class="p">][</span><span class="s2">&#34;obs_censored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C0&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Posterior Predictive Censored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_113_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_113_1.png" type="" alt="png"  /></p>
<h1 id="5-conclusion">5. Conclusion</h1>
<p>In this notebook, we conducted a comprehensive analysis using <strong>Bayesian survival modeling</strong> techniques to predict survival times in a dataset of cirrhosis patients. We began with exploratory data analysis (EDA), examining key variables and correlations to gain an understanding of the dataset. We then moved to <strong>classification modeling</strong> using a <strong>Bayesian logistic regression model</strong> to predict survival status, where we explored the relationships between covariates and the binary outcome.</p>
<p>Afterwards, we shifted our focus to <strong>survival analysis</strong>. We first implemented a <strong>Weibull survival model</strong> to model survival times based solely on the observed uncensored data. We explored the shortcomings of this model, particularly when considering the flexibility of the data. Subsequently, we introduced a <strong>log-normal survival model</strong>, which proved more appropriate for capturing the distribution of survival times.</p>
<p>To account for individual-specific factors, we introduced <strong>covariates</strong> into the survival models, enabling more personalized predictions based on factors such as <strong>bilirubin levels</strong>, <strong>age</strong>, and <strong>albumin</strong>. This allowed us to model survival times as a function of these covariates, further improving the predictive power of the model.</p>
<p>Finally, we tackled the challenge of <strong>censored data</strong>, which occurs when the exact event time is unknown for some individuals. We adjusted the <strong>log-normal model</strong> to account for both censored and uncensored observations, demonstrating that Bayesian modeling can handle these complexities effectively. We used <strong>posterior predictive checks (PPC)</strong> to validate the models and showed that the predicted survival times align well with the observed data, both for censored and uncensored cases.</p>
<p>Through this journey, we demonstrated how <strong>Bayesian inference</strong> and <strong>probabilistic programming</strong> provide powerful tools for survival analysis. By leveraging <strong>MCMC sampling</strong>, we were able to estimate the posterior distributions of key parameters and perform robust predictions, while accounting for the uncertainty inherent in the data. Overall, we illustrated the flexibility and capability of Bayesian methods for complex survival analysis tasks.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Bayesian Linear Regression with PyMC</title>
      <link>http://localhost:1313/posts/20240527_baylinreg_pymc/e01_baylinreg_pymc/</link>
      <pubDate>Mon, 27 May 2024 16:57:31 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240527_baylinreg_pymc/e01_baylinreg_pymc/</guid>
      <description>Learn the basics of Bayesian linear regression using the excellent PyMC Probabilistic Programming package. This focuses on model formulation in PyMC, interpretation, and how to make predictions on out-of-sample data.</description>
      <content:encoded><![CDATA[<p><a href="https://colab.research.google.com/github/vflores-io/Portfolio/blob/main/Bayesian%20Methods%20Tutorials/Python/PyMC/E01_BayLinReg/E01_BayLinReg_PyMC.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<hr>
<h3 id="problem-statement">Problem Statement</h3>
<p>In this notebook, we will explore the relationship between height and weight using Bayesian linear regression. Our goal is to fit a linear model of the form:</p>
<p>$$ y = \alpha + \beta x + \varepsilon $$</p>
<p>where:</p>
<ul>
<li>$y$ represents the weight,</li>
<li>$x$ represents the height,</li>
<li>$\alpha$ is the intercept,</li>
<li>$\beta$ is the slope,</li>
<li>$\varepsilon$ is the error term, modeled as Gaussian white noise, i.e., $\varepsilon \sim \mathcal{N}(0, \sigma)$, where $\sigma$ is the standard deviation of the noise.</li>
</ul>
<p>We will use Bayesian inference to estimate the posterior distributions of $\alpha$ and $\beta$ given our data and prior assumptions. Bayesian methods provide a natural way to quantify uncertainty in our parameter estimates and predictions.</p>
<h3 id="approach">Approach</h3>
<p>To achieve our goal, we will:</p>
<ol>
<li><strong>Load Real Data:</strong> We will use an actual dataset representing the heights and weights of individuals, sourced from <a href="https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset">Kaggle</a>.</li>
<li><strong>Define the Bayesian Model:</strong> Using the probabilistic programming package <code>PyMC</code>, we will define our Bayesian linear regression model, specifying our priors for $\alpha$, $\beta$, and $\sigma$.</li>
<li><strong>Perform Inference:</strong> We will use Markov Chain Monte Carlo (MCMC) algorithms, such as the No-U-Turn Sampler (NUTS), to sample from the posterior distributions of our model parameters.</li>
<li><strong>Visualization and Prediction:</strong> We will visualize the results, including the regression lines sampled from the posterior, the uncertainty intervals, and make predictions on new, unobserved data points.</li>
</ol>
<h3 id="reference">Reference</h3>
<p>This notebook is inspired by examples from the <code>PyMC</code> documentation, specifically the <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html">Generalized Linear Regression tutorial</a>. It also builds upon a <a href="https://vflores-io.github.io/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/">similar implementation in Julia using <code>Turing.jl</code></a>. This <code>PyMC</code> recreation aims at providing a more complete illustration of the use of probabilistic programming languages.</p>
<h3 id="initial-setup">Initial setup</h3>
<p>Import the necessary packages.</p>
<p>Additionally, this notebook is supposed to be used in Google Colab. The data set (CSV) file is hosted in a private github repo. Therefore, include the github cloning to the temporary session so that the data can be accessed and used in the Colab session.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">xarray</span> <span class="k">as</span> <span class="nn">xr</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;text.usetex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.family&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;STIXGeneral&#39;</span>
</span></span></code></pre></div><h2 id="bayesian-workflow">Bayesian Workflow</h2>
<p>For this exercise, I will implement the following workflow:</p>
<ul>
<li>Collect data: this will be implemented by downloading the relevant data set</li>
<li>Build a Bayesian model: this will be built using <code>PyMC</code></li>
<li>Infer the posterior distributions of the parameters $\alpha$ and $\beta$, as well as the model noise</li>
<li>Evaluate the fit of the model</li>
</ul>
<h3 id="collecting-the-data">Collecting the data</h3>
<p>The data to be analyzed will be the height vs. weight data from <a href="https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset">https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># load the data and print the header</span>
</span></span><span class="line"><span class="cl"><span class="n">csv_path</span> <span class="o">=</span> <span class="s1">&#39;data/SOCR-HeightWeight.csv&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Index</th>
      <th>Height(Inches)</th>
      <th>Weight(Pounds)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>65.78331</td>
      <td>112.9925</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>71.51521</td>
      <td>136.4873</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>69.39874</td>
      <td>153.0269</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>68.21660</td>
      <td>142.3354</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>67.78781</td>
      <td>144.2971</td>
    </tr>
  </tbody>
</table>
</div>
<p>Let&rsquo;s instead work with the International System.</p>
<p>Convert the values to centimeters and kilograms.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Renaming columns 2 and 3</span>
</span></span><span class="line"><span class="cl"><span class="n">new_column_names</span> <span class="o">=</span> <span class="p">{</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="s1">&#39;Height (cm)&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span> <span class="s1">&#39;Weight (kg)&#39;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="n">new_column_names</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert the values to SI units</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">*</span><span class="mf">2.54</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span><span class="o">*</span><span class="mf">0.454</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># assign the relevant data to variables for easier manipulation</span>
</span></span><span class="line"><span class="cl"><span class="n">height</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Height (cm)&#39;</span><span class="p">][:</span><span class="mi">1000</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">weight</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Weight (kg)&#39;</span><span class="p">][:</span><span class="mi">1000</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Index</th>
      <th>Height (cm)</th>
      <th>Weight (kg)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>167.089607</td>
      <td>51.298595</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>181.648633</td>
      <td>61.965234</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>176.272800</td>
      <td>69.474213</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>173.270164</td>
      <td>64.620272</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>172.181037</td>
      <td>65.510883</td>
    </tr>
  </tbody>
</table>
</div>
<h3 id="visualize-the-data">Visualize the data</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># scatter plot of the data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Height vs. Weight&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Height (cm)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Weight (kg)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># plt.show()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_11_0.png" type="" alt="png"  /></p>
<h2 id="building-a-bayesian-model-with-pymc">Building a Bayesian model with <code>PyMC</code></h2>
<p>First, we assume that the weight is a variable dependent on the height. Thus, we can express the Bayesian model as:</p>
<p>$$y \sim \mathcal{N}(\alpha + \beta \mathbf{X}, \sigma^2)$$</p>
<p>Since we want to <em>infer</em> the posterior distribution of the parameters $\theta = {\alpha, \beta, \sigma }$, we need to assign priors to those variables. Remember that $\sigma$ is a measure of the uncertainty in <em>the model</em>.</p>
<p>$$
\begin{align*}
\alpha &amp;\sim \mathcal{N}(0,10) \\
\beta &amp;\sim \mathcal{N}(0,1) \\
\sigma &amp;\sim \mathcal{TN}(0,100; 0, \infty)
\end{align*}
$$
The last distribution is a <em>truncated normal distribution</em> bounded from 0 to $\infty$.</p>
<p><strong>Note</strong>: Here, we define the input data <code>height</code> as a <code>MutableData</code> container. The reason for this is because, later, we will want to change this input data, to make predictions. This will become clear a bit later.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">blr_model</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">MutableData</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># define the priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">lower</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># define the likelihood - assign the variable name &#34;y&#34; to the observations</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">weight</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># inference - crank up the bayes!</span>
</span></span><span class="line"><span class="cl">    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [alpha, beta, sigma]
</code></pre>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [8000/8000 00:37&lt;00:00 Sampling 4 chains, 0 divergences]
</div>
<pre><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 53 seconds.
</code></pre>
<p>We can explore the trace object.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">trace</span><span class="o">.</span><span class="n">to_dataframe</span><span class="p">()</span><span class="o">.</span><span class="n">columns</span>
</span></span></code></pre></div><pre><code>Index([                                  'chain',
                                          'draw',
                          ('posterior', 'alpha'),
                           ('posterior', 'beta'),
                          ('posterior', 'sigma'),
           ('sample_stats', 'perf_counter_diff'),
          ('sample_stats', 'perf_counter_start'),
             ('sample_stats', 'smallest_eigval'),
               ('sample_stats', 'step_size_bar'),
         ('sample_stats', 'index_in_trajectory'),
                      ('sample_stats', 'energy'),
            ('sample_stats', 'max_energy_error'),
                ('sample_stats', 'energy_error'),
             ('sample_stats', 'acceptance_rate'),
                  ('sample_stats', 'tree_depth'),
           ('sample_stats', 'process_time_diff'),
                   ('sample_stats', 'step_size'),
                     ('sample_stats', 'n_steps'),
              ('sample_stats', 'largest_eigval'),
                   ('sample_stats', 'diverging'),
                          ('sample_stats', 'lp'),
       ('sample_stats', 'reached_max_treedepth')],
      dtype='object')
</code></pre>
<h4 id="visualize-the-inference-diagnostics">Visualize the inference diagnostics</h4>
<p>Now that we have performed Bayesian inference using the <code>NUTS()</code> algorithm, we can visualize the results. Additionally, call for a summary of the statistics of the inferred posterior distributions of $\theta$.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># visualize the results</span>
</span></span><span class="line"><span class="cl"><span class="c1"># az.style.use(&#39;arviz-darkgrid&#39;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">labeller</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">MapLabeller</span><span class="p">(</span><span class="n">var_name_map</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="sa">r</span><span class="s1">&#39;$\alpha$&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="sa">r</span><span class="s1">&#39;$\beta$&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">],</span> <span class="n">labeller</span> <span class="o">=</span> <span class="n">labeller</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># plt.show()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_17_0.png" type="" alt="png"  /></p>
<h4 id="interpreting-the-mcmc-diagnostics-plots">Interpreting the MCMC Diagnostics Plots</h4>
<p>Trace plots are crucial for diagnosing the performance of Markov Chain Monte Carlo (MCMC) algorithms. These plots typically consist of two parts for each parameter: the trace plot and the posterior density plot.</p>
<p>The trace plot shows the sampled values of a parameter across iterations. A well-behaved trace plot should look like a &ldquo;hairy caterpillar,&rdquo; indicating good mixing. This means the trace should move around the parameter space without getting stuck and should not display any apparent patterns or trends. If the trace shows a clear trend or drift, it suggests that the chain has not yet converged. For the parameters $\alpha$ (intercept), $\beta$ (slope), and $\sigma$ (standard deviation of noise), we want to see the traces for different chains mixing well and stabilizing around a constant mean.</p>
<p>The posterior density plot shows the distribution of the sampled values of a parameter. This plot helps visualize the posterior distribution of the parameter. A good density plot should be smooth and unimodal, indicating that the parameter has a well-defined posterior distribution. If multiple chains are used, their density plots should overlap significantly, suggesting that all chains are sampling from the same distribution. For $\alpha$, $\beta$, and $\sigma$, overlapping density plots indicate that the chains have converged to the same posterior distribution.</p>
<p>Next, we can visualize the posterior distributions of the inferred parameters.eters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># visualize the posterior distributions</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">],</span> <span class="n">labeller</span> <span class="o">=</span> <span class="n">labeller</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_19_0.png" type="" alt="png"  /></p>
<p>After visualizing the inference diagnostics and the posterior distributions of the paramters, we can also obtain the summary statistics.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get the summary statistics of the posterior distributions</span>
</span></span><span class="line"><span class="cl"><span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s2">&#34;stats&#34;</span><span class="p">)</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha</th>
      <td>-28.557</td>
      <td>4.558</td>
      <td>-36.650</td>
      <td>-19.619</td>
    </tr>
    <tr>
      <th>beta</th>
      <td>0.500</td>
      <td>0.026</td>
      <td>0.449</td>
      <td>0.548</td>
    </tr>
    <tr>
      <th>sigma</th>
      <td>4.657</td>
      <td>0.100</td>
      <td>4.474</td>
      <td>4.850</td>
    </tr>
  </tbody>
</table>
</div>
<h3 id="visualize-the-results">Visualize the results</h3>
<p>Now that we have posterior distributions for the parameters $\theta$, we can plot the the resulting linear regression functions. The following is an excerpt from PyMC&rsquo;s <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html">Generalized Linear Regression tutorial</a>:</p>
<blockquote>
<p>In GLMs, we do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. We can manually generate these regression lines using the posterior samples directly.</p></blockquote>
<p>Below, what we will effectively be doing is:</p>
<p>$$ y_i = \alpha_i + \beta_i \mathbf{X} \ \ \ , \ \ \ {i = 1, \ldots , N_{samples}}$$</p>
<p>where $N_{samples}$ are the number of samples from the posterior. This number comes from the inference procedure, and in practical terms is the umber of samples we asked <code>PyMC</code> to produce.</p>
<p>In other words, plotting the samples from the posterior distribution involves plotting the regression lines sampled from the posterior. Each sample represents a possible realization of the regression line based on the sampled values of the parameters $\alpha$ (intercept) and $\beta$ (slope).</p>
<p>These sample regression lines ullustrate the uncertainty in the regression model&rsquo;s parameters and how this uncertainty propagates into the predictions (of the regression line).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># use the posterior to create regression line samples</span>
</span></span><span class="line"><span class="cl"><span class="c1"># equivalent to: y[i]  = alpha[i] + beta[i]*X</span>
</span></span><span class="line"><span class="cl"><span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&#34;y_posterior&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&#34;alpha&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&#34;beta&#34;</span><span class="p">]</span><span class="o">*</span><span class="n">xr</span><span class="o">.</span><span class="n">DataArray</span><span class="p">(</span><span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the regression lines</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_lm</span><span class="p">(</span><span class="n">idata</span> <span class="o">=</span> <span class="n">trace</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">weight</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">height</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">y_model</span><span class="o">=</span><span class="s2">&#34;y_posterior&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;b&#34;</span><span class="p">,</span> <span class="s2">&#34;alpha&#34;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span> <span class="s2">&#34;markeredgecolor&#34;</span><span class="p">:</span><span class="s2">&#34;k&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span><span class="s2">&#34;Observed Data&#34;</span><span class="p">,</span> <span class="s2">&#34;markersize&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_model_plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s2">&#34;zorder&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;#00cc99&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_model_mean_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;red&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_23_0.png" type="" alt="png"  /></p>
<h2 id="using-the-linear-regression-model-to-make-predictions">Using the Linear Regression Model to Make Predictions</h2>
<p>Now that we have a fitted Bayesian linear regression model, we can use it to make predictions. This involves sampling from the posterior predictive distribution, which allows us to generate predictions for new data points while incorporating the uncertainty from the posterior distribution <em>of the parameters</em>.</p>
<h4 id="sample-from-the-posterior-predictive-distribution">Sample from the Posterior Predictive Distribution:</h4>
<ul>
<li>This step involves using the inferred <code>trace</code> from our Bayesian linear regression model <code>blr_model</code> to generate predictions. The <code>pm.sample_posterior_predictive</code> function in PyMC allows us to do this. It uses the posterior samples of the parameters to compute the predicted values of the outcome variable.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># now predict the outcomes using the inferred trace</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">blr_model</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># use the updated values and predict outcomes and probabilities:</span>
</span></span><span class="line"><span class="cl">    <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">trace</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">var_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">extend_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><pre><code>Sampling: [y]
</code></pre>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [4000/4000 00:00&lt;00:00]
</div>
<h4 id="exploring-the-trace-object">Exploring the Trace Object</h4>
<p>The trace object stores the results of our inference. Initially, it contained the posterior samples of the model parameters (e.g., intercept and slope).</p>
<p>After running <code>pm.sample_posterior_predictive</code>, the trace object is extended to include the posterior predictive samples. These are the predicted values for the outcome variable, given the posterior distribution of the model parameters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># explore the trace object again</span>
</span></span><span class="line"><span class="cl"><span class="n">trace</span><span class="o">.</span><span class="n">to_dataframe</span><span class="p">()</span><span class="o">.</span><span class="n">columns</span>
</span></span></code></pre></div><pre><code>Index([                                  'chain',
                                          'draw',
                          ('posterior', 'alpha'),
                           ('posterior', 'beta'),
                          ('posterior', 'sigma'),
              ('posterior', 'y_posterior[0]', 0),
          ('posterior', 'y_posterior[100]', 100),
          ('posterior', 'y_posterior[101]', 101),
          ('posterior', 'y_posterior[102]', 102),
          ('posterior', 'y_posterior[103]', 103),
       ...
                ('sample_stats', 'energy_error'),
             ('sample_stats', 'acceptance_rate'),
                  ('sample_stats', 'tree_depth'),
           ('sample_stats', 'process_time_diff'),
                   ('sample_stats', 'step_size'),
                     ('sample_stats', 'n_steps'),
              ('sample_stats', 'largest_eigval'),
                   ('sample_stats', 'diverging'),
                          ('sample_stats', 'lp'),
       ('sample_stats', 'reached_max_treedepth')],
      dtype='object', length=2022)
</code></pre>
<p>We can observe how now we have another inference data container: <code>posterior_predictive</code>. This was generated by passing the <code>extend_inferencedata</code> argument to the <code>pm.sample_posterior_predictive</code> function above.</p>
<p>This data contains predictions by passing the observed heights through our linear model and making predictions. Note that these &ldquo;predictions&rdquo; are made on <strong>observed data</strong>. This is similar to using validating the predictions on training data in machine learning, i.e. comparing the model predictions to the actual data on an observed input.</p>
<p>We can use the linear regression model to make predictions. It should be noted that, again, the linear regression model is not a single regression line, but rather a set of regression lines generated from the posterior probability of $\theta$.</p>
<h4 id="visualize-the-prediction-confidence-interval">Visualize the Prediction Confidence Interval</h4>
<p>After we sampled from the posterior, we might want to visualize this to understand the posterior predictive distribution.</p>
<p>In the code below, there are two things going on, let&rsquo;s go through them.</p>
<ol>
<li>Plotting the samples from the posterior distribution</li>
</ol>
<p>This part is exactly what we did before, which is plotting the sample posteriors of the <strong>regression line</strong>. These sample regression lines are a natural product of propagating the uncertainty from the parameters unto the prediction line.</p>
<ol start="2">
<li>Plotting the uncertainty in the mean and the observations</li>
</ol>
<p>Now we can add a ribbon to show the uncertainty not only in the regression line, but in the prediction points themselves. That is, that ribbon will tell us where we might expect a prediction point $i+1$, i.e.</p>
<p>$$ y_{i+1} = \alpha_{i+1} + \beta_{i+1} x^* $$</p>
<p>where $x^*$ is a test input point. In other words, and more specific to this demonstration:</p>
<blockquote>
<p>what is the <em>interval</em> where we would expect a predicted weight $y_{i+1}$ of an individual with a height $x*$.</p></blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># use the posterior to create regression line samples</span>
</span></span><span class="line"><span class="cl"><span class="c1"># trace.posterior[&#34;y_posterior&#34;] = trace.posterior[&#34;alpha&#34;] + trace.posterior[&#34;beta&#34;]*xr.DataArray(height)  # y_posterior = alpha + beta*x</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_lm</span><span class="p">(</span><span class="n">idata</span> <span class="o">=</span> <span class="n">trace</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">weight</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">height</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">y_model</span><span class="o">=</span><span class="s2">&#34;y_posterior&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;b&#34;</span><span class="p">,</span> <span class="s2">&#34;alpha&#34;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span> <span class="s2">&#34;markeredgecolor&#34;</span><span class="p">:</span><span class="s2">&#34;k&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span><span class="s2">&#34;Observed Data&#34;</span><span class="p">,</span> <span class="s2">&#34;markersize&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_model_plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s2">&#34;zorder&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;#00cc99&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_model_mean_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;red&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the prediction interval</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_hdi</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&#34;y&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">fill_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_30_0.png" type="" alt="png"  /></p>
<h3 id="making-predictions-on-unobserved-data-inputs">Making Predictions on <em>Unobserved Data Inputs</em></h3>
<p>Now, how about the case when we want to make predictions on test data that we have not seen? That is, predict the weight of an individual whose height/weight we have not observed (measured)</p>
<p>In other words, we have some test input data, i.e. some heights for which we want to predict the weights.</p>
<p>Some references of where I learned how to do this:</p>
<ol>
<li>
<p>In <a href="https://www.pymc.io/projects/examples/en/latest/fundamentals/data_container.html#applied-example-height-of-toddlers-as-a-function-of-age">this example</a> and <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html">this other example</a> it says that we can generate out-of-sample predictions by using <code>pm.sample_posterior_predictive</code> and it shows an example of how to use the syntax.</p>
</li>
<li>
<p>More recently, <a href="https://www.pymc-labs.com/blog-posts/out-of-model-predictions-with-pymc/">this demo blog post</a> clarifies how to make predictions on out-of-model samples.</p>
</li>
</ol>
<p>Let&rsquo;s do just that now. First, we will define the test inputs we want to predict for, <code>pred_height</code>. Then, inside the model, we replace the data (which was defined as <code>MutableData</code>, with the new data we want to make predictions on. This is done as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># set new data inputs:</span>
</span></span><span class="line"><span class="cl"><span class="n">pred_height</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="s1">&#39;new_data&#39;</span> <span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">blr_model</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s1">&#39;height&#39;</span><span class="p">:</span> <span class="n">pred_height</span><span class="p">})</span>
</span></span></code></pre></div><p>What this is effectively doing is telling <code>sample_posterior_predictive</code> that we need to make predictions on <code>height</code> which now happens to be different.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># define the out-of-sample predictors</span>
</span></span><span class="line"><span class="cl"><span class="n">pred_height</span> <span class="o">=</span> <span class="p">[</span><span class="mf">158.0</span><span class="p">,</span> <span class="mf">185.5</span><span class="p">,</span> <span class="mf">165.2</span><span class="p">,</span> <span class="mf">178.0</span><span class="p">,</span>  <span class="mf">180.0</span><span class="p">,</span> <span class="mf">170.2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">pred_height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">blr_model</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># set the new data we want to make predictions for</span>
</span></span><span class="line"><span class="cl">    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s1">&#39;height&#39;</span><span class="p">:</span> <span class="n">pred_height</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">post_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">trace</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">predictions</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><pre><code>Sampling: [y]


[158.0, 185.5, 165.2, 178.0, 180.0, 170.2]
</code></pre>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [4000/4000 00:00&lt;00:00]
</div>
<p>What we have done above is create an inference data object called <code>post_pred</code>. This object contains the samples of the predictions on the new data. Specifically, it includes two containers: <code>predictions</code> and <code>predictions_constant_data</code>.</p>
<p>The <code>predictions</code> container holds the predicted samples for our new heights. The <code>predictions_constant_data</code> holds the new heights we passed into the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">post_pred</span><span class="o">.</span><span class="n">to_dataframe</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chain</th>
      <th>draw</th>
      <th>(y[0], 0)</th>
      <th>(y[1], 1)</th>
      <th>(y[2], 2)</th>
      <th>(y[3], 3)</th>
      <th>(y[4], 4)</th>
      <th>(y[5], 5)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>48.981930</td>
      <td>62.971186</td>
      <td>62.143385</td>
      <td>59.300742</td>
      <td>56.100237</td>
      <td>54.329348</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>55.481192</td>
      <td>65.132876</td>
      <td>54.761877</td>
      <td>61.312254</td>
      <td>59.220124</td>
      <td>51.817360</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>2</td>
      <td>49.471550</td>
      <td>66.016910</td>
      <td>60.646273</td>
      <td>57.876344</td>
      <td>56.203720</td>
      <td>60.318281</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>3</td>
      <td>53.373737</td>
      <td>66.593653</td>
      <td>53.085799</td>
      <td>63.437949</td>
      <td>64.336626</td>
      <td>45.372830</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>4</td>
      <td>52.981309</td>
      <td>69.320059</td>
      <td>51.590686</td>
      <td>60.372046</td>
      <td>62.210738</td>
      <td>48.188656</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3995</th>
      <td>3</td>
      <td>995</td>
      <td>52.303814</td>
      <td>61.931117</td>
      <td>47.544216</td>
      <td>60.824401</td>
      <td>61.469545</td>
      <td>62.353284</td>
    </tr>
    <tr>
      <th>3996</th>
      <td>3</td>
      <td>996</td>
      <td>56.032295</td>
      <td>56.979040</td>
      <td>54.584837</td>
      <td>55.894216</td>
      <td>65.943908</td>
      <td>50.929285</td>
    </tr>
    <tr>
      <th>3997</th>
      <td>3</td>
      <td>997</td>
      <td>56.062352</td>
      <td>50.889499</td>
      <td>51.441003</td>
      <td>57.841533</td>
      <td>62.898654</td>
      <td>52.749139</td>
    </tr>
    <tr>
      <th>3998</th>
      <td>3</td>
      <td>998</td>
      <td>48.228772</td>
      <td>65.983383</td>
      <td>52.381164</td>
      <td>55.283946</td>
      <td>65.468049</td>
      <td>70.367514</td>
    </tr>
    <tr>
      <th>3999</th>
      <td>3</td>
      <td>999</td>
      <td>58.434184</td>
      <td>54.739363</td>
      <td>56.773260</td>
      <td>53.128112</td>
      <td>61.695469</td>
      <td>54.874142</td>
    </tr>
  </tbody>
</table>
<p>4000 rows × 8 columns</p>
</div>
<p>We can visualize the posterior distributions of the predictions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">post_pred</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&#34;predictions&#34;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_36_1.png" type="" alt="png"  /></p>
<p>We can obtain point estimates by taking the mean of each prediction distribution. This is done by taking the mean of the predictions over the <code>chain</code> and <code>draw</code> dimensions, as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">pred_weight</span> <span class="o">=</span> <span class="n">post_pred</span><span class="o">.</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;chain&#39;</span><span class="p">,</span> <span class="s1">&#39;draw&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Predicted weights: &#34;</span><span class="p">,</span> <span class="n">pred_weight</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Predicted weights:  [50.37415152 64.29241929 54.02070975 60.60276731 61.36759368 56.53983895]
</code></pre>
<p>Finally, we can visualize where the predictions fall by adding a scatter plot with the new ${x^<em>, y^</em>}$ data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># use the posterior to create regression line samples</span>
</span></span><span class="line"><span class="cl"><span class="c1"># trace.posterior[&#34;y_posterior&#34;] = trace.posterior[&#34;alpha&#34;] + trace.posterior[&#34;beta&#34;]*xr.DataArray(height)  # y_posterior = alpha + beta*x</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_lm</span><span class="p">(</span><span class="n">idata</span> <span class="o">=</span> <span class="n">trace</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">weight</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">height</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">y_model</span><span class="o">=</span><span class="s2">&#34;y_posterior&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;b&#34;</span><span class="p">,</span> <span class="s2">&#34;alpha&#34;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span> <span class="s2">&#34;markeredgecolor&#34;</span><span class="p">:</span><span class="s2">&#34;k&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span><span class="s2">&#34;Observed Data&#34;</span><span class="p">,</span> <span class="s2">&#34;markersize&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_model_plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s2">&#34;zorder&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;#00cc99&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">           <span class="n">y_model_mean_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span><span class="s2">&#34;red&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the prediction interval</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_hdi</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&#34;y&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">fill_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># add predicted weights to the plot</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">pred_weight</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Predicted Weights&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">zorder</span> <span class="o">=</span> <span class="mi">15</span>
</span></span><span class="line"><span class="cl">           <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240527_BayLinReg_PyMC/output_40_0.png" type="" alt="png"  /></p>
<h2 id="thank-you">Thank you!</h2>
<p>This demo focused on a relatively simple task. Here, however, we focused more on what a Bayesian approach means in the context of a linear regression. Additionally, we focused on using <code>PyMC</code> for developing the model, visualizing the results and, just as importantly, on making predictions using those results.</p>
<p>Victor</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Bayesian Time Series Analysis with Julia and Turing.jl</title>
      <link>http://localhost:1313/posts/20240222_bayesian_time_series_analysis/20240222_bayesian_time_series_analysis/</link>
      <pubDate>Sat, 02 Mar 2024 16:57:07 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240222_bayesian_time_series_analysis/20240222_bayesian_time_series_analysis/</guid>
      <description>This tutorial covers the fundamentals of Bayesian approaches to time series, model construction, and practical implementation, using real-world data for hands-on learning.</description>
      <content:encoded><![CDATA[<hr>
<h2 id="introduction">Introduction</h2>
<p>In this tutorial, an AR(p) (Autoregressive model of order <em>p</em>) is employed to analyze the trneds of a time series and forecast the behavior of the signal.</p>
<p>Auto-regressive models are based on the assumption the behavior of a time series or signal depends on past values. The order of the AR model tells &ldquo;how far back&rdquo; the past values will affect the current value.</p>
<h4 id="credits">Credits</h4>
<p>This exercise is mostly following <a href="https://youtu.be/vfTYCm_Fr8I?si=D3Grgk82tV_Qzdxw">this tutorial</a>.</p>
<h3 id="definition">Definition</h3>
<p>The <em>AR(p)</em> model is defined as:</p>
<p>$$
X_t = \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t
$$</p>
<p>where $\varepsilon \sim \mathcal{N}(0,\sigma^2)$ is the model uncertainty represented as white Gaussian noise, i.e. it follows a normal distribution of mean $\mu=0$ and standard deviation $\sigma$.</p>
<p>It follows that an <em>AR(2)</em> model is defined as:</p>
<p>$$
X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \varepsilon_t
$$</p>
<p>Naturally, we want to find the parameters $\theta={\phi_1, \phi_2,\sigma}$. Since these are unobserved quantities of interest, we need to use an inference method to reveal these parameters. We will use Bayesian inference to achieve this goal.</p>
<h2 id="data-exploration">Data Exploration</h2>
<p>For this example, I will generate artificial data. This will be done by first defining some values for the parameters $\theta$ and then we will generate random data using those parameters by initializing the $X_1, X_2$ values, and then applying the AR(2) equation to generate the subsequent values.</p>
<p>First, we import the relevant packages.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">StatsPlots</span><span class="p">,</span> <span class="n">Turing</span><span class="p">,</span> <span class="n">LaTeXStrings</span><span class="p">,</span> <span class="n">Random</span><span class="p">,</span> <span class="n">DataFrames</span>
</span></span><span class="line"><span class="cl"><span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>TaskLocalRNG()
</code></pre>
<p>Now we create some artificial data. The steps involved in this are as follows:</p>
<ol>
<li>Define some values for the parameters $\theta$</li>
<li>Set the number of timesteps <em>t</em></li>
<li>Initialize an empty vector of size $\mathbb{R}^{t+p}$</li>
<li>Initialize the first two $X$ values with randomly generated numbers using <code>rand</code></li>
<li>Populate the vector by using the equation for $X_t$</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># define true values for θ</span>
</span></span><span class="line"><span class="cl"><span class="n">true_phi_1</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.4</span>
</span></span><span class="line"><span class="cl"><span class="n">true_phi_2</span> <span class="o">=</span> <span class="mf">0.3</span>
</span></span><span class="line"><span class="cl"><span class="n">true_sigma</span> <span class="o">=</span> <span class="mf">0.12</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># define the time steps</span>
</span></span><span class="line"><span class="cl"><span class="n">time</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="c"># create an empty X vector</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="kt">Vector</span><span class="p">{</span><span class="kt">Float64</span><span class="p">}(</span><span class="nb">undef</span><span class="p">,</span> <span class="n">time</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># initialize the X vector with two random values at time steps 1 and 2</span>
</span></span><span class="line"><span class="cl"><span class="c"># to do this, use a random normally distributed number with mean zero and standard deviation σ, i.e., ε~N(0, σ)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># populate vector X</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">t</span> <span class="k">in</span> <span class="mi">3</span><span class="o">:</span><span class="p">(</span><span class="n">time</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">true_phi_1</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">	<span class="n">true_phi_2</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">	<span class="n">rand</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>	
</span></span></code></pre></div><h3 id="visualize-the-artificial-data">Visualize the (Artificial) Data</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">p_data</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">3</span><span class="o">:</span><span class="k">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c"># xlims = (0, 60),</span>
</span></span><span class="line"><span class="cl">    <span class="c"># ylims = (-0.6, 0.6),</span>
</span></span><span class="line"><span class="cl">    <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Bayesian Autoregressive AR(2) Model&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">xlabel</span> <span class="o">=</span> <span class="sa">L</span><span class="s">&#34;t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">ylabel</span> <span class="o">=</span> <span class="sa">L</span><span class="s">&#34;X_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">widen</span> <span class="o">=</span> <span class="nb">true</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240222_Bayesian_Time_Series_Analysis/output_5_0.svg" type="" alt="svg"  /></p>
<h2 id="modeling">Modeling</h2>
<p>The next step is to construct our probabilistic model. Again, the goal here is to infer the values of the model parameters $\theta$. Once we have inferred these parameters, we can make probabilistic predictions on the future behavior of the signal $X$.</p>
<h3 id="bayesian-model">Bayesian model</h3>
<p>Since we are using a Bayesian approach, our goal, in Bayesian terms, is to find the <em>posterior distribution</em> of the parameters $\theta$, given a prior distribution, or prior knowledge, of the parameters before making any observations, i.e., seeing any data, and also a likelihood function, which reflects what kind of distribution (we assume) that the data is sourced from. Another way of understanding the likelihood function is the probability of making a set of observations $X$ given the parameters $\theta$.</p>
<p>This relationship is established by Bayes&rsquo; Theorem:</p>
<p>$$
P(\theta | X) \propto P(X | \theta)P(\theta)
$$</p>
<p>In summary, constructing the Bayesian model in this case comprises a selection of prior distributions for our unknown parameters $\theta$ and a likelihood function. We will do this using the <code>Turing.jl</code> package.</p>
<p>The model therefore will consist of the prior distributions:</p>
<p>$$
\begin{align*}
\phi_1 &amp; \sim \mathcal{N}(0, 1) \
\phi_2 &amp; \sim \mathcal{N}(0, 1) \
\sigma &amp; \sim \text{Exp}(1)
\end{align*}
$$</p>
<p>And the likelihood:</p>
<p>$$
X_t \sim \mathcal{N}(\mu_t, \sigma)
$$</p>
<p>where $\mu_t = \sum_{i=1}^{p} \phi_i X_{t-i}$ is the mean function of the distribution that governs X_t.</p>
<h4 id="a-comment-on-the-choice-of-priors">A comment on the choice of priors</h4>
<p>For autoregressive parameters, using a normal distribution is a common choice. This is because the normal distribution is convenient and allows for a range of plausible values.</p>
<p>For the prior on the model uncertainty, the exponential distribution is sometimes used for non-negative parameters and has a similar role to the inverse gamma.</p>
<p>Furthermore, the inverse gamma distribution is often chosen as a prior for the standard deviation because it is conjugate to the normal likelihood. This means that the posterior distribution will have a known form, making computations more tractable.</p>
<h3 id="bayesian-model-using-turingjl">Bayesian model using <code>Turing.jl</code></h3>
<p>Now we proceed to set up the model using the <code>Turing.jl</code> package.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">ar</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">time</span><span class="p">)</span>    <span class="c"># pass the data X and the time vector</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c"># priors</span>
</span></span><span class="line"><span class="cl">		
</span></span><span class="line"><span class="cl">		<span class="n">phi_1</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">phi_2</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">sigma</span> <span class="o">~</span> <span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c"># likelihood</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c"># initialize with random initial values</span>
</span></span><span class="line"><span class="cl">		<span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c"># populate with samples</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">3</span><span class="o">:</span><span class="p">(</span><span class="n">time</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">			<span class="n">mu</span> <span class="o">=</span> <span class="n">phi_1</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">phi_2</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">			<span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="k">end</span>
</span></span><span class="line"><span class="cl">	<span class="k">end</span>
</span></span></code></pre></div><pre><code>ar (generic function with 2 methods)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">ar</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">time</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sampler</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">samples</span> <span class="o">=</span> <span class="mi">1_000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.4
[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:01[39m





Chains MCMC chain (1000×15×1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 11.59 seconds
Compute duration  = 11.59 seconds
parameters        = phi_1, phi_2, sigma
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m e[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m  [0m ⋯

       phi_1   -0.3830    0.1047    0.0036   836.6151   762.4445    0.9996     ⋯
       phi_2    0.1587    0.1012    0.0035   838.3014   749.6718    1.0002     ⋯
       sigma    0.1083    0.0079    0.0003   755.4034   743.3822    1.0014     ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

       phi_1   -0.5733   -0.4562   -0.3858   -0.3141   -0.1771
       phi_2   -0.0339    0.0913    0.1562    0.2256    0.3549
       sigma    0.0943    0.1030    0.1079    0.1130    0.1257
</code></pre>
<h3 id="visualize-and-summarize-the-results">Visualize and Summarize the Results</h3>
<p>Next we can access the MCMC Diagnostics and generate a summary of the results.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240222_Bayesian_Time_Series_Analysis/output_10_0.svg" type="" alt="svg"  /></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">DataFrame</span><span class="p">(</span><span class="n">summarystats</span><span class="p">(</span><span class="n">chain</span><span class="p">))</span>
</span></span></code></pre></div><div><div style = "float: left;"><span>3×8 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">parameters</th><th style = "text-align: left;">mean</th><th style = "text-align: left;">std</th><th style = "text-align: left;">mcse</th><th style = "text-align: left;">ess_bulk</th><th style = "text-align: left;">ess_tail</th><th style = "text-align: left;">rhat</th><th style = "text-align: left;">ess_per_sec</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Symbol" style = "text-align: left;">Symbol</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: left;">phi_1</td><td style = "text-align: right;">-0.383019</td><td style = "text-align: right;">0.104695</td><td style = "text-align: right;">0.00361324</td><td style = "text-align: right;">836.615</td><td style = "text-align: right;">762.444</td><td style = "text-align: right;">0.999585</td><td style = "text-align: right;">72.1655</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: left;">phi_2</td><td style = "text-align: right;">0.158661</td><td style = "text-align: right;">0.101196</td><td style = "text-align: right;">0.00351463</td><td style = "text-align: right;">838.301</td><td style = "text-align: right;">749.672</td><td style = "text-align: right;">1.00021</td><td style = "text-align: right;">72.311</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: left;">sigma</td><td style = "text-align: right;">0.108342</td><td style = "text-align: right;">0.00788622</td><td style = "text-align: right;">0.000291067</td><td style = "text-align: right;">755.403</td><td style = "text-align: right;">743.382</td><td style = "text-align: right;">1.00145</td><td style = "text-align: right;">65.1603</td></tr></tbody></table></div>
<h2 id="predictions">Predictions</h2>
<h3 id="making-predictions">Making Predictions</h3>
<p>To make predictions, the following steps are taken:</p>
<ol>
<li>Set the number of time steps into the future, $t_f$</li>
<li>Initialize an empty matrix for the forecasted $X$ values - This will be a matrix because it will be a collection of vectors. Each vector will represent one sample forecast</li>
<li>Initialize two steps of each of the sample vectors to be generated - In practical terms, initialize the first number of each column; each <em>column</em> will represent a forecast time series</li>
</ol>
<p>Keep in mind that what will be done here is to create samples of the future behavior of the signal $t_f$ number of time steps into the future. To do this, we will generate signals that use the posterior distributions of the parameters $\theta$ by calling the function <code>rand(chain[:,Z,Z])</code> which will randomly pick a number out of the sample pool, effectively &ldquo;sampling&rdquo; from that posterior distribution (sample pool).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">time_future</span> <span class="o">=</span> <span class="mi">15</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X_future</span> <span class="o">=</span> <span class="kt">Matrix</span><span class="p">{</span><span class="kt">Float64</span><span class="p">}(</span><span class="nb">undef</span><span class="p">,</span> <span class="n">time_future</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># Initialize the first two time steps for every forecast</span>
</span></span><span class="line"><span class="cl"><span class="n">X_future</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">:</span><span class="p">]</span> <span class="o">.=</span> <span class="n">X</span><span class="p">[</span><span class="n">time</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">X_future</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">:</span><span class="p">]</span> <span class="o">.=</span> <span class="n">X</span><span class="p">[</span><span class="n">time</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># populate the forecast vectors by sampling from the posterior sample pool of the parameters θ</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">col</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">samples</span>
</span></span><span class="line"><span class="cl">	<span class="n">phi_1_future</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">	<span class="n">phi_2_future</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">	<span class="n">error_future</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">	<span class="n">noise_future</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">error_future</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">row</span> <span class="k">in</span> <span class="mi">3</span><span class="o">:</span><span class="p">(</span><span class="n">time_future</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">X_future</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> 
</span></span><span class="line"><span class="cl">			<span class="n">phi_1_future</span> <span class="o">*</span> <span class="n">X_future</span><span class="p">[</span><span class="n">row</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">			<span class="n">phi_2_future</span> <span class="o">*</span> <span class="n">X_future</span><span class="p">[</span><span class="n">row</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">			<span class="n">noise_future</span>
</span></span><span class="line"><span class="cl">	<span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><h4 id="visualize-the-forecast">Visualize the forecast</h4>
<p>Now that we <em>propagated the uncertainty</em> of in the posterior distribution of the parameters $\theta$, we can plot the posterior predictive distribution of $X$, $P(X^*|\theta)$.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">time_predict</span> <span class="o">=</span> <span class="n">time</span><span class="o">:</span><span class="p">(</span><span class="n">time</span> <span class="o">+</span> <span class="n">time_future</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">samples</span>
</span></span><span class="line"><span class="cl">	<span class="n">plot!</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">time_predict</span><span class="p">,</span> <span class="n">X_future</span><span class="p">[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">	<span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="c"># predictions</span>
</span></span><span class="line"><span class="cl">	<span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="ss">:green</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl">	<span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p_data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># visualize mean values for predictions</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X_future_mean</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean</span><span class="p">(</span><span class="n">X_future</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="o">:</span><span class="n">samples</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">2</span><span class="o">:</span><span class="p">(</span><span class="n">time_future</span><span class="o">+</span><span class="mi">2</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plot!</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">time_predict</span><span class="p">,</span> <span class="n">X_future_mean</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">	<span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">	<span class="n">color</span> <span class="o">=</span> <span class="ss">:red</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">	<span class="n">linestyle</span> <span class="o">=</span> <span class="ss">:dot</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240222_Bayesian_Time_Series_Analysis/output_15_0.svg" type="" alt="svg"  /></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"></code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>Bayesian Poisson Regression with Julia and Turing.jl</title>
      <link>http://localhost:1313/posts/20240217_bayesian_poisson_regression/20240217_bayesian_poisson_regression/</link>
      <pubDate>Sat, 17 Feb 2024 11:57:07 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240217_bayesian_poisson_regression/20240217_bayesian_poisson_regression/</guid>
      <description>Explore Bayesian Poisson regression for modeling count data with Julia and Turing.jl. This tutorial includes model setup, implementation, and performance assessment with a practical example.</description>
      <content:encoded><![CDATA[<hr>
<p>In this example, I am following the tutorials found in:</p>
<ul>
<li><a href="https://turinglang.org/dev/tutorials/07-poisson-regression/">Turing.jl - Bayesian Poisson Regression</a></li>
<li><a href="https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-poisson-regression.html">PyMC - GLM: Poisson Regression</a></li>
</ul>
<p>Both examples show the interaction between some variables and a discrete outcome. In this case, the outcome is the number of sneezes per day (i.e. a discrete outcome) in some study subjects, and whether or not they take antihistamine medicine and whether or not they drink alcohol.</p>
<p>This example explores how these factors, and more specifically, the combination of these factors, affect the number of times a person sneezes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">CSV</span><span class="p">,</span> <span class="n">DataFrames</span><span class="p">,</span> <span class="n">Turing</span><span class="p">,</span> <span class="n">StatsPlots</span><span class="p">,</span> <span class="n">Plots</span><span class="p">,</span> <span class="n">Random</span>
</span></span><span class="line"><span class="cl"><span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>TaskLocalRNG()
</code></pre>
<h2 id="collect-generate-the-data">Collect (generate) the data</h2>
<p>In this example, we will generate the data in the same way as in the tutorials:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center">No Alcohol</th>
          <th style="text-align: center">Alcohol</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>No Meds</strong></td>
          <td style="text-align: center">6</td>
          <td style="text-align: center">36</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Meds</strong></td>
          <td style="text-align: center">1</td>
          <td style="text-align: center">3</td>
      </tr>
  </tbody>
</table>
<p>Those values will be used to create the artificial data by generating Poisson-distributed random samples.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">theta_noalc_nomed</span> <span class="o">=</span> <span class="mi">6</span>
</span></span><span class="line"><span class="cl"><span class="n">theta_noalc_med</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">theta_alc_nomed</span> <span class="o">=</span> <span class="mi">36</span>
</span></span><span class="line"><span class="cl"><span class="n">theta_alc_med</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ns</span> <span class="o">=</span> <span class="mi">500</span>    <span class="c"># number of samples</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># create a data frame</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">hcat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">vcat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">rand</span><span class="p">(</span><span class="n">Poisson</span><span class="p">(</span><span class="n">theta_noalc_med</span><span class="p">),</span> <span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">rand</span><span class="p">(</span><span class="n">Poisson</span><span class="p">(</span><span class="n">theta_alc_med</span><span class="p">),</span> <span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">rand</span><span class="p">(</span><span class="n">Poisson</span><span class="p">(</span><span class="n">theta_noalc_nomed</span><span class="p">),</span> <span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">rand</span><span class="p">(</span><span class="n">Poisson</span><span class="p">(</span><span class="n">theta_alc_nomed</span><span class="p">),</span> <span class="n">ns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">vcat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">falses</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">trues</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">falses</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">trues</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">vcat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">falses</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">falses</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">trues</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">trues</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span> <span class="ss">:auto</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># assign names to headers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">head_names</span> <span class="o">=</span> <span class="p">[</span><span class="ss">:n_sneezes</span><span class="p">,</span> <span class="ss">:alcohol</span><span class="p">,</span> <span class="ss">:nomeds</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sneeze_data</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">head_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">first</span><span class="p">(</span><span class="n">sneeze_data</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span></code></pre></div><div><div style = "float: left;"><span>10×3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">n_sneezes</th><th style = "text-align: left;">alcohol</th><th style = "text-align: left;">nomeds</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">6</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">7</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">8</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">9</td><td style = "text-align: right;">2</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">10</td><td style = "text-align: right;">2</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td></tr></tbody></table></div>
<h3 id="visualize-the-data">Visualize the data</h3>
<p>Now that we have &ldquo;collected&rdquo; some data on the number of sneezes per day from a number of people, we visualize the data.</p>
<p>The way we are collecting and plotting these data sub-sets is as follows:</p>
<ol>
<li>Call the histogram function</li>
<li>Create a histogram of the dataframe &ldquo;sneeze_data&rdquo; we &ldquo;collected&rdquo; previously</li>
<li>Select a subset of that dataframe</li>
<li>All the rows of the columns where alcohol is <code>false</code> i.e. 0 AND all the rows where no medicine was taken is also <code>false</code></li>
<li>All the rows of the columns where alcohol is <code>false</code> AND all the rows of where medicine is <code>true</code></li>
<li>&hellip; and so on</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># create separate histograms for each case</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">sneeze_data</span><span class="p">[(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:alcohol</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">.&amp;</span> <span class="p">(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:nomeds</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">0</span><span class="p">),</span> <span class="ss">:n_sneezes</span><span class="p">];</span> <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;No alcohol + No Meds&#34;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">&#34;People Count&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">p2</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">sneeze_data</span><span class="p">[(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:alcohol</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">.&amp;</span> <span class="p">(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:nomeds</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">0</span><span class="p">),</span> <span class="ss">:n_sneezes</span><span class="p">];</span> <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;No alcohol + Meds&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">p3</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">sneeze_data</span><span class="p">[(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:alcohol</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">.&amp;</span> <span class="p">(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:nomeds</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">1</span><span class="p">),</span> <span class="ss">:n_sneezes</span><span class="p">];</span> <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Alcohol + No Meds&#34;</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">&#34;Sneezes/Day&#34;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">&#34;People Count&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">p4</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">sneeze_data</span><span class="p">[(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:alcohol</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">.&amp;</span> <span class="p">(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="ss">:nomeds</span><span class="p">]</span> <span class="o">.==</span> <span class="mi">1</span><span class="p">),</span> <span class="ss">:n_sneezes</span><span class="p">];</span> <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Alcohol + Meds&#34;</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s">&#34;Sneezes/Day&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p4</span><span class="p">;</span> <span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240217_Bayesian_Poisson_Regression/output_5_0.svg" type="" alt="svg"  /></p>
<h3 id="interpreting-the-data">Interpreting the data</h3>
<p>The histograms show that the data from the &ldquo;study&rdquo; resembles a Poisson distribution (as mentioned in the PyMC tutorial, this is obvious, because that&rsquo;s how the data is generated!). Furthermore, the data is telling us something:</p>
<ul>
<li>Looking at the plot for &ldquo;no alcohol and medicine&rdquo; it is clear that most people reported very few sneezes; notice how the histogram skews towards large counts (of people) for very few sneezes</li>
<li>On the other hand, notice how the &ldquo;alcohol and <em>no</em> medicine&rdquo; seems to tell us that many reported somewhere around 35 sneezes per day</li>
</ul>
<p>Again, we can start thinking of a pattern just by looking at the data, and it seems like the data is telling us that if you don&rsquo;t drink alcohol and take antihistamines, you are less likely to be sneezing around than if you drink alcohol and don&rsquo;t take any allergy meds. Makes sense, right?</p>
<h2 id="model">Model</h2>
<p>We established that the data looks like it could be modelled as a Poisson distribution. Thus, we can define our probabilistic model as follows:</p>
<p>$$Y_{obs} \sim Poisson(\lambda)$$</p>
<p>$$\log(\lambda) = \theta&rsquo;\mathbf{x} = \alpha + \beta&rsquo; \mathbf{x}$$</p>
<p>What the above means is that we assume that the observed data outcomes, i.e., the number of sneezes per day, follow a Poisson distribution, which is a discrete probability distribution that models the number of events that occur in a fixed interval of time or space. The rate or intensity of the events, $\lambda$, depends on the predictor variables (the input data) $\mathcal{x}$, such as the season, the temperature, or, in our case, whether a person ingested alcohol and whether the person took antihistamines.</p>
<p>The linear predictor $\theta&rsquo; \mathcal{x}$ is the function that links the predictor variables to the rate parameter, where $\theta = {\alpha, \beta&rsquo;}$ are the parameters of the model.</p>
<p>Looking at the structure of the linear relationship between the paramters of the model, and the predictors:</p>
<p>$$\log(\lambda) = \alpha + \beta&rsquo; \mathcal{x}$$</p>
<p>we can understand that the parameter $\alpha$ is the intercept, which is the expected number of sneezes when all the predictor variables are zero. The parameter $\beta&rsquo;$ is a vector of coefficients, which measure the effect of each predictor variable $\mathcal{x}$ on the number of sneezes. The log link function ensures that the rate parameter $\lambda$ is always positive and allows for multiplicative effects of the predictor variables on the response variable.</p>
<h3 id="define-the-model-with-turingjl">Define the model with <code>Turing.jl</code></h3>
<p>Now that we know how we are modeling our data, we use the package <code>Turing.jl</code> to define the model. <code>Turing.jl</code> is a tool that helps us write models in Julia and find the best parameters for them.</p>
<p>The model has two parts: the prior and the likelihood. The prior is what we think or guess about the parameters before we see the data. The likelihood is how likely the data is under the parameters. The parameters are the numbers that control the model, such as the rate of sneezes.</p>
<p>We use the Poisson distribution for the likelihood, because it is good for counting things, like sneezes. The Poisson distribution has one parameter, the rate of sneezes. The higher the rate, the more sneezes we expect.</p>
<p>We use any distribution for the prior, depending on how much we know about the parameters. If we know nothing, we use a flat prior, which does not favor any value. The prior affects the final answer, because it is our starting point.</p>
<p>We use Bayes’ theorem to combine the prior and the likelihood and get the final answer. The final answer is the posterior, which is what we believe about the parameters after we see the data. The posterior is the best fit for the model and the data.</p>
<p><strong>Let&rsquo;s crank up the Bayes!</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">poisson</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c"># define the priors</span>
</span></span><span class="line"><span class="cl">		<span class="n">alpha</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">alcohol</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">nomeds</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c"># alc_med ~ Normal(0,1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c"># define the likelihood</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	        <span class="n">log_lambda</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">alcohol</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">nomeds</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">	        <span class="n">lambda</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">log_lambda</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	        <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">lambda</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	    <span class="k">end</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="k">end</span>
</span></span></code></pre></div><pre><code>poisson (generic function with 2 methods)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># pass the data to the model function</span>
</span></span><span class="line"><span class="cl">	<span class="c"># pass the predictor data as a Matrix for efficiency</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">(</span><span class="kt">Matrix</span><span class="p">(</span><span class="n">sneeze_data</span><span class="p">[</span><span class="o">!</span><span class="p">,[</span><span class="ss">:alcohol</span><span class="p">,</span> <span class="ss">:nomeds</span><span class="p">]</span> <span class="p">]),</span> <span class="n">sneeze_data</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="ss">:n_sneezes</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># select the sampler</span>
</span></span><span class="line"><span class="cl"><span class="n">sampler</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># define the number of sampler</span>
</span></span><span class="line"><span class="cl"><span class="n">samples</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># set number of chains</span>
</span></span><span class="line"><span class="cl"><span class="n">num_chains</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="c"># crank up the Bayes!</span>
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="n">samples</span><span class="p">,</span> <span class="n">num_chains</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.00625
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.0125
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.00625
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.0125
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.0125
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.00625
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.00625
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.0125
[32mSampling (8 threads): 100%|█████████████████████████████| Time: 0:00:00[39m





Chains MCMC chain (1000×15×8 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 8
Samples per chain = 1000
Wall duration     = 13.66 seconds
Compute duration  = 100.67 seconds
parameters        = alpha, alcohol, nomeds
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat [0m [1m[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64 [0m [90m[0m ⋯

       alpha   -0.5025    0.0277    0.0005   2943.5608   2841.2874    1.0030   ⋯
     alcohol    1.7333    0.0186    0.0003   3801.1996   3652.2403    1.0022   ⋯
      nomeds    2.3348    0.0236    0.0004   2901.3750   3410.6453    1.0020   ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

       alpha   -0.5568   -0.5212   -0.5023   -0.4839   -0.4486
     alcohol    1.6974    1.7205    1.7331    1.7458    1.7698
      nomeds    2.2891    2.3189    2.3346    2.3506    2.3824
</code></pre>
<p><strong>NOTE:</strong> The above routine employs the MCMCThreads method to sample multiple chains. However, in order to implement this, one needs to change the environment variables for the number of threads Julia can use. These two threads might shed some light as to how to achieve this:</p>
<ol>
<li><a href="https://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading">https://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading</a></li>
<li><a href="https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794">https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794</a></li>
</ol>
<p>Of course, if you don&rsquo;t want to bother, then just change the last two functional lines in the cell above so that they read:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl">	<span class="c"># set number of chains - comment this out:</span>
</span></span><span class="line"><span class="cl">	<span class="c"># num_chains = 8</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="c"># crank up the Bayes! - delete MCMCThreads() and num_chains</span>
</span></span><span class="line"><span class="cl">	<span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="visualize-the-results">Visualize the results</h3>
<p>We can see above that we have obtained a sample pool of the posterior distribution of the parameters. This is what we were looking for. What this means is that now we have a posterior distribution (in the form of a sample pool), which we can also summarize with summary statistics.</p>
<p>Let&rsquo;s look at the diagnostics plots and the summary statistics.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240217_Bayesian_Poisson_Regression/output_13_0.svg" type="" alt="svg"  /></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">DataFrame</span><span class="p">(</span><span class="n">summarystats</span><span class="p">(</span><span class="n">chain</span><span class="p">))</span>
</span></span></code></pre></div><div><div style = "float: left;"><span>3×8 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">parameters</th><th style = "text-align: left;">mean</th><th style = "text-align: left;">std</th><th style = "text-align: left;">mcse</th><th style = "text-align: left;">ess_bulk</th><th style = "text-align: left;">ess_tail</th><th style = "text-align: left;">rhat</th><th style = "text-align: left;">ess_per_sec</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Symbol" style = "text-align: left;">Symbol</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: left;">alpha</td><td style = "text-align: right;">-0.502519</td><td style = "text-align: right;">0.0276553</td><td style = "text-align: right;">0.000511069</td><td style = "text-align: right;">2943.56</td><td style = "text-align: right;">2841.29</td><td style = "text-align: right;">1.00298</td><td style = "text-align: right;">29.2397</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: left;">alcohol</td><td style = "text-align: right;">1.7333</td><td style = "text-align: right;">0.0186097</td><td style = "text-align: right;">0.000301611</td><td style = "text-align: right;">3801.2</td><td style = "text-align: right;">3652.24</td><td style = "text-align: right;">1.00224</td><td style = "text-align: right;">37.759</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: left;">nomeds</td><td style = "text-align: right;">2.3348</td><td style = "text-align: right;">0.0236269</td><td style = "text-align: right;">0.000436385</td><td style = "text-align: right;">2901.38</td><td style = "text-align: right;">3410.65</td><td style = "text-align: right;">1.00197</td><td style = "text-align: right;">28.8207</td></tr></tbody></table></div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># taking the first chain</span>
</span></span><span class="line"><span class="cl"><span class="n">c1</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="o">:</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># Calculating the exponentiated means</span>
</span></span><span class="line"><span class="cl"><span class="n">b0_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">c1</span><span class="p">[</span><span class="ss">:alpha</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="n">b1_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">c1</span><span class="p">[</span><span class="ss">:alcohol</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="n">b2_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">c1</span><span class="p">[</span><span class="ss">:nomeds</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">println</span><span class="p">(</span><span class="s">&#34;The exponent of the mean of the weights (or coefficients) are: </span><span class="se">\n</span><span class="s">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">println</span><span class="p">(</span><span class="s">&#34;b0: &#34;</span><span class="p">,</span> <span class="n">b0_exp</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">println</span><span class="p">(</span><span class="s">&#34;b1: &#34;</span><span class="p">,</span> <span class="n">b1_exp</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">println</span><span class="p">(</span><span class="s">&#34;b2: &#34;</span><span class="p">,</span> <span class="n">b2_exp</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>The exponent of the mean of the weights (or coefficients) are: 

b0: 0.604415461752317
b1: 5.658573583760772
b2: 10.342642711232362
</code></pre>
<p>Notice how we are <strong>not</strong> recovering the original $\lambda$ values that were used to create this data set, i.e.:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl">	<span class="n">theta_noalc_nomed</span> <span class="o">=</span> <span class="mi">6</span>
</span></span><span class="line"><span class="cl">	<span class="n">theta_noalc_med</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">	<span class="n">theta_alc_nomed</span> <span class="o">=</span> <span class="mi">36</span>
</span></span><span class="line"><span class="cl">	<span class="n">theta_alc_med</span> <span class="o">=</span> <span class="mi">3</span>
</span></span></code></pre></div><p>Instead, we are recovering <em>the parameters of the linear function</em>, in other words, $\theta = {\alpha, \beta&rsquo;}$ in the linear relation:</p>
<p>$$\log(\lambda) = \alpha + \beta_1 x_{alc} + \beta_2 x_{meds}$$</p>
<p>where $x_{(\cdot)}$ represents the binary variable of whether the subject took alcohol/medicine or not.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This tutorial shows how to perform Bayesian inference on <em>discrete</em> data, e.g. the record of how many sneezes per day a group of people had, and classified according to their alcohol and medication consumption.</p>
<p>In real-world scenarios, we would obviously not know the parameter values, since this is precisely what we want to find out by incorporating whatever we knew about them into what we observed.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Bayesian Logistic Regression with Julia and Turing.jl</title>
      <link>http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/</link>
      <pubDate>Tue, 09 Jan 2024 11:57:07 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240109_bayesian-logistic-regression/20240109_bayesian-logistic-regression/</guid>
      <description>Applying Turing.jl package in Julia for a probabilistic approach to a classification problem on a real-world dataset.</description>
      <content:encoded><![CDATA[<hr>
<h2 id="problem-statement">Problem Statement</h2>
<p>You are interested in studying the factors that influence the likelihood of heart disease among patients.</p>
<p>You have a dataset of 303 patients, each with 14 variables: age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise induced angina, oldpeak, slope, number of major vessels, thalassemia, and diagnosis of heart disease.</p>
<p>You want to use Bayesian logistic regression to model the probability of heart disease (the outcome variable) as a function of some or all of the other variables (the predictor variables).</p>
<p>You also want to compare different models and assess their fit and predictive performance.</p>
<h2 id="bayesian-workflow">Bayesian Workflow</h2>
<p>For this project, I will try to follow this workflow:</p>
<ol>
<li>
<p>Data exploration: Explore the data using descriptive statistics and visualizations to get a sense of the distribution, range, and correlation of the variables. Identify any outliers, missing values, or potential errors in the data. Transform or standardize the variables if needed.</p>
</li>
<li>
<p>Model specification: Specify a probabilistic model that relates the outcome variable to the predictor variables using a logistic regression equation. Choose appropriate priors for the model parameters, such as normal, student-t, or Cauchy distributions. You can use the <code>brms</code> package in Julia to define and fit Bayesian models using a formula syntax similar to <code>lme4</code>. However, try to use <code>Turing.jl</code></p>
</li>
<li>
<p>Model fitting: Fit the model using a sampling algorithm such as Hamiltonian Monte Carlo (HMC) or No-U-Turn Sampler (NUTS). You can use the <code>DynamicHMC</code> or <code>Turing.jl</code> package in Julia to implement these algorithms. Check the convergence and mixing of the chains using diagnostics such as trace plots, autocorrelation plots, effective sample size, and potential scale reduction factor. You can use the <code>MCMCDiagnostics</code> or the included diagnostics features in <code>Turing.jl</code> package in Julia to compute these diagnostics.</p>
</li>
<li>
<p>Model checking: Check the fit and validity of the model using posterior predictive checks, residual analysis, and sensitivity analysis. You can use the <code>PPCheck</code> package in Julia to perform posterior predictive checks, which compare the observed data to data simulated from the posterior predictive distribution. You can use the <code>BayesianRidgeRegression</code> package in Julia to perform residual analysis, which plots the residuals against the fitted values and the predictor variables. You can use the <code>Sensitivity</code> package in Julia to perform sensitivity analysis, which measures how the posterior distribution changes with respect to the prior distribution or the likelihood function.</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># import packages</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">CSV</span><span class="p">,</span> <span class="n">Turing</span><span class="p">,</span> <span class="n">DataFrames</span><span class="p">,</span> <span class="n">StatsPlots</span><span class="p">,</span> <span class="n">LaTeXStrings</span><span class="p">,</span> <span class="n">Distributions</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Images</span><span class="p">,</span> <span class="n">ImageIO</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Random</span><span class="o">:</span> <span class="n">seed!</span>
</span></span><span class="line"><span class="cl"><span class="n">seed!</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Random.TaskLocalRNG()
</code></pre>
<h2 id="data-exploration">Data Exploration</h2>
<p>After &ldquo;collecting&rdquo; the data, we may import it and arrange it so we can use it further.</p>
<p>The data set can be found in this <a href="https://www.kaggle.com/datasets/aavigan/cleveland-clinic-heart-disease-dataset">Kaggle link</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s">&#34;data/processed_cleveland.csv&#34;</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">map!</span><span class="p">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">x</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">?</span> <span class="mi">1</span> <span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">num</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">num</span><span class="p">);</span> <span class="c"># make the outcome binary</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span>
</span></span></code></pre></div><div><div style = "float: left;"><span>303×14 DataFrame</span></div><div style = "float: right;"><span style = "font-style: italic;">278 rows omitted</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">age</th><th style = "text-align: left;">sex</th><th style = "text-align: left;">cp</th><th style = "text-align: left;">trestbps</th><th style = "text-align: left;">chol</th><th style = "text-align: left;">fbs</th><th style = "text-align: left;">restecg</th><th style = "text-align: left;">thalach</th><th style = "text-align: left;">exang</th><th style = "text-align: left;">oldpeak</th><th style = "text-align: left;">slope</th><th style = "text-align: left;">ca</th><th style = "text-align: left;">thal</th><th style = "text-align: left;">num</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "String1" style = "text-align: left;">String1</th><th title = "String1" style = "text-align: left;">String1</th><th title = "Int64" style = "text-align: left;">Int64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">63</td><td style = "text-align: right;">1</td><td style = "text-align: right;">1</td><td style = "text-align: right;">145</td><td style = "text-align: right;">233</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2</td><td style = "text-align: right;">150</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2.3</td><td style = "text-align: right;">3</td><td style = "text-align: left;">0</td><td style = "text-align: left;">6</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">67</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">160</td><td style = "text-align: right;">286</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">108</td><td style = "text-align: right;">1</td><td style = "text-align: right;">1.5</td><td style = "text-align: right;">2</td><td style = "text-align: left;">3</td><td style = "text-align: left;">3</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">67</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">120</td><td style = "text-align: right;">229</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">129</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2.6</td><td style = "text-align: right;">2</td><td style = "text-align: left;">2</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">37</td><td style = "text-align: right;">1</td><td style = "text-align: right;">3</td><td style = "text-align: right;">130</td><td style = "text-align: right;">250</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">187</td><td style = "text-align: right;">0</td><td style = "text-align: right;">3.5</td><td style = "text-align: right;">3</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">41</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">130</td><td style = "text-align: right;">204</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">172</td><td style = "text-align: right;">0</td><td style = "text-align: right;">1.4</td><td style = "text-align: right;">1</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">6</td><td style = "text-align: right;">56</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2</td><td style = "text-align: right;">120</td><td style = "text-align: right;">236</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">178</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.8</td><td style = "text-align: right;">1</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">7</td><td style = "text-align: right;">62</td><td style = "text-align: right;">0</td><td style = "text-align: right;">4</td><td style = "text-align: right;">140</td><td style = "text-align: right;">268</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">160</td><td style = "text-align: right;">0</td><td style = "text-align: right;">3.6</td><td style = "text-align: right;">3</td><td style = "text-align: left;">2</td><td style = "text-align: left;">3</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">8</td><td style = "text-align: right;">57</td><td style = "text-align: right;">0</td><td style = "text-align: right;">4</td><td style = "text-align: right;">120</td><td style = "text-align: right;">354</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">163</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0.6</td><td style = "text-align: right;">1</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">9</td><td style = "text-align: right;">63</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">130</td><td style = "text-align: right;">254</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">147</td><td style = "text-align: right;">0</td><td style = "text-align: right;">1.4</td><td style = "text-align: right;">2</td><td style = "text-align: left;">1</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">10</td><td style = "text-align: right;">53</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">140</td><td style = "text-align: right;">203</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2</td><td style = "text-align: right;">155</td><td style = "text-align: right;">1</td><td style = "text-align: right;">3.1</td><td style = "text-align: right;">3</td><td style = "text-align: left;">0</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">11</td><td style = "text-align: right;">57</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">140</td><td style = "text-align: right;">192</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">148</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.4</td><td style = "text-align: right;">2</td><td style = "text-align: left;">0</td><td style = "text-align: left;">6</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">12</td><td style = "text-align: right;">56</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">140</td><td style = "text-align: right;">294</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">153</td><td style = "text-align: right;">0</td><td style = "text-align: right;">1.3</td><td style = "text-align: right;">2</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">13</td><td style = "text-align: right;">56</td><td style = "text-align: right;">1</td><td style = "text-align: right;">3</td><td style = "text-align: right;">130</td><td style = "text-align: right;">256</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2</td><td style = "text-align: right;">142</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0.6</td><td style = "text-align: right;">2</td><td style = "text-align: left;">1</td><td style = "text-align: left;">6</td><td style = "text-align: right;">1</td></tr><tr><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td><td style = "text-align: right;">&vellip;</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">292</td><td style = "text-align: right;">55</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">132</td><td style = "text-align: right;">342</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">166</td><td style = "text-align: right;">0</td><td style = "text-align: right;">1.2</td><td style = "text-align: right;">1</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">293</td><td style = "text-align: right;">44</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">120</td><td style = "text-align: right;">169</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">144</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2.8</td><td style = "text-align: right;">3</td><td style = "text-align: left;">0</td><td style = "text-align: left;">6</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">294</td><td style = "text-align: right;">63</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">140</td><td style = "text-align: right;">187</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">144</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4.0</td><td style = "text-align: right;">1</td><td style = "text-align: left;">2</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">295</td><td style = "text-align: right;">63</td><td style = "text-align: right;">0</td><td style = "text-align: right;">4</td><td style = "text-align: right;">124</td><td style = "text-align: right;">197</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">136</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">2</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">296</td><td style = "text-align: right;">41</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2</td><td style = "text-align: right;">120</td><td style = "text-align: right;">157</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">182</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">1</td><td style = "text-align: left;">0</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">297</td><td style = "text-align: right;">59</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">164</td><td style = "text-align: right;">176</td><td style = "text-align: right;">1</td><td style = "text-align: right;">2</td><td style = "text-align: right;">90</td><td style = "text-align: right;">0</td><td style = "text-align: right;">1.0</td><td style = "text-align: right;">2</td><td style = "text-align: left;">2</td><td style = "text-align: left;">6</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">298</td><td style = "text-align: right;">57</td><td style = "text-align: right;">0</td><td style = "text-align: right;">4</td><td style = "text-align: right;">140</td><td style = "text-align: right;">241</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">123</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0.2</td><td style = "text-align: right;">2</td><td style = "text-align: left;">0</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">299</td><td style = "text-align: right;">45</td><td style = "text-align: right;">1</td><td style = "text-align: right;">1</td><td style = "text-align: right;">110</td><td style = "text-align: right;">264</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">132</td><td style = "text-align: right;">0</td><td style = "text-align: right;">1.2</td><td style = "text-align: right;">2</td><td style = "text-align: left;">0</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">300</td><td style = "text-align: right;">68</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">144</td><td style = "text-align: right;">193</td><td style = "text-align: right;">1</td><td style = "text-align: right;">0</td><td style = "text-align: right;">141</td><td style = "text-align: right;">0</td><td style = "text-align: right;">3.4</td><td style = "text-align: right;">2</td><td style = "text-align: left;">2</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">301</td><td style = "text-align: right;">57</td><td style = "text-align: right;">1</td><td style = "text-align: right;">4</td><td style = "text-align: right;">130</td><td style = "text-align: right;">131</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">115</td><td style = "text-align: right;">1</td><td style = "text-align: right;">1.2</td><td style = "text-align: right;">2</td><td style = "text-align: left;">1</td><td style = "text-align: left;">7</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">302</td><td style = "text-align: right;">57</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">130</td><td style = "text-align: right;">236</td><td style = "text-align: right;">0</td><td style = "text-align: right;">2</td><td style = "text-align: right;">174</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">2</td><td style = "text-align: left;">1</td><td style = "text-align: left;">3</td><td style = "text-align: right;">1</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">303</td><td style = "text-align: right;">38</td><td style = "text-align: right;">1</td><td style = "text-align: right;">3</td><td style = "text-align: right;">138</td><td style = "text-align: right;">175</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0</td><td style = "text-align: right;">173</td><td style = "text-align: right;">0</td><td style = "text-align: right;">0.0</td><td style = "text-align: right;">1</td><td style = "text-align: left;">?</td><td style = "text-align: left;">3</td><td style = "text-align: right;">0</td></tr></tbody></table></div>
<p>In the above data frame, the attributes are as follows:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Variable Name</th>
          <th style="text-align: center">Role</th>
          <th style="text-align: center">Type</th>
          <th style="text-align: center">Demographic</th>
          <th style="text-align: center">Description</th>
          <th style="text-align: center">Units</th>
          <th style="text-align: center">Missing Values</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">age</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Integer</td>
          <td style="text-align: center">Age</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">years</td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">sex</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Categorical</td>
          <td style="text-align: center">Sex</td>
          <td style="text-align: center"></td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">cp</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Categorical</td>
          <td style="text-align: center"></td>
          <td style="text-align: center"></td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">trestbps</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Integer</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">resting blood pressure (on admission to the hospital)</td>
          <td style="text-align: center">mm Hg</td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">chol</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Integer</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">serum cholestoral</td>
          <td style="text-align: center">mg/dl</td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">fbs</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Categorical</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">fasting blood sugar &gt; 120 mg/dl</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">restecg</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Categorical</td>
          <td style="text-align: center"></td>
          <td style="text-align: center"></td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">thalach</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Integer</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">maximum heart rate achieved</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">exang</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Categorical</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">exercise induced angina</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td style="text-align: center">oldpeak</td>
          <td style="text-align: center">Feature</td>
          <td style="text-align: center">Integer</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">ST depression induced by exercise relative to rest</td>
          <td style="text-align: center"></td>
          <td style="text-align: center">no</td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>Complete attribute documentation:</p>
<pre><code>1. age: age in years
2. sex: sex (1 = male; 0 = female)
3. cp: chest pain type
	- Value 1: typical angina
	- Value 2: atypical angina
	- Value 3: non-anginal pain
	- Value 4: asymptomatic
4. trestbps: resting blood pressure (in mm Hg on admission to the
hospital)
5. chol: serum cholestoral in mg/dl
6.fbs: fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false)
7. restecg: resting electrocardiographic results
	- Value 0: normal
	- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)
	- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
8. thalach: maximum heart rate achieved
9. exang: exercise induced angina (1 = yes; 0 = no)
10. oldpeak: ST depression induced by exercise relative to rest
11. slope: the slope of the peak exercise ST segment
	- Value 1: upsloping
	- Value 2: flat
	- Value 3: downsloping
12. ca: number of major vessels (0-3) colored by flourosopy (for calcification of vessels)
13. thal: results of nuclear stress test (3 = normal; 6 = fixed defect; 7 = reversable defect)
14. num: target variable representing diagnosis of heart disease (angiographic disease status) in any major vessel
	- Value 0: &lt; 50% diameter narrowing
	- Value 1: &gt; 50% diameter narrowing
</code></pre>
<h2 id="data-interpretation">Data Interpretation</h2>
<p>After collecting the data, it has been imported as a Data Frame. Now, to understand what we will do with this exercise, we need to analyze the data by means of Bayesian Logistic Regression.</p>
<p>With this type of analysis, we can make predictions on (typically) binary outcomes, based on a set of parameters. In this particular case, we are interested in predicting whether a patient will have heart disease based on a set of parameters such as age, chest pain, blood pressure, etc.</p>
<p>In terms of the data available, we have a set of 303 observations (303 patients) whose symptoms and circumstances have been recorded, and the <strong>outcome</strong> is the heart disease diagnosis. To simplify things, this data set has a binary outcome, i.e. heart disease <em>present/not present</em>.</p>
<p>Additionally, this study is divided in two parts: first, I will set up the logistic regression model to include only one predictor, i.e., <strong>age</strong>. Afterwards, an analysis will be performed including two or more predictors.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># find the range for the age, to set the plot limits below</span>
</span></span><span class="line"><span class="cl"><span class="c"># min_age = minimum(df.age)</span>
</span></span><span class="line"><span class="cl"><span class="n">min_age</span> <span class="o">=</span> <span class="mi">15</span> 
</span></span><span class="line"><span class="cl"><span class="n">max_age</span> <span class="o">=</span> <span class="mi">85</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># visualize data</span>
</span></span><span class="line"><span class="cl"><span class="n">p_data</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">num</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">xlims</span> <span class="o">=</span> <span class="p">(</span><span class="n">min_age</span><span class="p">,</span> <span class="n">max_age</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">	<span class="n">color</span> <span class="o">=</span> <span class="ss">:red</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">markersize</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Probability of Heart Disease&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">xlabel</span> <span class="o">=</span> <span class="s">&#34;Age (years)&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">ylabel</span> <span class="o">=</span> <span class="s">&#34;Probability of Heart Disease&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">widen</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">dpi</span> <span class="o">=</span> <span class="mi">150</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240109_Bayesian_Logistic_Regression/output_7_0.svg" type="" alt="svg"  /></p>
<h2 id="model-specification">Model Specification</h2>
<p>In this stage of the workflow, we will specify the Bayesian model and then use <code>Turing.jl</code> to program it in Julia.</p>
<p>The model I will use for this analysis is a Bayesian Logistic Regression model, which relates the probability of heart disease to a <em>linear combination of the predictor variables</em>, using a logistic function. The model can be written as:</p>
<p>$$\begin{aligned}
y_i &amp;\sim Bernoulli(p_i) \\
p_i &amp;= \frac{1}{1+e^{-\eta_i}} \\
\eta_i &amp;= \alpha + {\beta_1 x_{i,1}} + {\beta_2 x_{i,2}} + \ldots + {\beta_{13} x_{i,13}} \\
\alpha &amp;\sim \mathcal{N}(\mu_\alpha,\sigma_\sigma) \\
\beta_j &amp;\sim \mathcal{N}(\mu_{\beta},\sigma_{\beta}) \\
\end{aligned}$$</p>
<p>where $y_i$ is the outcome for the <em>i-th</em> patient, $p_i$ is the probability of heart disease for the <em>i-th</em> patient, $\eta_i$ is the linear predictor for the <em>i-th</em> patient, $\alpha$ and $\beta_j$ are the intercept and coefficient for the <em>j-th</em> predictor variable, respectively, and $x_{ij}$ is the value of the <em>j-th</em> predictor variable for the <em>i-th</em> patient.</p>
<p>The assumptions that I am making are:</p>
<ol>
<li>The outcome variable follows a Bernoulli distribution, i.e. $y_i \sim Bernoulli(p_i)$, which is appropriate for binary outcomes</li>
<li>The predictor variables are linearly related to the log-odds of the outcome variable, i.e. $\log(\frac{p}{1-p})$ which is a common assumption for logistic regression models</li>
<li>The prior distributions for the model parameters are uniform, which are weakly informative and reflect my prior beliefs about the plausible range of the parameters</li>
</ol>
<p>Regarding point (2):</p>
<p>That statement means that the log-odds of the outcome variable (the log of the odds ratio) can be expressed as a linear function of the predictor variables. Mathematically, this can be written as:</p>
<p>$$\log(\frac{p}{1-p}) = \alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k$$</p>
<p>where $p$ is the probability of the outcome variable being 1, $x_1, x_2, \ldots, x_k$ are the predictor variables, and $\alpha, \beta_1, \beta_2, \ldots, \beta_k$ are the coefficients (parameters).</p>
<p>This assumption implies that the effect of each predictor variable on the log-odds of the outcome variable is contant, regardless of the values of the other predictor variables. It also implies that the relationship between the predictor variables and the probability of the outcome variable is non-linear, as the probability is obtained by applying the inverse of the log-odds function, which is the logistic function:</p>
<p>$$p = \frac{1}{1+e^{-(\alpha + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k)}}$$</p>
<p>The logistic function is an S-shaped curve that maps any real number to a value between 0 and 1. It has the property that as the linear predictor increases, the probability approaches 1, and as the linear predictor decreases, the probability approaches 0.</p>
<h3 id="model-specification-using-turingjl">Model Specification Using <code>Turing.jl</code></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># define the Bayesian model</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">logit_model</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">disease</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c"># priors</span>
</span></span><span class="line"><span class="cl">	<span class="n">α</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">β</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c"># likelihood</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">η</span> <span class="o">=</span> <span class="n">α</span> <span class="o">.+</span> <span class="n">β</span><span class="o">.*</span><span class="n">predictors</span>
</span></span><span class="line"><span class="cl">	<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">./</span> <span class="p">(</span><span class="mi">1</span> <span class="o">.+</span> <span class="n">exp</span><span class="o">.</span><span class="p">(</span><span class="o">-</span><span class="n">η</span><span class="p">))</span>     <span class="c"># remember to include the &#34;.&#34;!</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">eachindex</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">disease</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">	<span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>logit_model (generic function with 2 methods)
</code></pre>
<h4 id="crank-up-the-bayes">Crank up the Bayes!</h4>
<p>Run the model using <code>sample(model, sampler, opt_argument, samples, chains)</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># infer posterior probability</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">logit_model</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">num</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sampler</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">samples</span> <span class="o">=</span> <span class="mi">1_000</span>
</span></span><span class="line"><span class="cl"><span class="n">num_chains</span> <span class="o">=</span> <span class="mi">8</span> 		<span class="c"># set the number of chains</span>
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="n">samples</span><span class="p">,</span> <span class="n">num_chains</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.025
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.0125
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.025
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.0125
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.025
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.05
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.025
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.025
[32mSampling (8 threads): 100%|█████████████████████████████| Time: 0:00:01[39m





Chains MCMC chain (1000×14×8 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 8
Samples per chain = 1000
Wall duration     = 13.18 seconds
Compute duration  = 100.1 seconds
parameters        = α, β
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat [0m [1m[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64 [0m [90m[0m ⋯

           α   -3.0326    0.7453    0.0210   1242.6057   1246.3034    1.0043   ⋯
           β    0.0524    0.0134    0.0004   1224.4182   1259.7727    1.0037   ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

           α   -4.4814   -3.5432   -3.0127   -2.5183   -1.5973
           β    0.0264    0.0432    0.0521    0.0617    0.0789
</code></pre>
<p><strong>NOTE</strong>: The above routine employs the <code>MCMCThreads()</code> method to sample multiple chains. However, to implement this, one needs to change the environment variables for the number of threads Julia can use. These two discussions might shed some light as to how to achieve this:</p>
<ol>
<li><a href="https://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading">https://docs.julialang.org/en/v1/manual/multi-threading/#man-multithreading</a></li>
<li><a href="https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794">https://discourse.julialang.org/t/julia-num-threads-in-vs-code-windows-10-wsl/28794</a></li>
</ol>
<p>Of course, if you don&rsquo;t want to bother, then just change the last two functional lines in the cell above so that they read:</p>
<pre><code># set number of chains - comment this out:
# num_chains = 8

# crank up the Bayes! - delete MCMCThreads() and num_chains
chain = sample(model, sampler, samples)
</code></pre>
<h4 id="plot-the-mcmc-diagnostics">Plot the MCMC Diagnostics</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">150</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240109_Bayesian_Logistic_Regression/output_15_0.svg" type="" alt="png"  /></p>
<h4 id="get-the-summary-statistics">Get the Summary Statistics</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">summarystats</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m  ess_tail [0m [1m    rhat [0m [1m[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m   Float64 [0m [90m Float64 [0m [90m[0m ⋯

           α   -3.0326    0.7453    0.0210   1242.6057   1246.3034    1.0043   ⋯
           β    0.0524    0.0134    0.0004   1224.4182   1259.7727    1.0037   ⋯
[36m                                                                1 column omitted[0m
</code></pre>
<h3 id="plot-and-interpret-the-results">Plot and Interpret the Results</h3>
<p>Ok, how do we interpret the results from a Bayesian approach? Let&rsquo;s start by plotting the results. This will help us understand not only the results, but really grasp the power of a Bayesian model in action.</p>
<p>From a frequentist or a machine learning approach, we would expect to find a function that models the data the best possible way, i.e. fit a model. If we were to visualize it, we would see one single sigmoid curve trying its best to explain the data.</p>
<p>How about this chart here, though? This chart is a collection of possible outcomes given that the <em>parameters</em> $\alpha$ and $\beta$ in this case, are modeled as random variables with some probability distribution. Therefore, there is an uncertainty associated with them. This uncertainty is naturally <em>propagated</em> onto the sigmoid function. Therefore, there is also an uncertainty associated with that sigmoid curve that we are trying to model.</p>
<p>Again, below we can see a collection of possible outcomes given the parameter sample space. There is a darker region where most sigmoid functions turned out, and these tend to be the most probable sigmoid functions, or, in other words, these sigmoid functions are the most probable functions that could fit the data, considering the distributions of the parameters too!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="kt">Int</span><span class="p">(</span><span class="n">samples</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>100
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">x_line</span> <span class="o">=</span> <span class="mi">15</span><span class="o">:</span><span class="mi">1</span><span class="o">:</span><span class="n">max_age</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">samples</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="n">m</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="n">line</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span><span class="n">b</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">line</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">plot!</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">x_line</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    	<span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span> <span class="ss">:blue</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">150</span>
</span></span><span class="line"><span class="cl">	<span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p_data</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240109_Bayesian_Logistic_Regression/output_20_0.svg" type="" alt="png"  /></p>
<h3 id="making-predictions">Making Predictions</h3>
<p>So why go through all this trouble, you might be asking. Well, one of the reasons we want to use probabilistic models is, first, to make predictions. But I would go further than that: these models are useful when making informed decisions. Let&rsquo;s try this out.</p>
<p>Let&rsquo;s make predictions for different arbitrary ages (50, 60, 70, 80, 20):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">new_Age</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">p_disease</span> <span class="o">=</span> <span class="n">fill</span><span class="p">(</span><span class="nb">missing</span><span class="p">,</span> <span class="n">length</span><span class="p">(</span><span class="n">new_Age</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">logit_model</span><span class="p">(</span><span class="n">new_Age</span><span class="p">,</span> <span class="n">p_disease</span><span class="p">),</span> <span class="n">chain</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">summarystats</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m [0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m [0m ⋯

  disease[1]    0.3855    0.4867    0.0055   7711.3762        NaN    0.9998    ⋯
  disease[2]    0.5258    0.4994    0.0055   8284.1301        NaN    0.9998    ⋯
  disease[3]    0.6432    0.4791    0.0056   7441.4457        NaN    1.0002    ⋯
  disease[4]    0.7555    0.4298    0.0050   7352.4368        NaN    0.9998    ⋯
  disease[5]    0.1224    0.3277    0.0039   7016.6404        NaN    1.0004    ⋯
[36m                                                                1 column omitted[0m
</code></pre>
<h4 id="interpreting-the-predictions">Interpreting the predictions</h4>
<p>The last operations make predictions of heart diseased based <em>on age only</em>. What the predictions mean is that, given the data, the probability distribution of an individual of age 50 to have heart disease has a mean of 0.379, and a standard deviation of 0.485 (this is highly uncertain, by the way).</p>
<p>Similarly, a 20-year-old individual has a probability with a mean of 0.13 and standard deviation of 0.336 of having heart disease.</p>
<p>These statistics are extremely powerful when you are trying to make decisions, such as when diagnosing Heart Disease. It stands to reason that, if you were a physician, you want to know what your model says might be wrong (or not) with your patient, but you also want to know how much you can trust that prediction.</p>
<p>If your model classifies Patient X as having heart disease, you would probably want to know how sure you are of this. And this certainty comes partially from&hellip; you guessed it: your priors <em>and</em> the data.</p>
<p>In the plot below, we can see the where the predictions lie. Note that these probabilities are on a continuum given by the sigmoid function. But we want our final decision to be a yes or a no. To do that, we need to set a decision threshold.</p>
<p>We will do that at the end of the next section.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">new_Age</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pred_mean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">pred_plot</span> <span class="o">=</span> <span class="n">scatter!</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="p">(</span><span class="n">new_Age</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">pred_mean</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p_data</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240109_Bayesian_Logistic_Regression/output_24_0.svg" type="" alt="png"  /></p>
<h2 id="model-specification-using-multiple-predictors">Model Specification Using Multiple Predictors</h2>
<h3 id="some-data-cleaning">Some Data Cleaning</h3>
<p>In this part, I am using the <code>Turing.jl</code> documentation tutorial found in <a href="https://turinglang.org/dev/tutorials/02-logistic-regression/">https://turinglang.org/dev/tutorials/02-logistic-regression/</a>.</p>
<p>In the tutorial, they quite rightly incorporate a train/test split, and data normalization, which is the recommended practice. I didn&rsquo;t do it in the first part of this tutorial to keep things simple!</p>
<p>Here is how they handle the split and the data normalization using <code>MLUtils</code>.</p>
<pre><code>function split_data(df, target; at=0.70)
    shuffled = shuffleobs(df)
    return trainset, testset = stratifiedobs(row -&gt; row[target], shuffled; p=at)
end

features = [:StudentNum, :Balance, :Income]
numerics = [:Balance, :Income]
target = :DefaultNum

trainset, testset = split_data(data, target; at=0.05)
for feature in numerics
    μ, σ = rescale!(trainset[!, feature]; obsdim=1)
    rescale!(testset[!, feature], μ, σ; obsdim=1)
end

# Turing requires data in matrix form, not dataframe
train = Matrix(trainset[:, features])
test = Matrix(testset[:, features])
train_label = trainset[:, target]
test_label = testset[:, target];
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">MLDataUtils</span><span class="o">:</span> <span class="n">shuffleobs</span><span class="p">,</span> <span class="n">stratifiedobs</span><span class="p">,</span> <span class="n">rescale!</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">StatsFuns</span> <span class="c"># we introduce this package so we can later call the </span>
</span></span><span class="line"><span class="cl">                <span class="c"># logistic function directly instead of defining it manually as before</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">split_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="p">;</span> <span class="n">at</span><span class="o">=</span><span class="mf">0.70</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">shuffled</span> <span class="o">=</span> <span class="n">shuffleobs</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">stratifiedobs</span><span class="p">(</span><span class="n">row</span> <span class="o">-&gt;</span> <span class="n">row</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">shuffled</span><span class="p">;</span> <span class="n">p</span><span class="o">=</span><span class="n">at</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="ss">:age</span><span class="p">,</span> <span class="ss">:cp</span><span class="p">,</span> <span class="ss">:chol</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="ss">:num</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">split_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="p">;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># convert the feature columns to float64 to ensure compatibility with rescale!</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">feature</span> <span class="k">in</span> <span class="n">features</span>
</span></span><span class="line"><span class="cl">    <span class="n">df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">float</span><span class="o">.</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">feature</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">feature</span> <span class="k">in</span> <span class="n">features</span>
</span></span><span class="line"><span class="cl">    <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span> <span class="o">=</span> <span class="n">rescale!</span><span class="p">(</span><span class="n">trainset</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">feature</span><span class="p">];</span> <span class="n">obsdim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">rescale!</span><span class="p">(</span><span class="n">testset</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">feature</span><span class="p">],</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">;</span> <span class="n">obsdim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="c"># Turing requires data in matrix form, not dataframe</span>
</span></span><span class="line"><span class="cl"><span class="n">train</span> <span class="o">=</span> <span class="kt">Matrix</span><span class="p">(</span><span class="n">trainset</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">features</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">test</span> <span class="o">=</span> <span class="kt">Matrix</span><span class="p">(</span><span class="n">testset</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">features</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">train_label</span> <span class="o">=</span> <span class="n">trainset</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">target</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_label</span> <span class="o">=</span> <span class="n">testset</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="n">target</span><span class="p">];</span>
</span></span></code></pre></div><h3 id="inference">Inference</h3>
<p>Now that our data is formatted, we can perform our Bayesian logistic regression with multiple predictors: using chest pain (cp), age (age), resting bloodpressure (tresttbps) and cholesterol (chol) levels.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">logreg_multi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c"># priors</span>
</span></span><span class="line"><span class="cl">	<span class="n">intercept</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">age</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">cp</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">chol</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">n</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">size</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
</span></span><span class="line"><span class="cl">		<span class="c"># call the logistic function directly, instead of manually</span>
</span></span><span class="line"><span class="cl">		<span class="n">v</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">intercept</span> <span class="o">+</span> <span class="n">age</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">cp</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">chol</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>logreg_multi (generic function with 2 methods)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">train</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">train_label</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">println</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">train</span><span class="p">),</span> <span class="n">size</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
</span></span></code></pre></div><pre><code>(212, 3)(91, 3)
</code></pre>
<p>Now we build the model and create the chain:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">model_multi</span> <span class="o">=</span> <span class="n">logreg_multi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">chain_multi</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model_multi</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="mi">2_000</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="c"># select 2000 samples directly</span>
</span></span></code></pre></div><pre><code>[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 1.6
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.8
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.8
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.8
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.8
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 1.6
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 0.8
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1m└ [22m[39m  ϵ = 1.6





Chains MCMC chain (2000×16×8 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 8
Samples per chain = 2000
Wall duration     = 11.32 seconds
Compute duration  = 87.27 seconds
parameters        = intercept, age, cp, chol
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m   ess_bulk [0m [1m   ess_tail [0m [1m    rhat[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m    Float64 [0m [90m    Float64 [0m [90m Float64[0m ⋯

   intercept   -0.2821    0.1647    0.0012   20113.9419   13042.8456    1.0003 ⋯
         age    0.6003    0.1760    0.0013   18327.5449   12926.7418    1.0001 ⋯
          cp    1.0699    0.1922    0.0014   19583.1899   13534.2405    0.9999 ⋯
        chol   -0.0073    0.1641    0.0012   18280.4944   12242.8280    1.0004 ⋯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m    2.5% [0m [1m   25.0% [0m [1m   50.0% [0m [1m   75.0% [0m [1m   97.5% [0m
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m

   intercept   -0.6118   -0.3923   -0.2817   -0.1711    0.0388
         age    0.2645    0.4792    0.5964    0.7178    0.9575
          cp    0.7106    0.9372    1.0636    1.1963    1.4603
        chol   -0.3283   -0.1177   -0.0080    0.1025    0.3151
</code></pre>
<h3 id="plot-the-mcmc-diagnostics-1">Plot the MCMC Diagnostics</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain_multi</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240109_Bayesian_Logistic_Regression/output_34_0.svg" type="" alt="png"  /></p>
<h3 id="summary-statistics">Summary Statistics</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">summarystats</span><span class="p">(</span><span class="n">chain_multi</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Summary Statistics
 [1m parameters [0m [1m    mean [0m [1m     std [0m [1m    mcse [0m [1m   ess_bulk [0m [1m   ess_tail [0m [1m    rhat[0m ⋯
 [90m     Symbol [0m [90m Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m    Float64 [0m [90m    Float64 [0m [90m Float64[0m ⋯

   intercept   -0.2821    0.1647    0.0012   20113.9419   13042.8456    1.0003 ⋯
         age    0.6003    0.1760    0.0013   18327.5449   12926.7418    1.0001 ⋯
          cp    1.0699    0.1922    0.0014   19583.1899   13534.2405    0.9999 ⋯
        chol   -0.0073    0.1641    0.0012   18280.4944   12242.8280    1.0004 ⋯
[36m                                                                1 column omitted[0m
</code></pre>
<h2 id="thank-you">Thank you!</h2>
<p>And that concludes this little tutorial showcasing the power of a Bayesian model and the fun of using Julia. Thank you for stopping by!</p>
<p>Victor Flores</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
