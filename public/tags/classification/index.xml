<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Classification on Victor Flores, PhD</title>
    <link>http://localhost:1313/tags/classification/</link>
    <description>Recent content in Classification on Victor Flores, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 24 Sep 2024 14:12:51 +0800</lastBuildDate><atom:link href="http://localhost:1313/tags/classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bayesian Classification and Survival Analysis with NumPyro</title>
      <link>http://localhost:1313/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/</link>
      <pubDate>Tue, 24 Sep 2024 14:12:51 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240924_numpyro_logreg_surv_analysis/np01_logreg_surv_analysis/</guid>
      <description>Applying Bayesian Classification and Survival Analysis to the Cirrhosis Patient Dataset.</description>
      <content:encoded><![CDATA[<p><a href="https://colab.research.google.com/github/vflores-io/Portfolio/blob/main/Bayesian%20Methods%20Tutorials/Python/NumPyro/NP01_LogReg_Surv_Analysis/NP01_LogReg_Surv_Analysis.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<hr>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/intro.png" type="" alt="image"  /></p>
<h1 id="1-introduction">1. Introduction</h1>
<p>Cirrhosis is a progressive liver disease characterized by the replacement of healthy liver tissue with scar tissue, leading to impaired liver function. Early prediction of patient survival can significantly impact treatment decisions and improve outcomes. In this project, we employ Bayesian statistical methods to analyze and predict the survival of patients with cirrhosis using a publicly available dataset from the UCI ML Dataset Repository.</p>
<ol>
<li>
<p><strong>Bayesian Classification (Logistic Regression):</strong> We develop a Bayesian logistic regression model to predict the survival status of patients based on various clinical features. This probabilistic approach allows us to incorporate prior knowledge and quantify uncertainty in our predictions.</p>
</li>
<li>
<p><strong>Bayesian Survival Analysis:</strong> We perform a comprehensive survival analysis using Bayesian methods. We start with a basic Weibull model without covariates to understand the baseline survival function. We then introduce covariates to the Weibull model, and despite encountering challenges with this approach, we proceed to implement a log-normal model with covariates, which demonstrates improved performance. Finally, we refine the Weibull model by including selected covariates and accounting for censored data to enhance the model&rsquo;s applicability to real-world scenarios.</p>
</li>
</ol>
<p>Throughout this project, we emphasize the iterative nature of model development in Bayesian statistics and showcase how to handle practical issues that may arise during analysis.</p>
<h2 id="11-importing-packages">1.1 Importing Packages</h2>
<p>To begin our analysis, we first import the necessary Python libraries for data manipulation, visualization, and Bayesian modeling.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">sys</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s1">&#39;numpyro&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
</span></span><span class="line"><span class="cl">        <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Running on Google Colab. NumPyro will be installed in this environment.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">numpyro</span><span class="nd">@git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pyro</span><span class="o">-</span><span class="n">ppl</span><span class="o">/</span><span class="n">numpyro</span> <span class="n">arviz</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;NumPyro is already installed. Skipping installation.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Running locally. Make sure NumPyro and dependencies are installed in the environment.&#34;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Running locally. Make sure NumPyro and dependencies are installed in the environment.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpyro</span>
</span></span><span class="line"><span class="cl"><span class="n">numpyro</span><span class="o">.</span><span class="n">set_host_device_count</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">numpyro.infer</span> <span class="kn">import</span> <span class="n">MCMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">,</span> <span class="n">Predictive</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">scale</span>
</span></span></code></pre></div><h2 id="12-loading-the-dataset">1.2 Loading the Dataset</h2>
<p>We load the cirrhosis dataset, adjusting the file path depending on whether we&rsquo;re running the code on Google Colab or locally. After reading the data into a pandas DataFrame, we display the first few rows to preview the dataset.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">  <span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;https://archive.ics.uci.edu/static/public/878/data.csv&#39;</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;data/cirrhosis.csv&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>N_Days</th>
      <th>Status</th>
      <th>Drug</th>
      <th>Age</th>
      <th>Sex</th>
      <th>Ascites</th>
      <th>Hepatomegaly</th>
      <th>Spiders</th>
      <th>Edema</th>
      <th>Bilirubin</th>
      <th>Cholesterol</th>
      <th>Albumin</th>
      <th>Copper</th>
      <th>Alk_Phos</th>
      <th>SGOT</th>
      <th>Tryglicerides</th>
      <th>Platelets</th>
      <th>Prothrombin</th>
      <th>Stage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>400</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>21464</td>
      <td>F</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>14.5</td>
      <td>261.0</td>
      <td>2.60</td>
      <td>156.0</td>
      <td>1718.0</td>
      <td>137.95</td>
      <td>172.0</td>
      <td>190.0</td>
      <td>12.2</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4500</td>
      <td>C</td>
      <td>D-penicillamine</td>
      <td>20617</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>1.1</td>
      <td>302.0</td>
      <td>4.14</td>
      <td>54.0</td>
      <td>7394.8</td>
      <td>113.52</td>
      <td>88.0</td>
      <td>221.0</td>
      <td>10.6</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1012</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>25594</td>
      <td>M</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
      <td>S</td>
      <td>1.4</td>
      <td>176.0</td>
      <td>3.48</td>
      <td>210.0</td>
      <td>516.0</td>
      <td>96.10</td>
      <td>55.0</td>
      <td>151.0</td>
      <td>12.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1925</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>19994</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>S</td>
      <td>1.8</td>
      <td>244.0</td>
      <td>2.54</td>
      <td>64.0</td>
      <td>6121.8</td>
      <td>60.63</td>
      <td>92.0</td>
      <td>183.0</td>
      <td>10.3</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1504</td>
      <td>CL</td>
      <td>Placebo</td>
      <td>13918</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>3.4</td>
      <td>279.0</td>
      <td>3.53</td>
      <td>143.0</td>
      <td>671.0</td>
      <td>113.15</td>
      <td>72.0</td>
      <td>136.0</td>
      <td>10.9</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="2-preparing-and-cleaning-the-data">2. Preparing and Cleaning the Data</h1>
<p>We examine the dataset&rsquo;s structure and data types using the <code>data.info()</code> method. This provides an overview of the dataset, including the number of entries, columns, non-null counts, data types, and memory usage. This step is crucial for identifying missing values and planning how to handle them in our analysis.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 20 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   ID             418 non-null    int64  
 1   N_Days         418 non-null    int64  
 2   Status         418 non-null    object 
 3   Drug           312 non-null    object 
 4   Age            418 non-null    int64  
 5   Sex            418 non-null    object 
 6   Ascites        312 non-null    object 
 7   Hepatomegaly   312 non-null    object 
 8   Spiders        312 non-null    object 
 9   Edema          418 non-null    object 
 10  Bilirubin      418 non-null    float64
 11  Cholesterol    284 non-null    float64
 12  Albumin        418 non-null    float64
 13  Copper         310 non-null    float64
 14  Alk_Phos       312 non-null    float64
 15  SGOT           312 non-null    float64
 16  Tryglicerides  282 non-null    float64
 17  Platelets      407 non-null    float64
 18  Prothrombin    416 non-null    float64
 19  Stage          412 non-null    float64
dtypes: float64(10), int64(3), object(7)
memory usage: 65.4+ KB
</code></pre>
<p>To assess the extent of missing data in our dataset, we calculate the total number of NaN (missing) values.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># check for nan</span>
</span></span><span class="line"><span class="cl"><span class="n">nan_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of NaN values:&#39;</span><span class="p">,</span> <span class="n">nan_values</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>Number of NaN values: 1033
</code></pre>
<p>This reveals that there are <strong>1,033 missing values</strong> across various columns. Understanding the amount and distribution of missing data is crucial for deciding how to handle it in our analysis, whether through imputation, removal, or other methods.</p>
<p>To pinpoint which columns contain missing values, we identify and list all columns with NaN entries. Recognizing these columns is essential for data cleaning and preprocessing steps. To handle the missing data, we choose to fill all NaN values with zeros using the <code>fillna(0)</code> method. This approach ensures that our dataset is complete and ready for analysis, without excluding any records due to missing values. After performing this operation, we display the updated dataset to confirm that all missing values have been addressed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># find columns with NaN values</span>
</span></span><span class="line"><span class="cl"><span class="n">nan_columns</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Columns with NaN values:</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">nan_columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># fill NaN values with 0</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span>
</span></span></code></pre></div><pre><code>Columns with NaN values:
 ['Drug', 'Ascites', 'Hepatomegaly', 'Spiders', 'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>N_Days</th>
      <th>Status</th>
      <th>Drug</th>
      <th>Age</th>
      <th>Sex</th>
      <th>Ascites</th>
      <th>Hepatomegaly</th>
      <th>Spiders</th>
      <th>Edema</th>
      <th>Bilirubin</th>
      <th>Cholesterol</th>
      <th>Albumin</th>
      <th>Copper</th>
      <th>Alk_Phos</th>
      <th>SGOT</th>
      <th>Tryglicerides</th>
      <th>Platelets</th>
      <th>Prothrombin</th>
      <th>Stage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>400</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>21464</td>
      <td>F</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>Y</td>
      <td>14.5</td>
      <td>261.0</td>
      <td>2.60</td>
      <td>156.0</td>
      <td>1718.0</td>
      <td>137.95</td>
      <td>172.0</td>
      <td>190.0</td>
      <td>12.2</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>4500</td>
      <td>C</td>
      <td>D-penicillamine</td>
      <td>20617</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>1.1</td>
      <td>302.0</td>
      <td>4.14</td>
      <td>54.0</td>
      <td>7394.8</td>
      <td>113.52</td>
      <td>88.0</td>
      <td>221.0</td>
      <td>10.6</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1012</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>25594</td>
      <td>M</td>
      <td>N</td>
      <td>N</td>
      <td>N</td>
      <td>S</td>
      <td>1.4</td>
      <td>176.0</td>
      <td>3.48</td>
      <td>210.0</td>
      <td>516.0</td>
      <td>96.10</td>
      <td>55.0</td>
      <td>151.0</td>
      <td>12.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1925</td>
      <td>D</td>
      <td>D-penicillamine</td>
      <td>19994</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>S</td>
      <td>1.8</td>
      <td>244.0</td>
      <td>2.54</td>
      <td>64.0</td>
      <td>6121.8</td>
      <td>60.63</td>
      <td>92.0</td>
      <td>183.0</td>
      <td>10.3</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1504</td>
      <td>CL</td>
      <td>Placebo</td>
      <td>13918</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>Y</td>
      <td>N</td>
      <td>3.4</td>
      <td>279.0</td>
      <td>3.53</td>
      <td>143.0</td>
      <td>671.0</td>
      <td>113.15</td>
      <td>72.0</td>
      <td>136.0</td>
      <td>10.9</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>413</th>
      <td>414</td>
      <td>681</td>
      <td>D</td>
      <td>0</td>
      <td>24472</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>1.2</td>
      <td>0.0</td>
      <td>2.96</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>174.0</td>
      <td>10.9</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>414</th>
      <td>415</td>
      <td>1103</td>
      <td>C</td>
      <td>0</td>
      <td>14245</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>0.9</td>
      <td>0.0</td>
      <td>3.83</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>180.0</td>
      <td>11.2</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>415</th>
      <td>416</td>
      <td>1055</td>
      <td>C</td>
      <td>0</td>
      <td>20819</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>1.6</td>
      <td>0.0</td>
      <td>3.42</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>143.0</td>
      <td>9.9</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>416</th>
      <td>417</td>
      <td>691</td>
      <td>C</td>
      <td>0</td>
      <td>21185</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>0.8</td>
      <td>0.0</td>
      <td>3.75</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>269.0</td>
      <td>10.4</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>417</th>
      <td>418</td>
      <td>976</td>
      <td>C</td>
      <td>0</td>
      <td>19358</td>
      <td>F</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>N</td>
      <td>0.7</td>
      <td>0.0</td>
      <td>3.29</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>350.0</td>
      <td>10.6</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
<p>418 rows × 20 columns</p>
</div>
<h3 id="21-data-preprocessing">2.1 Data Preprocessing</h3>
<p>To prepare the dataset for modeling, we perform several preprocessing steps, including handling categorical variables, scaling numerical features, and encoding target variables.</p>
<p><strong>1. Extracting and Processing Categorical Data:</strong></p>
<ul>
<li>
<p><strong>Selecting Categorical Columns:</strong> We extract all columns with data type <code>'object'</code>, which represent categorical variables in the dataset.</p>
</li>
<li>
<p><strong>Including <code>'Stage'</code> as Categorical:</strong> Although the <code>'Stage'</code> column is numeric, it represents categorical stages of cirrhosis, so we include it in the categorical data.</p>
</li>
<li>
<p><strong>Mapping <code>'Status'</code> to Binary Values:</strong> The <code>'Status'</code> column indicates the patient&rsquo;s survival status with values <code>'C'</code> (Censored), <code>'CL'</code> (Censored Liver), and <code>'D'</code> (Deceased). We map these to binary values for modeling, where <code>'C'</code> and <code>'CL'</code> are mapped to 0 (survived), and <code>'D'</code> is mapped to 1 (did not survive).</p>
</li>
</ul>
<p><strong>2. Extracting and Scaling Numerical Data:</strong></p>
<ul>
<li>
<p><strong>Selecting Numerical Features:</strong> We select all numerical columns from the dataset, excluding <code>'ID'</code>, <code>'N_Days'</code>, and <code>'Stage'</code>.</p>
<ul>
<li><strong><code>'ID'</code>:</strong> A unique identifier for each patient, which does not contribute to the model and can be excluded.</li>
<li><strong><code>'N_Days'</code>:</strong> Represents the number of days of follow-up and will be used as the target variable in survival analysis.</li>
<li><strong><code>'Stage'</code>:</strong> Already included as a categorical variable.</li>
</ul>
</li>
<li>
<p><strong>Scaling Numerical Features:</strong> We scale the numerical features using standard scaling (mean = 0, variance = 1) to normalize the data, which can improve the performance of many machine learning models.</p>
</li>
</ul>
<p><strong>3. Encoding Categorical Variables:</strong></p>
<ul>
<li><strong>Label Encoding:</strong> We encode the categorical variables into numerical format using label encoding, which assigns unique integer values to each category in a column.</li>
</ul>
<p><strong>4. Combining Processed Data:</strong></p>
<ul>
<li><strong>Concatenating DataFrames:</strong> We concatenate the encoded categorical data, the scaled numerical data, and the <code>'ID'</code>, <code>'N_Days'</code>, and <code>'Stage'</code> columns into a single DataFrame. This consolidated dataset is now ready for modeling.</li>
</ul>
<p>Finally, we display the first few rows of the processed dataset to verify that the preprocessing steps have been applied correctly.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># separate categorical and numerical features</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span> <span class="o">=</span> <span class="s1">&#39;object&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># include &#39;Stage&#39; in categorical data</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span><span class="p">[</span><span class="s1">&#39;Stage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Stage&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># map status to binary vlaues</span>
</span></span><span class="line"><span class="cl"><span class="n">status_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;CL&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">categorical_data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">status_mapping</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># select numerical features, excluding &#39;Stage&#39;, &#39;ID&#39; and &#39;N_Days&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;Stage&#39; is actually a categorical feature</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;ID&#39; has no relevance</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;N_Days&#39; is the target value</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span> <span class="o">=</span> <span class="s1">&#39;number&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Days&#39;</span><span class="p">,</span> <span class="s1">&#39;Stage&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># scale numerical features</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_scaled</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">numerical_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_scaled_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">numerical_scaled</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">numerical_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># label encoder</span>
</span></span><span class="line"><span class="cl"><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">categorical_data</span> <span class="o">=</span> <span class="n">categorical_data</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># concatenate encoded categorical and scaled numerical features</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">categorical_data</span><span class="p">,</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Days&#39;</span><span class="p">,</span> <span class="s1">&#39;Stage&#39;</span><span class="p">]],</span> <span class="n">numerical_scaled_df</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Status</th>
      <th>Drug</th>
      <th>Sex</th>
      <th>Ascites</th>
      <th>Hepatomegaly</th>
      <th>Spiders</th>
      <th>Edema</th>
      <th>Stage</th>
      <th>ID</th>
      <th>N_Days</th>
      <th>...</th>
      <th>Age</th>
      <th>Bilirubin</th>
      <th>Cholesterol</th>
      <th>Albumin</th>
      <th>Copper</th>
      <th>Alk_Phos</th>
      <th>SGOT</th>
      <th>Tryglicerides</th>
      <th>Platelets</th>
      <th>Prothrombin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>1</td>
      <td>400</td>
      <td>...</td>
      <td>0.768941</td>
      <td>2.562152</td>
      <td>0.038663</td>
      <td>-2.114296</td>
      <td>0.981918</td>
      <td>0.116853</td>
      <td>0.642305</td>
      <td>1.110012</td>
      <td>-0.572406</td>
      <td>1.20688</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>2</td>
      <td>4500</td>
      <td>...</td>
      <td>0.546706</td>
      <td>-0.481759</td>
      <td>0.198060</td>
      <td>1.513818</td>
      <td>-0.216383</td>
      <td>2.902613</td>
      <td>0.304654</td>
      <td>0.048897</td>
      <td>-0.277943</td>
      <td>-0.06384</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>1012</td>
      <td>...</td>
      <td>1.852567</td>
      <td>-0.413611</td>
      <td>-0.291793</td>
      <td>-0.041088</td>
      <td>1.616312</td>
      <td>-0.473001</td>
      <td>0.063889</td>
      <td>-0.367969</td>
      <td>-0.942860</td>
      <td>1.04804</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>1925</td>
      <td>...</td>
      <td>0.383244</td>
      <td>-0.322748</td>
      <td>-0.027428</td>
      <td>-2.255651</td>
      <td>-0.098903</td>
      <td>2.277917</td>
      <td>-0.426348</td>
      <td>0.099427</td>
      <td>-0.638898</td>
      <td>-0.30210</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>5</td>
      <td>1504</td>
      <td>...</td>
      <td>-1.210972</td>
      <td>0.040704</td>
      <td>0.108642</td>
      <td>0.076708</td>
      <td>0.829193</td>
      <td>-0.396938</td>
      <td>0.299540</td>
      <td>-0.153220</td>
      <td>-1.085342</td>
      <td>0.17442</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>
<h3 id="22-exploratory-data-analysis-eda">2.2 Exploratory Data Analysis (EDA)</h3>
<p>With the data preprocessed, we now perform Exploratory Data Analysis to gain insights into the dataset. This involves visualizing the distributions of key variables and exploring patterns that might inform our modeling approach.</p>
<h4 id="visualizing-survival-status">Visualizing Survival Status</h4>
<p>We begin by examining the distribution of the &lsquo;Status&rsquo; variable, which indicates whether a patient survived (0) or did not survive (1). Understanding the class balance is crucial for classification tasks and can impact the performance of our predictive models.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># count plot</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;Status&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;lightblue&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Status&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_16_0.png" type="" alt="png"  /></p>
<p>The bar plot above shows the distribution of patient survival status in the dataset. The <code>'Status'</code> variable is binary, with 0 representing patients who survived and 1 representing those who did not survive. The plot reveals that there are more patients who survived (labeled as 0) compared to those who did not survive (labeled as 1), indicating a slight class imbalance in the dataset.</p>
<h4 id="correlation-heatmap">Correlation Heatmap</h4>
<p>Next, we examine the relationships between the numerical features by visualizing their pairwise correlations using a heatmap. A correlation heatmap helps us understand how different variables are related to one another, which is particularly useful for identifying multicollinearity or discovering variables that might be strong predictors of survival.</p>
<p>In this plot, the correlation coefficients range from -1 to 1:</p>
<ul>
<li><strong>Positive correlation (values closer to 1)</strong> indicates that as one variable increases, the other also increases.</li>
<li><strong>Negative correlation (values closer to -1)</strong> indicates that as one variable increases, the other decreases.</li>
<li><strong>Values near 0</strong> suggest little to no linear relationship between the variables.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># correlation heatmap</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">numerical_data</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Correlation Heatmap&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_19_0.png" type="" alt="png"  /></p>
<p><strong>Interpretation of the Correlation Heatmap</strong></p>
<p>The heatmap reveals several interesting relationships between the features. For example:</p>
<ul>
<li><strong>Cholesterol and SGOT</strong> have a relatively high positive correlation (0.61), indicating that as cholesterol levels increase, SGOT tends to increase as well.</li>
<li><strong>Cholesterol and Triglycerides</strong> also show a strong positive correlation (0.63), which suggests that these two features might capture similar information.</li>
<li><strong>Bilirubin and Copper</strong> exhibit a moderate positive correlation (0.36), which could indicate some physiological link between these features.</li>
</ul>
<p>By identifying correlated features, we can make informed decisions on feature selection for modeling to avoid multicollinearity or redundancy in the predictors.</p>
<h4 id="correlation-of-features-with-the-target">Correlation of Features with the Target</h4>
<p>To further investigate the predictive power of each feature, we calculate the absolute correlation between each feature and the target variable, <code>Status</code>, which represents patient survival. Correlation with the target helps us identify which features are most strongly associated with survival and might be valuable predictors in our classification and survival models.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># calculate correlations with the target variable</span>
</span></span><span class="line"><span class="cl"><span class="n">correlations</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Days&#39;</span><span class="p">,</span> <span class="s1">&#39;Stage&#39;</span><span class="p">,</span> <span class="s1">&#39;Status&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">corrwith</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate colors</span>
</span></span><span class="line"><span class="cl"><span class="n">n_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">correlations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot horizontal bar plot for feature correlations with the target</span>
</span></span><span class="line"><span class="cl"><span class="n">correlations</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Feature Correlations with the Target&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Correlation&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_22_0.png" type="" alt="png"  /></p>
<p><strong>Plotting Feature Correlations</strong></p>
<p>In the horizontal bar plot above, the features are sorted by their correlation with the target variable. Features with higher correlations are likely to be more informative in predicting the patient&rsquo;s survival status.</p>
<p><strong>Interpretation of the Plot</strong></p>
<ul>
<li><strong>Bilirubin</strong>, <strong>Edema</strong>, and <strong>Copper</strong> are among the features with the strongest correlation to the target, suggesting they may be highly influential in determining survival outcomes.</li>
<li><strong>Sex</strong> and <strong>Drug</strong> show relatively weak correlations, indicating they might have less impact on survival predictions.</li>
<li>This plot will help guide the selection of features in our model, focusing on those with higher correlations for better predictive performance.</li>
</ul>
<h4 id="pair-plot-of-selected-features">Pair Plot of Selected Features</h4>
<p>To explore the relationships between the most significant features and the target variable, we create a pair plot. This type of plot is particularly useful for visualizing the pairwise relationships between features, along with their distributions, while also distinguishing between different classes of the target variable.</p>
<p><strong>Selecting Top Features</strong></p>
<p>We focus on the top features identified from the previous correlation analysis to avoid overloading the plot with too many variables. The selected features have the strongest correlation with the target variable, <code>Status</code>, and are likely to be the most informative for our analysis.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># pairplot</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># first, take care of the warning:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># FutureWarning: use_inf_as_na option is deprecated and will be removed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># in a future version. Convert inf values to NaN before operating instead.</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># there are too many features, so we only plot some of them</span>
</span></span><span class="line"><span class="cl"><span class="c1"># we have previously assigned the correlations and sorted them</span>
</span></span><span class="line"><span class="cl"><span class="c1"># so now we select the top features</span>
</span></span><span class="line"><span class="cl"><span class="n">top_features</span> <span class="o">=</span> <span class="n">correlations</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">7</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">selected_features</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">selected_features</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s1">&#39;Status&#39;</span><span class="p">);</span>
</span></span></code></pre></div><pre><code>['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos']
</code></pre>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_25_1.png" type="" alt="png"  /></p>
<p><strong>Interpretation of the Pair Plot</strong></p>
<p>In the pair plot above, we observe scatter plots for each pair of selected features, with data points colored by the patient&rsquo;s survival status (<code>Status</code>):</p>
<ul>
<li><strong>Blue</strong> represents patients who survived (<code>Status = 0</code>).</li>
<li><strong>Orange</strong> represents patients who did not survive (<code>Status = 1</code>).</li>
</ul>
<p>Diagonal plots show the distribution of individual features, while off-diagonal plots reveal the relationships between different features. This helps us identify potential clusters, outliers, and separations between survival classes based on feature combinations. For example, we can observe how <strong>Bilirubin</strong> and <strong>Edema</strong> may differ between the two groups, providing insights into their predictive power.</p>
<h1 id="3-bayesian-logistic-regression-for-survival-classification">3. Bayesian Logistic Regression for Survival Classification</h1>
<p>In this section, we implement a Bayesian logistic regression model to predict patient survival based on clinical features. The logistic regression model is a widely used method for binary classification tasks. In a Bayesian framework, the model allows us to incorporate prior knowledge and quantify uncertainty in the predictions, making it especially suitable for medical applications where uncertainty plays a crucial role.</p>
<h2 id="31-defining-the-bayesian-logistic-regression-model">3.1 Defining the Bayesian Logistic Regression Model</h2>
<p>The logistic regression model predicts the probability of an event (in this case, patient survival) occurring. The model&rsquo;s structure can be described as:</p>
<p>\begin{aligned}
\text{logit}(P(y = 1 \mid X)) = \alpha + X\beta
\end{aligned}</p>
<p>Where:</p>
<ul>
<li>$ y $ is the binary outcome (survival: 0 or 1).</li>
<li>$ X $ is the matrix of features (predictors).</li>
<li>$ \alpha $ is the intercept term (a scalar).</li>
<li>$ \beta $ is the vector of coefficients corresponding to the features in $ X $.</li>
<li>$ \text{logit}(p) $ is the log-odds transformation: $ \text{logit}(p) = \log\left(\frac{p}{1 - p}\right) $.</li>
</ul>
<p>The posterior distribution for the model parameters $ \alpha $ and $ \beta $ is obtained using Bayesian inference, which combines prior distributions with the likelihood of the data.</p>
<h3 id="code-explanation">Code Explanation</h3>
<p>In the code below, we define the Bayesian logistic regression model using <a href="https://num.pyro.ai/en/stable/index.html"><strong>NumPyro</strong></a>:</p>
<ul>
<li>
<p><strong>Priors:</strong> We assume normal priors for both the intercept term $ \alpha $ and the coefficients $ \beta $. These priors reflect our belief about the parameters before observing the data:</p>
<ul>
<li>$ \alpha \sim \mathcal{N}(0, 1) $</li>
<li>$ \beta_j \sim \mathcal{N}(0, 1) $ for each feature $ j $.</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong> The likelihood function specifies how the data (observed outcomes) are generated given the parameters. In this case, the likelihood follows a Bernoulli distribution, where the probability of success (survival) is given by the logistic function applied to the linear combination of the features.</p>
</li>
</ul>
<p>\begin{aligned}
P(y_i = 1 \mid X_i, \alpha, \beta) = \frac{1}{1 + \exp^{-(\alpha + X_i\beta)}}
\end{aligned}</p>
<p>The code block defines the model structure, including priors and likelihood, and will be used in the next steps for inference using MCMC (Markov Chain Monte Carlo).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># define the bayesian model</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">cirrhosis_classification_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">alpha</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                              <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]),</span>
</span></span><span class="line"><span class="cl">                              <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
</span></span><span class="line"><span class="cl">                          <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># define likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="preparing-the-data-for-modeling">Preparing the Data for Modeling</h3>
<p>Before fitting the Bayesian logistic regression model, we need to prepare the input data. We use the top features identified from the correlation analysis as our predictors, and the <code>Status</code> column as the binary outcome (survival).</p>
<ul>
<li><strong><code>X</code>:</strong> The matrix of predictor variables (features), consisting of the top correlated features selected during EDA.</li>
<li><strong><code>y</code>:</strong> The target variable, representing the patient&rsquo;s survival status (0 for survived, 1 for not survived).</li>
</ul>
<p>We then check the shape of <code>X</code> to confirm that it has the correct dimensions for modeling (rows corresponding to patients and columns corresponding to features).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prepare the data</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">()]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span></code></pre></div><pre><code>(418, 7)
</code></pre>
<h3 id="visualizing-the-model-structure">Visualizing the Model Structure</h3>
<p>To better understand the structure of our Bayesian logistic regression model, we generate a graphical representation using <code>numpyro.render_model()</code>. This function provides a clear visualization of the model&rsquo;s components, including the priors, likelihood, and data dependencies.</p>
<p>In the rendered graph, we can see:</p>
<ul>
<li><strong>Priors:</strong> The intercept <code>alpha</code> and coefficients <code>beta</code> are sampled from normal distributions.</li>
<li><strong>Likelihood:</strong> The binary target variable <code>y</code> follows a Bernoulli distribution with probabilities defined by the logistic transformation of the linear combination of features (<code>logits</code>).</li>
</ul>
<p>This graphical model representation helps ensure that the model is correctly specified and provides insights into how the data interacts with the parameters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">numpyro</span><span class="o">.</span><span class="n">render_model</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">,</span> <span class="n">model_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_32_0.svg" type="" alt="svg"  /></p>
<h3 id="32-running-mcmc-inference">3.2 Running MCMC Inference</h3>
<p>We use <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong> to perform Bayesian inference on our logistic regression model. MCMC is a powerful method for sampling from the posterior distribution of our model&rsquo;s parameters, especially in cases where analytical solutions are intractable.</p>
<ul>
<li><strong><code>NUTS</code>:</strong> A variant of Hamiltonian Monte Carlo (HMC) that automatically tunes the step size and trajectory length, improving the efficiency of the sampling process.</li>
<li><strong><code>num_warmup</code>:</strong> The number of warm-up steps (burn-in period) where the sampler adapts to the posterior distribution.</li>
<li><strong><code>num_samples</code>:</strong> The number of samples to draw from the posterior after the warm-up phase.</li>
<li><strong><code>num_chains</code>:</strong> The number of independent MCMC chains to run in parallel, allowing us to evaluate the convergence of the model.</li>
</ul>
<p>We then execute the sampling procedure using the <code>run()</code> method, which generates samples from the posterior distribution of the model parameters. After sampling, we print the summary of the results, which includes posterior means, standard deviations, and diagnostics such as effective sample size (ESS) and R-hat (a measure of convergence).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
     alpha     -0.54      0.14     -0.54     -0.76     -0.32   6398.50      1.00
   beta[0]      1.21      0.21      1.21      0.85      1.55   6336.86      1.00
   beta[1]      0.65      0.30      0.65      0.11      1.09   4819.01      1.00
   beta[2]      0.32      0.15      0.32      0.06      0.55   6200.94      1.00
   beta[3]      0.44      0.15      0.44      0.21      0.69   6358.73      1.00
   beta[4]     -0.14      0.13     -0.15     -0.36      0.07   6798.94      1.00
   beta[5]      0.64      0.14      0.64      0.41      0.86   7289.60      1.00
   beta[6]      0.41      0.13      0.40      0.19      0.62   6308.57      1.00

Number of divergences: 0
</code></pre>
<h4 id="mcmc-inference-summary">MCMC Inference Summary</h4>
<p>The output of the MCMC inference provides a summary of the posterior distributions for each parameter in the model, including the intercept (<code>alpha</code>) and the coefficients for the selected features (<code>beta</code> values). Each row represents a parameter, and the summary includes the following key statistics:</p>
<ul>
<li><strong>mean:</strong> The mean of the posterior distribution, which gives us a point estimate of the parameter.</li>
<li><strong>std:</strong> The standard deviation of the posterior, indicating the uncertainty in the parameter estimate.</li>
<li><strong>median:</strong> The median of the posterior distribution, often used as a robust point estimate.</li>
<li><strong>5.0% and 95.0% percentiles:</strong> The lower and upper bounds of the 90% credible interval, giving us a range within which the true value of the parameter is likely to lie with 90% probability.</li>
<li><strong>n_eff:</strong> The effective sample size, which quantifies the amount of independent information in the MCMC samples. A higher value suggests that the chain is well-mixed and the samples are less correlated.</li>
<li><strong>r_hat:</strong> The R-hat statistic, which measures the convergence of the MCMC chains. Values close to 1 indicate good convergence, meaning the chains have converged to the same distribution.</li>
</ul>
<h4 id="interpretation">Interpretation</h4>
<ul>
<li>
<p><strong><code>alpha</code>:</strong> The intercept has a posterior mean of -0.54 with a standard deviation of 0.14, indicating moderate uncertainty. The 90% credible interval ranges from -0.76 to -0.32.</p>
</li>
<li>
<p><strong><code>beta[0]</code>:</strong> This coefficient has the highest posterior mean (1.21) and the smallest credible interval, suggesting that it has a strong positive effect on the probability of survival.</p>
</li>
<li>
<p><strong><code>beta[1]</code> to <code>beta[6]</code>:</strong> These coefficients represent the effects of the top features on survival. The values range between -0.14 and 0.65, indicating varying levels of influence. For example, <code>beta[5]</code> (0.64) and <code>beta[6]</code> (0.41) show relatively strong positive associations with the target variable.</p>
</li>
<li>
<p><strong>Effective Sample Size (n_eff):</strong> All parameters have high effective sample sizes (in the thousands), indicating that the MCMC sampler performed efficiently and produced a large number of independent samples.</p>
</li>
<li>
<p><strong>R-hat:</strong> The R-hat values are all 1.00, confirming that the MCMC chains have converged successfully for each parameter.</p>
</li>
</ul>
<p>The inference results show that the model has converged well, and we have meaningful posterior estimates for the model parameters. The posterior means and credible intervals provide insights into how the features are associated with the probability of survival, with several coefficients (e.g., <code>beta[0]</code>, <code>beta[5]</code>) standing out as strong predictors.</p>
<h3 id="33-trace-and-pair-plots-for-mcmc-diagnostics">3.3 Trace and Pair Plots for MCMC Diagnostics</h3>
<p>To assess the performance and convergence of the MCMC sampler, we generate trace plots and pair plots of the sampled posterior distributions. These plots allow us to visually inspect how well the sampler explored the parameter space and whether there were any issues, such as divergences.</p>
<ol>
<li>
<p><strong>Trace Plot:</strong>
The trace plot shows the sampled values of each parameter across the MCMC iterations. Ideally, we expect to see &ldquo;well-mixed&rdquo; chains, where the samples quickly explore the parameter space without getting stuck, indicating good convergence.</p>
</li>
<li>
<p><strong>Pair Plot:</strong>
The pair plot visualizes the relationships between the parameters by plotting their joint posterior distributions. We also include information about divergences, which can indicate potential problems with the sampling process. Divergences suggest that the sampler struggled to explore certain regions of the posterior, often due to ill-specified priors or likelihoods.</p>
</li>
</ol>
<p>These plots provide valuable diagnostics for ensuring that the MCMC sampling was successful and that the posterior estimates are reliable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">trace</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_37_0.png" type="" alt="png"  /></p>
<h4 id="interpreting-the-trace-plot">Interpreting the Trace Plot</h4>
<p>The <strong>trace plot</strong> shows the sampled values of each parameter (both <code>alpha</code> and <code>beta</code> coefficients) across the iterations of the MCMC chains. The left-hand side of each panel displays the posterior distribution (density plot), while the right-hand side shows the trace of the samples.</p>
<ul>
<li>
<p><strong>Posterior Distributions:</strong> Each parameter&rsquo;s posterior distribution appears well-behaved and roughly Gaussian, with all MCMC chains overlapping significantly. This indicates good convergence across the different chains.</p>
</li>
<li>
<p><strong>Trace of Samples:</strong> The trace plots show that the MCMC samples fluctuate around a stable mean, with no obvious trends or drifts. This behavior suggests that the chains have mixed well and are sampling from the stationary distribution, which is essential for reliable posterior estimates.</p>
</li>
</ul>
<p>Overall, the trace plot confirms that the model parameters have converged and the posterior distributions are stable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">divergences</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_39_0.png" type="" alt="png"  /></p>
<h4 id="interpreting-the-pair-plot">Interpreting the Pair Plot</h4>
<p>The <strong>pair plot</strong> shows pairwise relationships between the parameters (<code>alpha</code> and <code>beta</code> coefficients), with each dot representing a sample from the posterior distribution. The diagonal plots display the marginal distributions for each parameter, while the off-diagonal scatter plots show the joint distributions between pairs of parameters.</p>
<ul>
<li>
<p><strong>Joint Distributions:</strong> The scatter plots between parameter pairs show no strong patterns, such as high correlations or dependencies. This suggests that the parameters are relatively independent of each other, which is a positive sign for the model&rsquo;s stability and interpretability.</p>
</li>
<li>
<p><strong>Divergences:</strong> No major divergences or problematic regions are visible in the joint distributions, indicating that the sampler did not struggle with any particular combination of parameters.</p>
</li>
</ul>
<p>The pair plot helps us visually confirm that the posterior samples form well-behaved distributions, with no obvious issues like strong correlations or divergences. Together, the trace and pair plots provide strong evidence that the MCMC sampling has performed effectively and that the posterior estimates are trustworthy.</p>
<h3 id="34-prior-predictive-checks">3.4 Prior Predictive Checks</h3>
<p>Before analyzing the posterior predictive distribution, it is important to first check the prior predictive distribution to ensure that our prior assumptions make sense. A <strong>prior predictive check</strong> involves generating data from the model using only the priors, without any influence from the observed data. This allows us to see if the priors we have chosen are reasonable and if they align with plausible outcomes.</p>
<ul>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to sample from the prior distribution of our Bayesian model.</li>
<li><strong><code>y_prior</code>:</strong> This contains samples generated from the prior distribution. These are drawn without considering the observed data, giving us a sense of the expected outcomes under the prior assumptions.</li>
<li><strong><code>prior_data</code>:</strong> The prior samples are converted into an <code>InferenceData</code> object, which allows us to use <strong>ArviZ</strong>&rsquo;s plotting utilities.</li>
<li><strong>Plotting Prior Predictive Samples:</strong> The prior predictive plot shows the range of outcomes generated purely from the prior distributions.</li>
</ul>
<p>This check ensures that our prior assumptions are not unrealistic and that the prior distributions provide a reasonable range of expected outcomes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># get the prior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">y_prior</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to InferenceData</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">prior</span> <span class="o">=</span> <span class="n">y_prior</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the prior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">prior_data</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;prior&#39;</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_42_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-prior-predictive-check">Interpretation of the Prior Predictive Check</h4>
<p>The prior predictive check plot shows the range of possible outcomes (in blue) generated from the model based purely on the prior distributions. The dashed orange line represents the mean of the prior predictive distribution, while the solid black line indicates the observed data.</p>
<ul>
<li>
<p><strong>Prior Predictive Range:</strong> The blue lines show a wide spread of potential outcomes, indicating that the priors we defined for the model parameters allow for a broad range of values for the binary target variable (<code>y</code>), which corresponds to patient survival. This ensures that the priors are not overly restrictive.</p>
</li>
<li>
<p><strong>Comparison to Observed Data:</strong> The observed data (black lines) mostly align within the range of the prior predictive distribution, suggesting that the priors are reasonable and that the model is well-calibrated to allow for plausible outcomes before even fitting to the actual data.</p>
</li>
</ul>
<p>Overall, this plot suggests that the prior distributions are neither too narrow nor too unrealistic, providing a good foundation for the next step: posterior inference based on observed data.</p>
<h3 id="35-posterior-predictive-check">3.5 Posterior Predictive Check</h3>
<p>After fitting the model to the observed data, we perform a <strong>posterior predictive check</strong>. This step involves generating new data from the posterior distribution of the model, which incorporates both the prior information and the observed data. By comparing the posterior predictive distribution to the actual observed outcomes, we can assess how well the model fits the data.</p>
<ul>
<li><strong><code>posterior_samples</code>:</strong> These are the samples drawn from the posterior distribution after fitting the model to the data using MCMC.</li>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to generate predictions based on the posterior samples.</li>
<li><strong>Posterior Predictive Distribution:</strong> The plot visualizes the range of predicted values from the posterior distribution, along with the observed data for comparison.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get samples from the posterior</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define a Predictive class</span>
</span></span><span class="line"><span class="cl"><span class="n">predictive</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">cirrhosis_classification_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># get the samples</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">predictive</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to inference data</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_data</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">posterior_data</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;posterior&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_45_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-posterior-predictive-plot">Interpretation of the Posterior Predictive Plot</h4>
<p>In the resulting plot:</p>
<ul>
<li>The <strong>blue lines</strong> represent the range of possible outcomes generated from the posterior predictive distribution.</li>
<li>The <strong>solid black line</strong> shows the observed data.</li>
<li>The <strong>dashed orange line</strong> indicates the posterior predictive mean.</li>
</ul>
<p>In contrast to the prior predictive plot, the posterior predictive distribution is now informed by the data:</p>
<ul>
<li>The posterior predictive outcomes are much more closely aligned with the observed data, indicating that the model has successfully learned from the data.</li>
<li>The <strong>posterior predictive mean</strong> closely matches the observed values, showing that the model is well-calibrated and provides accurate predictions for the binary survival outcomes.</li>
</ul>
<p>This posterior predictive check confirms that the model is capable of producing predictions that are consistent with the observed data, providing confidence in its predictive power.</p>
<h3 id="36-visualizing-the-logistic-regression-sigmoid-curves">3.6 Visualizing the Logistic Regression Sigmoid Curves</h3>
<p>To better understand the relationship between <strong>Bilirubin</strong> (one of the top predictive features) and the <strong>survival status</strong> of patients, we plot the sigmoid function for each of the posterior samples. The sigmoid curve represents the probability of survival as a function of the Bilirubin levels.</p>
<ul>
<li>
<p><strong>Sigmoid Function:</strong> The logistic regression model uses the sigmoid function to transform the linear combination of features into a probability:</p>
<p>\begin{aligned}
\text{sigmoid}(x) = \frac{1}{1 + \exp{-(\alpha + x \cdot \beta)}}
\end{aligned}</p>
<p>where $ \alpha $ is the intercept and $ \beta $ is the coefficient for Bilirubin in this case.</p>
</li>
<li>
<p><strong>Posterior Samples:</strong> We draw several samples from the posterior distribution of the model parameters (<code>alpha</code> and <code>beta</code>), and for each sample, we plot the corresponding sigmoid curve. This gives us a range of possible outcomes, representing the uncertainty in the model.</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># define the sigmoid function</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate a range of X values for plotting the sigmoid functions</span>
</span></span><span class="line"><span class="cl"><span class="n">x_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the data</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">edgecolors</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>  <span class="c1"># bilirubin vs status</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot sigmoid functions from the posterior samples</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_range</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">    <span class="n">alpha_sample</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta_sample</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_sample</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">alpha_sample</span><span class="p">,</span> <span class="n">beta_sample</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">y_sample</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">top_features</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Status&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cirrhosis Classification Model&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_48_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-plot">Interpretation of the Plot</h4>
<ul>
<li>
<p><strong>Red Points:</strong> These represent the actual data, with the x-axis showing Bilirubin levels and the y-axis showing the observed survival status (0 or 1).</p>
</li>
<li>
<p><strong>Blue Sigmoid Curves:</strong> The faint blue lines represent the range of sigmoid functions sampled from the posterior distribution. These curves show how the model&rsquo;s predictions vary depending on different samples from the posterior. The spread of the curves reflects the uncertainty in the model&rsquo;s predictions.</p>
</li>
<li>
<p><strong>General Trend:</strong> As Bilirubin levels increase, the probability of survival decreases, as shown by the steep rise of the sigmoid curves from 0 to 1. This suggests that higher Bilirubin levels are associated with a higher likelihood of not surviving.</p>
</li>
</ul>
<p>This plot provides a clear visualization of how the model is using Bilirubin to predict patient survival and the uncertainty in those predictions.</p>
<h3 id="37-model-evaluation-confusion-matrix">3.7 Model Evaluation: Confusion Matrix</h3>
<p>After generating predictions using the posterior samples, we evaluate the model&rsquo;s performance by comparing the predicted classes to the actual survival outcomes. This is done using a <strong>confusion matrix</strong>, which summarizes the classification results in terms of:</p>
<ul>
<li><strong>True Positives (TP):</strong> Correct predictions of non-survival.</li>
<li><strong>True Negatives (TN):</strong> Correct predictions of survival.</li>
<li><strong>False Positives (FP):</strong> Incorrect predictions of non-survival.</li>
<li><strong>False Negatives (FN):</strong> Incorrect predictions of survival.</li>
</ul>
<p>In the code:</p>
<ul>
<li>We use the sigmoid function to convert the posterior samples into probabilities.</li>
<li>We then average the probabilities across all posterior samples and apply a threshold of 0.5 to classify each patient as either survived (0) or not survived (1).</li>
<li>Finally, we generate a confusion matrix to visually assess the performance of the classifier.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate predictions</span>
</span></span><span class="line"><span class="cl"><span class="c1"># use sigmoid function defined previously</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate predictions using the posterior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">n_samples</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># calculate the mean probability across posterior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">predicted_probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">posterior_samples</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">             <span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">             <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert probabilities to binary predictions</span>
</span></span><span class="line"><span class="cl"><span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">predicted_classes</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted_probabilities</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># compare predicted classes with true classes</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># actual labels</span>
</span></span><span class="line"><span class="cl"><span class="n">y_true</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">predicted_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_51_0.png" type="" alt="png"  /></p>
<h4 id="confusion-matrix-interpretation">Confusion Matrix Interpretation</h4>
<p>In the plot:</p>
<ul>
<li><strong>233 true negatives</strong>: The model correctly predicted 233 patients as survivors.</li>
<li><strong>98 true positives</strong>: The model correctly predicted 98 patients as non-survivors.</li>
<li><strong>24 false positives</strong>: The model incorrectly predicted 24 patients as non-survivors when they actually survived.</li>
<li><strong>63 false negatives</strong>: The model incorrectly predicted 63 patients as survivors when they did not survive.</li>
</ul>
<h4 id="overall-performance">Overall Performance</h4>
<p>The confusion matrix shows that the model performs well on survival predictions, with more correct predictions than incorrect ones. The balance between true positives and true negatives suggests that the model handles both classes reasonably well, but there are more false negatives than false positives, indicating that the model might underpredict non-survival cases slightly.</p>
<h1 id="4-bayesian-survival-analysis">4. Bayesian Survival Analysis</h1>
<p>In this section, we shift our focus from classification to <strong>Bayesian survival analysis</strong>. Survival analysis is a statistical method used to estimate the time until an event of interest occurs, often referred to as &ldquo;time-to-event&rdquo; data. In our case, we are interested in modeling the time until either a patient succumbs to cirrhosis or is censored (i.e., the event does not occur during the observation period). This form of analysis allows us to incorporate censored data and provides insights into the distribution of survival times.</p>
<h2 id="41-basics-of-survival-analysis">4.1 Basics of Survival Analysis</h2>
<p>In classical survival analysis, we model the <strong>survival function</strong> $S(t)$, which represents the probability of survival beyond time $t$:</p>
<p>\begin{aligned}
S(t) = P(T &gt; t)
\end{aligned}</p>
<p>Where:</p>
<ul>
<li>$T$ is the random variable representing the time to the event (e.g., death or censoring).</li>
<li>$t$ is the specific time of interest.</li>
</ul>
<p>The complementary function, called the <strong>hazard function</strong>, $h(t)$, represents the instantaneous rate at which events occur at time $t$, given that the individual has survived up to that time. The hazard function is defined as:</p>
<p>\begin{aligned}
h(t) = \frac{f(t)}{S(t)}
\end{aligned}</p>
<p>Where $f(t)$ is the probability density function of survival times.</p>
<h3 id="bayesian-survival-analysis">Bayesian Survival Analysis</h3>
<p>In a Bayesian framework, we treat the parameters governing the survival function (or hazard function) as random variables, and we estimate their posterior distributions using observed data. By incorporating prior beliefs about these parameters, Bayesian methods offer a natural way to handle uncertainty, particularly in the presence of censored data.</p>
<p>The general approach involves:</p>
<ol>
<li><strong>Defining a Parametric Survival Model:</strong> A common choice for the survival model is the Weibull distribution, which provides flexibility in modeling both increasing and decreasing hazard rates.</li>
<li><strong>Priors:</strong> Assign prior distributions to the model parameters (e.g., shape and scale parameters of the Weibull distribution).</li>
<li><strong>Likelihood:</strong> The likelihood is defined based on the observed survival times and whether the event was censored.</li>
<li><strong>Posterior Inference:</strong> Use Markov Chain Monte Carlo (MCMC) or similar methods to draw samples from the posterior distribution of the parameters.</li>
</ol>
<p>For example, the <strong>Weibull distribution</strong> is a common parametric model for survival times. Its survival function is given by:</p>
<p>\begin{aligned}
S(t) = \exp \left( - \left( \frac{t}{\lambda} \right)^\kappa \right)
\end{aligned}</p>
<p>Where:</p>
<ul>
<li>$\lambda$ is the scale parameter.</li>
<li>$\kappa$ is the shape parameter.</li>
</ul>
<p>In the Bayesian context, we assign priors to $\lambda$ and $\kappa$, then use the observed data to update these priors to form the posterior distributions.</p>
<h3 id="censored-data">Censored Data</h3>
<p>In survival analysis, it&rsquo;s common to have censored data, where we know that the event of interest did not occur before a certain time, but we do not know the exact time of the event. Bayesian methods handle censored data naturally within the likelihood function by accounting for both observed events and censored observations.</p>
<p>The likelihood for a censored observation is:</p>
<p>\begin{aligned}
P(T &gt; t \mid \text{censored}) = S(t)
\end{aligned}</p>
<p>Thus, in a Bayesian survival model, the posterior distribution is based on both observed survival times and the fact that certain observations were censored.</p>
<p>This framework allows for a flexible and powerful analysis of time-to-event data while handling uncertainties in parameter estimates.</p>
<h2 id="42-weibull-survival-model-without-covariates">4.2 Weibull Survival Model Without Covariates</h2>
<p>We begin by defining a <strong>Weibull survival model</strong> for time-to-event data, with no covariates. The <strong>Weibull distribution</strong> is a flexible parametric model commonly used in survival analysis due to its ability to model both increasing and decreasing hazard rates over time, depending on the value of its shape parameter $k$.</p>
<p>The model assumes that survival times follow a Weibull distribution parameterized by:</p>
<ul>
<li>$k$ (shape parameter): This determines the form of the hazard function. A value of $k &gt; 1$ suggests an increasing hazard rate, while $k &lt; 1$ implies a decreasing hazard rate.</li>
<li>$\lambda$ (scale parameter): This controls the scale of the distribution and shifts the time-to-event distribution horizontally.</li>
</ul>
<h4 id="code-explanation-1">Code Explanation</h4>
<ol>
<li>
<p><strong>Priors:</strong></p>
<ul>
<li>
<p>We place <strong>Exponential(1.0)</strong> priors on both the shape parameter $k$ and the scale parameter $\lambda$. The exponential distribution is a standard choice when we do not have strong prior information and expect non-negative values for these parameters.</p>
</li>
<li>
<p>$k \sim \text{Exponential}(1.0)$</p>
</li>
<li>
<p>$\lambda \sim \text{Exponential}(1.0)$</p>
</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong></p>
<ul>
<li>
<p>The likelihood assumes that the observed survival times follow a <strong>Weibull distribution</strong>. We model the observed data (<code>obs</code>) as being drawn from a Weibull distribution, parameterized by the sampled values of $k$ and $\lambda$.</p>
</li>
<li>
<p>The survival times in <code>data</code> are modeled using this likelihood, and if no data is provided, a placeholder dimension (1000) is used to define the plate.</p>
</li>
</ul>
</li>
</ol>
<p>The Weibull model is ideal for analyzing baseline survival without introducing any covariates to the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">survival_weibull_model</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">lam</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;lam&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Weibull</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lam</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="preparing-the-data-for-survival-analysis">Preparing the Data for Survival Analysis</h3>
<p>Before fitting the survival model, we need to preprocess the data by selecting only the <strong>uncensored</strong> survival times. In survival analysis, censored data refers to cases where the event of interest (e.g., death) did not occur during the observation period, and we do not know the exact time of the event. For this initial analysis, we focus on observed events.</p>
<ul>
<li><strong><code>survival_times_observed</code>:</strong> We extract the survival times for patients where the event (death) was observed, indicated by <code>Status == 1</code>. This ensures that only complete (uncensored) data is included in the model.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prepare the data</span>
</span></span><span class="line"><span class="cl"><span class="c1"># select only the uncensored data</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times_observed</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span></code></pre></div><h3 id="fitting-the-weibull-survival-model">Fitting the Weibull Survival Model</h3>
<p>We now proceed to fit the <strong>Weibull survival model</strong> to the uncensored survival times using <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong>. This step involves estimating the posterior distributions of the model parameters ($k$ and $\lambda$) based on the observed data.</p>
<ul>
<li><strong><code>NUTS</code>:</strong> The No-U-Turn Sampler is a variant of Hamiltonian Monte Carlo (HMC), which efficiently explores the posterior distribution by dynamically tuning the step size and trajectory length.</li>
<li><strong><code>num_warmup</code>:</strong> The number of warm-up (burn-in) iterations, where the sampler adapts to the posterior distribution before drawing final samples.</li>
<li><strong><code>num_samples</code>:</strong> The number of posterior samples to draw after the warm-up phase.</li>
<li><strong><code>num_chains</code>:</strong> The number of independent MCMC chains to run in parallel, which helps assess convergence and reduce bias.</li>
</ul>
<p>After running the MCMC sampler, we print a summary of the estimated posterior distributions for the model parameters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># fit the model</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">survival_weibull_model</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
         k     48.25      7.99     47.86     35.15     61.08   1442.43      1.00
       lam      0.25      0.02      0.25      0.22      0.28   1419.78      1.00

Number of divergences: 0
</code></pre>
<h4 id="interpretation-of-weibull-model-posterior-estimates">Interpretation of Weibull Model Posterior Estimates</h4>
<p>The output from the MCMC run provides the posterior distributions for the shape parameter $k$ and the scale parameter $\lambda$ of the Weibull survival model. Here’s a brief interpretation of the results:</p>
<ul>
<li>
<p><strong>$k$ (shape parameter):</strong> The posterior mean of $k$ is <strong>48.25</strong>, with a standard deviation of <strong>7.99</strong>. The 90% credible interval (from 5% to 95%) ranges from <strong>35.15</strong> to <strong>61.08</strong>. A high value of $k$ suggests that the hazard rate increases rapidly over time, indicating that the risk of the event (death) increases as time progresses.</p>
</li>
<li>
<p><strong>$\lambda$ (scale parameter):</strong> The posterior mean of $\lambda$ is <strong>0.25</strong>, with a standard deviation of <strong>0.02</strong>. The 90% credible interval ranges from <strong>0.22</strong> to <strong>0.28</strong>. The scale parameter affects the distribution&rsquo;s spread; smaller values of $\lambda$ indicate that the survival times are more concentrated around lower values, meaning shorter survival times are more likely.</p>
</li>
<li>
<p><strong>Effective Sample Size (n_eff):</strong> Both $k$ and $\lambda$ have high effective sample sizes, indicating that the MCMC chains were well-mixed and independent, producing a sufficient number of effective samples.</p>
</li>
<li>
<p><strong>$\hat{R}$ values:</strong> The $\hat{R}$ values for both parameters are exactly <strong>1.00</strong>, indicating that the MCMC chains have converged well, and the posterior estimates are reliable.</p>
</li>
<li>
<p><strong>Number of Divergences:</strong> There were <strong>0 divergences</strong>, meaning that the NUTS sampler encountered no issues while exploring the posterior distribution.</p>
</li>
</ul>
<p>The posterior estimates suggest that the risk of the event (death) increases as time passes, with the hazard rate rising rapidly due to the large shape parameter $k$. The small scale parameter $\lambda$ indicates that shorter survival times are common in the dataset.</p>
<h3 id="prior-predictive-check-for-the-weibull-survival-model">Prior Predictive Check for the Weibull Survival Model</h3>
<p>Before we assess the model&rsquo;s posterior predictions, we perform a <strong>prior predictive check</strong> to ensure that the prior distributions we&rsquo;ve chosen are reasonable. This step involves generating data based purely on the priors, without any observed data influencing the predictions. By comparing the prior predictive distribution to the observed data, we can determine if our prior assumptions lead to plausible outcomes.</p>
<h4 id="code-explanation-2">Code Explanation</h4>
<ul>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to generate survival times from the prior distribution by drawing 1,000 samples.</li>
<li><strong>Prior Predictive Plot:</strong> The plot visualizes the prior predictive distribution, represented by the dashed orange line (the mean of the prior predictions), and the solid blue line shows the individual prior predictive samples.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># perform prior predictive check</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_predictive_survival_weibull</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">survival_weibull_model</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_samples_survival_weibull</span> <span class="o">=</span> <span class="n">prior_predictive_survival_weibull</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to InferenceData</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_predictive_data_survival_weibull</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_weibull</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                                         <span class="n">prior</span> <span class="o">=</span> <span class="n">prior_samples_survival_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># prior_samples_survival_weibull</span>
</span></span><span class="line"><span class="cl"><span class="n">prior_predictive_data_survival_weibull</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the prior samples</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">prior_predictive_data_survival_weibull</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;prior&#39;</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_62_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-plot-1">Interpretation of the Plot</h4>
<ul>
<li>The <strong>solid blue line</strong> represents the individual prior predictive samples, while the <strong>dashed orange line</strong> represents the mean of these prior predictions.</li>
<li>The plot shows how the model&rsquo;s prior assumptions generate survival times, and we can compare this to the range of observed data to evaluate the plausibility of the priors.</li>
<li>In this case, the prior predictive distribution seems to cover a reasonable range of potential survival times, suggesting that the prior assumptions about the shape and scale of the Weibull distribution are not overly restrictive or implausible.</li>
</ul>
<p>This prior predictive check gives us confidence that the model&rsquo;s priors are sensible, providing a good foundation for further inference when we include the observed data.</p>
<h3 id="posterior-trace-plot-for-weibull-survival-model">Posterior Trace Plot for Weibull Survival Model</h3>
<p>After fitting the Weibull survival model, we generate a <strong>trace plot</strong> to assess the MCMC sampling process and the posterior distributions of the model parameters. The trace plot shows both the posterior distributions and the sampling traces for the shape parameter $k$ and the scale parameter $\lambda$.</p>
<h4 id="code-explanation-3">Code Explanation</h4>
<ul>
<li><strong><code>survival_posterior_samples_weibull</code>:</strong> We extract the posterior samples for the model parameters.</li>
<li><strong><code>az.plot_trace</code>:</strong> This function generates the trace plot, showing the posterior distribution and the corresponding MCMC trace for each parameter.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">survival_posterior_samples_weibull</span> <span class="o">=</span> <span class="n">mcmc_survival_weibull</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az_survival_weibull</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">az_survival_weibull</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_65_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-trace-plot">Interpretation of the Trace Plot</h4>
<ol>
<li>
<p><strong>Posterior Distributions (Left Panels):</strong></p>
<ul>
<li>The left panels show the posterior distributions for $k$ and $\lambda$. Both parameters exhibit well-defined, unimodal distributions, which indicate stable and meaningful posterior estimates.</li>
<li>For $k$, the distribution has a peak around <strong>48</strong>, while for $\lambda$, the peak is around <strong>0.25</strong>.</li>
</ul>
</li>
<li>
<p><strong>Sampling Trace (Right Panels):</strong></p>
<ul>
<li>The right panels show the sampling trace for each parameter across the MCMC iterations. Both $k$ and $\lambda$ exhibit well-mixed traces, with no visible trends or drifts over time. This indicates that the sampler explored the parameter space effectively, without getting stuck in any particular region.</li>
<li>The trace plots confirm that the MCMC chains have converged well, as there is good overlap between the chains and no sign of poor mixing.</li>
</ul>
</li>
</ol>
<p>Overall, the trace plot shows that the MCMC sampling worked efficiently, with well-behaved posterior distributions and convergence for both $k$ and $\lambda$. This gives us confidence in the reliability of the posterior estimates for the survival model.</p>
<h3 id="posterior-predictive-check-for-weibull-survival-model">Posterior Predictive Check for Weibull Survival Model</h3>
<p>We now perform a <strong>posterior predictive check</strong> to evaluate how well the Weibull survival model, after being fitted to the data, predicts survival times. Posterior predictive checks allow us to compare the model&rsquo;s predicted values to the observed data, providing insight into how well the model captures the true underlying patterns.</p>
<h4 id="code-explanation-4">Code Explanation</h4>
<ul>
<li><strong><code>Predictive</code>:</strong> We use the <code>Predictive</code> class to generate survival times based on the posterior samples of the model parameters ($k$ and $\lambda$).</li>
<li><strong>Posterior Predictive Data:</strong> We create an <code>InferenceData</code> object using the posterior predictive samples, which includes both the predicted survival times and the observed data.</li>
<li><strong>Posterior Predictive Plot:</strong> The plot visualizes the posterior predictive distribution (blue lines), the observed data (solid black line), and the posterior predictive mean (dashed orange line).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">survival_posterior_predictive_weibull</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">survival_weibull_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">survival_posterior_samples_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_times_predicted_weibull</span> <span class="o">=</span> <span class="n">survival_posterior_predictive_weibull</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">survival_times_observed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_posterior_data_weibull</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_weibull</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                          <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">survival_times_predicted_weibull</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_68_0.png" type="" alt="png"  /></p>
<h3 id="plot-interpretation-for-weibull-posterior-predictive-vs-observed-data-uncensored">Plot Interpretation for Weibull Posterior Predictive vs. Observed Data (Uncensored)</h3>
<p>This plot shows a comparison between the <strong>Weibull posterior predictive distribution</strong> and the <strong>observed uncensored survival data</strong>, without any covariates involved. The purpose of this plot is to evaluate how well the Weibull model, fit to the uncensored data, can replicate the distribution of survival times.</p>
<h4 id="explanation-of-the-plot">Explanation of the Plot:</h4>
<ul>
<li>
<p><strong>X-axis (Survival Times):</strong></p>
<ul>
<li>The x-axis represents the survival times (in days), ranging from 0 to about 4500 days.</li>
</ul>
</li>
<li>
<p><strong>Y-axis (Density):</strong></p>
<ul>
<li>The y-axis shows the <strong>density</strong> of survival times, or the likelihood of individuals surviving for certain durations, based on both the observed data and the model&rsquo;s posterior predictions.</li>
</ul>
</li>
</ul>
<h4 id="lines-in-the-plot">Lines in the Plot:</h4>
<ul>
<li>
<p><strong>Blue Line (Weibull Posterior Predictive):</strong></p>
<ul>
<li>This line represents the predicted survival times from the Weibull model. It shows the expected distribution of survival times based on the model&rsquo;s posterior distribution.</li>
</ul>
</li>
<li>
<p><strong>Green Line (Weibull Observed Data):</strong></p>
<ul>
<li>This line represents the true observed distribution of uncensored survival times. It shows the actual survival time density for individuals who experienced the event (e.g., death).</li>
</ul>
</li>
</ul>
<h4 id="interpretation-1">Interpretation:</h4>
<ul>
<li>
<p><strong>Fit Between Lines:</strong></p>
<ul>
<li>The <strong>green line</strong> (observed data) and the <strong>blue line</strong> (posterior predictive) show a relatively good alignment in terms of overall trend, particularly in the early time periods (0 to 2000 days). The posterior predictive distribution follows the general trend of the observed data, which suggests that the Weibull model does an acceptable job at capturing the distribution of survival times in this range.</li>
</ul>
</li>
<li>
<p><strong>Discrepancies:</strong></p>
<ul>
<li>Similar to the lognormal model, we observe that the posterior predictive line fluctuates more in the earlier survival times, indicating higher uncertainty or variability in the predictions.</li>
<li>The fit becomes more diverging in the later periods (beyond 2500 days), where the posterior predictive distribution starts to exhibit higher peaks and troughs compared to the observed data, suggesting that the Weibull model may struggle to accurately capture extreme or late survival times.</li>
</ul>
</li>
<li>
<p><strong>Conclusion:</strong></p>
<ul>
<li>The Weibull model generally captures the trend of survival times, but as seen, there are some inconsistencies in the later survival times, where the model tends to overestimate or underestimate specific time periods. These fluctuations indicate that while the model is useful for capturing general trends, it might not fully account for more extreme cases of survival.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">[</span><span class="s1">&#39;posterior_predictive&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span> <span class="p">:</span> <span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Weibull Posterior Predictive&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">[</span><span class="s1">&#39;observed_data&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span> <span class="p">:</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Weibull Observed Data&#39;</span><span class="p">})</span>
</span></span></code></pre></div><pre><code>&lt;Axes: &gt;
</code></pre>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_70_1.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-posterior-predictive-check-plot">Interpretation of the Posterior Predictive Check Plot</h4>
<ul>
<li>
<p><strong>Blue Lines (Posterior Predictive Samples):</strong> These represent the survival times generated from the posterior distribution. The spread of the lines shows the model&rsquo;s uncertainty in predicting survival times based on the posterior samples.</p>
</li>
<li>
<p><strong>Solid Black Line (Observed Data):</strong> This represents the actual observed survival times.</p>
</li>
<li>
<p><strong>Dashed Orange Line (Posterior Predictive Mean):</strong> This shows the mean of the posterior predictive samples.</p>
</li>
</ul>
<p>In this case, the posterior predictive samples align well with the observed survival data, suggesting that the Weibull survival model provides a good fit to the data. The predicted survival times fall within a reasonable range around the observed data, and the posterior predictive mean closely follows the observed survival trend. This check confirms that the model has successfully learned from the data and is capable of generating realistic survival time predictions.</p>
<h3 id="posterior-summary-for-weibull-survival-model">Posterior Summary for Weibull Survival Model</h3>
<p>The summary table provides key statistics for the posterior distributions of the shape parameter ($k$) and scale parameter ($\lambda$) of the Weibull survival model. These values give us insights into the uncertainty and characteristics of the estimated parameters after fitting the model to the data.</p>
<h4 id="key-metrics">Key Metrics:</h4>
<ul>
<li>
<p><strong>Mean:</strong> The posterior mean of each parameter, representing the expected value based on the posterior distribution.</p>
<ul>
<li>$k$ (shape): <strong>48.25</strong> — This value indicates that the hazard rate increases rapidly over time.</li>
<li>$\lambda$ (scale): <strong>0.25</strong> — A small scale parameter, which suggests that the model predicts shorter survival times on average.</li>
</ul>
</li>
<li>
<p><strong>Standard Deviation (sd):</strong> The standard deviation of the posterior distribution, reflecting the uncertainty in the parameter estimates.</p>
<ul>
<li>$k$: <strong>7.99</strong></li>
<li>$\lambda$: <strong>0.017</strong></li>
</ul>
</li>
<li>
<p><strong>HDI 3% - 97%:</strong> The highest density interval (HDI) represents the 94% credible interval for the parameters, showing the range of likely values.</p>
<ul>
<li>$k$: <strong>34.08</strong> to <strong>63.65</strong></li>
<li>$\lambda$: <strong>0.219</strong> to <strong>0.283</strong></li>
</ul>
</li>
<li>
<p><strong>ESS (Effective Sample Size):</strong> Indicates the number of effectively independent samples in the posterior. High values of ESS mean that the MCMC chains mixed well, and the posterior is reliable.</p>
<ul>
<li>$k$: <strong>1451.0</strong></li>
<li>$\lambda$: <strong>1423.0</strong></li>
</ul>
</li>
<li>
<p><strong>$\hat{R}$ (r_hat):</strong> A diagnostic measure for MCMC convergence. Values close to <strong>1.0</strong> indicate good convergence, meaning the chains have reached the same distribution.</p>
<ul>
<li>Both $k$ and $\lambda$ have $\hat{R} = 1.0$, confirming that the model has converged.</li>
</ul>
</li>
</ul>
<p>The summary shows that the model has successfully converged, with high effective sample sizes and credible intervals that provide reasonable bounds on the parameter estimates. The increasing hazard rate (high $k$ value) suggests that the risk of death increases over time, while the small $\lambda$ value indicates relatively short survival times. This analysis gives us a good understanding of the survival patterns in the data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">survival_posterior_data_weibull</span><span class="p">)</span>
</span></span></code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>k</th>
      <td>48.249</td>
      <td>7.987</td>
      <td>34.077</td>
      <td>63.653</td>
      <td>0.209</td>
      <td>0.148</td>
      <td>1451.0</td>
      <td>1795.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>lam</th>
      <td>0.251</td>
      <td>0.017</td>
      <td>0.219</td>
      <td>0.283</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1423.0</td>
      <td>1762.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="43-weibull-survival-model-with-covariates">4.3 Weibull Survival Model With Covariates</h2>
<p>In this section, we will extend the Weibull survival model to account for individual-specific covariates. by introducing covariates, we allow the model to make &ldquo;personalized&rdquo; predictions for survival times, based on characteristics like <strong>age, bilirubin levels</strong> and <strong>albumin</strong>.</p>
<p>The scale parameter $\lambda_i$, which influences the expected survival time, will now depend on a <strong>combination of covariates (characteristics)</strong> for each individual. This makes the model more flexible and capable of capturing how different factors impact survival.</p>
<p>The scale parameter $\lambda_i$ for individual $i$ is modeled as:</p>
<p>$$\lambda_i = \exp \left( \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \dots + \beta_k \cdot x_{ki} \right)$$</p>
<p>Where:</p>
<ul>
<li>$ x_{1i}, x_{2i}, \dots, x_{ki} $ are the covariates for individual  i ,</li>
<li>$ \beta_0, \beta_1, \dots, \beta_k $ are the regression coefficients for each covariate.</li>
</ul>
<p>The likelihood function for the observed survival times $t_i$ is given by the Weibull distribution:</p>
<p>$$ f(t_i | k, \lambda_i) = \frac{k}{\lambda_i} \left( \frac{t_i}{\lambda_i}  \right)^{k-1} \exp \left(- \left(\frac{t_i}{\lambda_i}\right)^k\right) $$</p>
<p>Where:</p>
<ul>
<li>$k$ is the shape parameter,</li>
<li>$\lambda_i$ is the scale parameter that varies with covariates.</li>
</ul>
<p>By estimating the coefficients $\beta$, we can assess how each covariate influences survival time. This approach personalizes the model to each individual&rsquo;s characteristics, leading to more tailored predictions.</p>
<p>We will implement this in the following steps:</p>
<ol>
<li>Define the model where $\lambda_i$ is a function of covariates.</li>
<li>Prepare the data (covariates and survival times)</li>
<li>Perform MCMC sampling to fit the model</li>
<li>Analyze the results.</li>
</ol>
<h3 id="define-the-weibull-survival-model-with-covariates">Define the Weibull Survival Model with Covariates</h3>
<p>We will now define the model with priors for both the <strong>shape parameter</strong> $k$ and the <strong>regression coefficients</strong> $\beta$. The scale parameter $\lambda_i$ will be modeled as the exponential of a linear combination of the covariates.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">survival_model_weibull_with_covariates</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span><span class="n">covariates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>  <span class="c1"># coefficients (beta)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># linear model for log(lambda)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_lambda</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>      <span class="c1"># linear combination of covariates</span>
</span></span><span class="line"><span class="cl">    <span class="n">lambda_i</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_lambda</span><span class="p">)</span>              <span class="c1"># ensure scale parameter lambda is positive</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Weibull</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lambda_i</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="selecting-covariates-for-the-weibull-model">Selecting Covariates for the Weibull Model</h3>
<p>Before fitting the Weibull model with covariates, we need to ensure that the relevant covariates are selected from the dataset. These covariates will be used to personalize the scale parameter $\lambda_i$ for each individual, allowing the model to capture how various features influence survival times.</p>
<p>In this step, we simply print the list of top features, which were previously selected based on their correlation with the survival outcome. These features will serve as the covariates in the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</span></span></code></pre></div><pre><code>['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos']
</code></pre>
<p>To extend the Weibull model with covariates, we need to prepare two key components:</p>
<ol>
<li><strong>Covariates:</strong> The selected covariates (features) that will be used to predict survival times for each individual.</li>
<li><strong>Survival Times:</strong> The observed survival times for the uncensored data (i.e., where the event of interest has occurred).</li>
</ol>
<h4 id="code-explanation-5">Code Explanation</h4>
<ul>
<li><strong><code>covariate_selection</code>:</strong> We explicitly select a subset of covariates that are expected to have an influence on survival, including <strong>Bilirubin</strong>, <strong>Edema</strong>, <strong>Prothrombin</strong>, <strong>Albumin</strong>, and <strong>Age</strong>.</li>
<li><strong><code>covariates_survival</code>:</strong> We filter the dataset to include only individuals with observed survival times (where <code>Status == 1</code>) and extract the covariate values.</li>
<li><strong><code>survival_times</code>:</strong> We extract the corresponding survival times for these individuals.</li>
</ul>
<p>Finally, we print the top features and the selected covariates to verify that the correct data is being used for the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">covariate_selection</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;Edema&#39;</span><span class="p">,</span> <span class="s1">&#39;Prothrombin&#39;</span><span class="p">,</span> <span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">covariates_survival</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="n">covariate_selection</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">top_features</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">covariate_selection</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>['Bilirubin', 'Edema', 'Copper', 'Prothrombin', 'Albumin', 'Age', 'Alk_Phos']
['Bilirubin', 'Edema', 'Prothrombin', 'Albumin', 'Age']
</code></pre>
<h3 id="fitting-the-weibull-survival-model-with-covariates">Fitting the Weibull Survival Model with Covariates</h3>
<p>We now fit the Weibull survival model with covariates using <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong>. This model accounts for the effects of the selected covariates on the survival times, making the predictions more personalized for each individual.</p>
<h4 id="code-explanation-6">Code Explanation</h4>
<ul>
<li><strong>NUTS:</strong> We use the NUTS kernel to efficiently explore the posterior distribution of the parameters.</li>
<li><strong>MCMC Setup:</strong> The model is run with 1,000 warm-up iterations, followed by 2,000 sampling iterations, and using 4 chains to assess convergence.</li>
<li><strong>Covariates:</strong> The scale parameter $\lambda_i$ is modeled as a function of the selected covariates, allowing the model to account for individual characteristics when predicting survival times.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">nuts_kernel_survival_weibull_with_covariates</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">survival_model_weibull_with_covariates</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull_with_covariates</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">nuts_kernel_survival_weibull_with_covariates</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull_with_covariates</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                  <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="n">covariates</span> <span class="o">=</span> <span class="n">covariates_survival</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_weibull_with_covariates</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



  0%|          | 0/3000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
   beta[0]      0.07      0.04      0.07      0.03      0.12      2.96      1.70
   beta[1]      0.03      0.54     -0.24     -0.36      0.96      2.02      8.85
   beta[2]     -0.05      0.05     -0.06     -0.11      0.04      2.42      2.29
   beta[3]      0.12      0.39     -0.10     -0.14      0.80      2.00     20.11
   beta[4]     -0.19      0.30     -0.03     -0.72      0.01      2.01     14.35
         k    284.18    163.08    372.41      2.42    392.76       nan     15.33

Number of divergences: 2000
</code></pre>
<h3 id="results-interpretation">Results Interpretation:</h3>
<p>The output provides the posterior estimates for the regression coefficients ($\beta$) and the shape parameter ($k$), along with diagnostics.</p>
<ul>
<li>
<p><strong>$\beta$ coefficients:</strong></p>
<ul>
<li><strong>beta[0] (Bilirubin):</strong> Mean of <strong>0.07</strong>, indicating a small positive effect on survival time, though there is substantial uncertainty as indicated by the standard deviation (std).</li>
<li><strong>beta[1] to beta[4] (Other Covariates):</strong> The estimates for the remaining covariates (Edema, Prothrombin, Albumin, Age) show wide credible intervals and large $\hat{R}$ values, suggesting poor convergence and high uncertainty in the estimates.</li>
</ul>
</li>
<li>
<p><strong>Shape Parameter ($k$):</strong> The mean estimate for $k$ is <strong>284.18</strong>, with a high standard deviation, indicating extreme variability in the estimates. Additionally, the model encountered <strong>2,000 divergences</strong>, signaling serious issues with the MCMC sampling process.</p>
</li>
<li>
<p><strong>Convergence Issues:</strong></p>
<ul>
<li>The <strong>$\hat{R}$</strong> values for most of the parameters are far from <strong>1.0</strong>, indicating that the chains did not converge.</li>
<li><strong>Effective Sample Size (<code>n_eff</code>):</strong> Extremely low values for the effective sample size show that the sampler struggled to explore the posterior efficiently, producing highly correlated samples.</li>
<li><strong>Divergences:</strong> The model reported <strong>2,000 divergences</strong>, suggesting that the NUTS sampler encountered significant problems in exploring the posterior, which likely stems from poor model specification or highly informative priors.</li>
</ul>
</li>
</ul>
<p>The results indicate that the model failed to converge, likely due to the high number of divergences and poor effective sample sizes. These issues point to potential problems with the model&rsquo;s specification, such as overly complex priors or insufficient data to support the model&rsquo;s structure. In subsequent steps, it may be necessary to simplify the model, adjust the priors, or address potential multicollinearity in the covariates.</p>
<h2 id="44-introducing-the-log-normal-survival-model">4.4 Introducing the Log-Normal Survival Model</h2>
<p>After encountering issues with the Weibull model with covariates, we modify the model structure and introduce a <strong>log-normal survival model</strong> with covariates. This change in model specification can sometimes lead to better performance or convergence when the original model struggles with the data.</p>
<p>The <strong>log-normal distribution</strong> assumes that the logarithm of the survival times follows a normal distribution, which is another common choice for modeling time-to-event data. Unlike the Weibull distribution, the log-normal distribution may provide better flexibility when the hazard rate does not follow the typical monotonic increasing or decreasing pattern.</p>
<h4 id="code-explanation-7">Code Explanation</h4>
<ul>
<li>
<p><strong>Priors:</strong></p>
<ul>
<li>We place <strong>Normal(0, 1)</strong> priors on the regression coefficients $\beta$, which influence how each covariate impacts the survival time.</li>
<li>An <strong>Exponential(1.0)</strong> prior is assigned to $\sigma$, the standard deviation, to ensure positivity.</li>
</ul>
</li>
<li>
<p><strong>Log-Mu (Mean of Log-Survival Times):</strong></p>
<ul>
<li>The linear combination of covariates is modeled as $ \log(\mu_i) = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \dots + \beta_k \cdot x_{ki} $, where $x_{1i}, x_{2i}, \dots, x_{ki}$ are the covariates for individual $i$.</li>
<li>This represents the log of the expected survival time for each individual.</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong></p>
<ul>
<li>The observed survival times are modeled as being drawn from a <strong>LogNormal distribution</strong>, where the mean is $\log(\mu)$ and the standard deviation is $\sigma$.</li>
<li>This distribution models the uncertainty in the survival times, allowing the log-transformed times to follow a normal distribution, while ensuring that the predicted survival times themselves are positive.</li>
</ul>
</li>
</ul>
<p>This adjustment in the model may lead to improved performance, especially if the underlying survival times fit better with a log-normal assumption rather than a Weibull distribution.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lognormal_survival_model_with_covariates</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span><span class="n">covariates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>  <span class="c1"># coefficients (beta)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># linear model for log(mu)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_mu</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>      <span class="c1"># linear combination of covariates</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="n">log_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="fitting-the-log-normal-survival-model-with-covariates">Fitting the Log-Normal Survival Model with Covariates</h3>
<p>We now fit the <strong>log-normal survival model</strong> with covariates using the <strong>No-U-Turn Sampler (NUTS)</strong>. This model assumes that the logarithm of the survival times follows a normal distribution, and the scale parameter (or mean of the log-survival times) depends on a linear combination of the selected covariates.</p>
<h4 id="code-explanation-8">Code Explanation</h4>
<ul>
<li><strong>Feature Selection:</strong> We choose a new set of covariates based on their relevance to survival: <strong>Bilirubin, Edema, Copper, Prothrombin, Albumin, and Age</strong>.
<ul>
<li>These features are expected to influence survival times based on prior domain knowledge or their relationship with survival.</li>
</ul>
</li>
<li><strong>MCMC with NUTS:</strong> The log-normal model is fitted using MCMC with 1,000 warm-up iterations and 1,000 sampling iterations.
<ul>
<li>The NUTS sampler explores the posterior distribution of the model parameters, accounting for the covariates in predicting survival times.</li>
</ul>
</li>
</ul>
<p>By running this code, we estimate the posterior distributions of the regression coefficients ($\beta$) and the standard deviation ($\sigma$) for the log-normal survival model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">selection_features_lognormal</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;Edema&#39;</span><span class="p">,</span> <span class="s1">&#39;Copper&#39;</span><span class="p">,</span> <span class="s1">&#39;Prothrombin&#39;</span><span class="p">,</span> <span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">covariates_survival_lognormal</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="n">selection_features_lognormal</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_with_covariates_lognormal</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">lognormal_survival_model_with_covariates</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_warmup</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">num_chains</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                  <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">covariates</span> <span class="o">=</span> <span class="n">covariates_survival_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
   beta[0]      0.79      0.34      0.80      0.21      1.34   5897.04      1.00
   beta[1]      1.57      0.60      1.56      0.64      2.56   5230.58      1.00
   beta[2]      0.56      0.38      0.56     -0.07      1.18   6122.26      1.00
   beta[3]      0.60      0.41      0.60     -0.04      1.30   6428.30      1.00
   beta[4]      0.03      0.42      0.03     -0.70      0.68   5850.73      1.00
   beta[5]      1.23      0.46      1.23      0.45      1.95   5866.68      1.00
     sigma      5.96      0.33      5.94      5.39      6.47   6314.24      1.00

Number of divergences: 0
</code></pre>
<h4 id="results-interpretation-for-the-log-normal-survival-model-with-covariates">Results Interpretation for the Log-Normal Survival Model with Covariates</h4>
<p>The output provides the posterior estimates for the regression coefficients ($\beta$) and the standard deviation ($\sigma$) in the log-normal survival model. The model successfully converged with no divergences, and the effective sample sizes ($n_{\text{eff}}$) and $\hat{R}$ values indicate good convergence.</p>
<ul>
<li>
<p><strong>$\beta$ coefficients:</strong></p>
<ul>
<li><strong>beta[0] (Bilirubin):</strong> Mean of <strong>0.79</strong>, suggesting that higher Bilirubin levels are associated with a higher log-survival time, meaning longer survival on average.</li>
<li><strong>beta[1] (Edema):</strong> Mean of <strong>1.57</strong>, showing a relatively strong positive effect on survival. This indicates that patients with Edema have longer predicted survival times.</li>
<li><strong>beta[2] (Copper):</strong> Mean of <strong>0.56</strong>, suggesting that higher copper levels are also associated with longer survival times, though the effect size is smaller than for Bilirubin and Edema.</li>
<li><strong>beta[3] (Prothrombin):</strong> Mean of <strong>0.60</strong>, indicating a positive relationship between Prothrombin and survival time.</li>
<li><strong>beta[4] (Albumin):</strong> Mean of <strong>0.03</strong>, suggesting that the effect of Albumin on survival time is weak and uncertain, as evidenced by the wide credible interval.</li>
<li><strong>beta[5] (Age):</strong> Mean of <strong>1.23</strong>, suggesting that older age is associated with longer survival times, which might be counterintuitive but could reflect a complex interaction with other covariates in this dataset.</li>
</ul>
</li>
<li>
<p><strong>Standard Deviation ($\sigma$):</strong></p>
<ul>
<li>The mean estimate for $\sigma$ is <strong>5.96</strong>, indicating that there is considerable variability in the log-survival times that the model is capturing. The narrow credible interval (5.39 to 6.47) suggests that the uncertainty around this parameter is low.</li>
</ul>
</li>
<li>
<p><strong>Effective Sample Size ($n_{\text{eff}}$) and $\hat{R}$ values:</strong></p>
<ul>
<li>All parameters have high effective sample sizes, indicating good exploration of the posterior distribution during sampling.</li>
<li>The $\hat{R}$ values are all <strong>1.00</strong>, indicating that the MCMC chains have converged well.</li>
</ul>
</li>
</ul>
<p>The results indicate that the log-normal model has fit the data well, with all parameters showing good convergence and meaningful posterior estimates. The positive $\beta$ coefficients for most covariates suggest that these factors are positively correlated with survival times. However, further analysis might be needed to interpret some of the results, such as the positive association between age and survival.</p>
<p>Using a <strong>log-normal model</strong> may be more suitable than the Weibull model in this case because the log-normal distribution can capture more flexible hazard patterns, including non-monotonic hazard rates, which the Weibull model may struggle with. If the survival data exhibits more complex or non-linear relationships, the log-normal model can provide a better fit by allowing the log of survival times to follow a normal distribution, capturing a wider range of survival behaviors.</p>
<h3 id="posterior-trace-and-pair-plots-for-the-log-normal-survival-model">Posterior Trace and Pair Plots for the Log-Normal Survival Model</h3>
<p>After fitting the log-normal survival model with covariates, we use trace and pair plots to assess the quality of the MCMC sampling and the relationships between the parameters. These visualizations help verify that the posterior distributions are well-behaved and that the sampler has explored the parameter space efficiently.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">trace_survival_with_covariates_lognormal</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_survival_with_covariates_lognormal</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace_survival_with_covariates_lognormal</span><span class="p">,</span> <span class="n">divergences</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_90_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_90_1.png" type="" alt="png"  /></p>
<h4 id="trace-plot-interpretation">Trace Plot Interpretation</h4>
<ul>
<li>
<p><strong>Posterior Distributions (Left Panels):</strong></p>
<ul>
<li>The left panels show the posterior distributions for the regression coefficients ($\beta$) and the standard deviation ($\sigma$). All the distributions appear unimodal and well-defined, with little to no skewness, indicating that the model has produced meaningful estimates for each parameter.</li>
<li>Each coefficient corresponds to one of the covariates selected for the model, and $\sigma$ represents the standard deviation of the log-normal distribution.</li>
</ul>
</li>
<li>
<p><strong>MCMC Trace (Right Panels):</strong></p>
<ul>
<li>The right panels show the MCMC sampling traces. All traces exhibit good mixing and stability, meaning that the chains are not stuck and have thoroughly explored the parameter space. The lack of any visible trends suggests that the MCMC chains have converged well.</li>
</ul>
</li>
</ul>
<h4 id="pair-plot-interpretation">Pair Plot Interpretation</h4>
<ul>
<li>The pair plots show the joint distributions and scatter plots of the posterior samples for each pair of parameters.
<ul>
<li>The plots indicate minimal correlation between the parameters, as the scatter plots are roughly circular, suggesting that each parameter is being estimated independently of the others.</li>
<li>No significant divergences are visible in the scatter plots, confirming that the NUTS sampler did not encounter serious issues during sampling.</li>
</ul>
</li>
</ul>
<p>The trace and pair plots demonstrate that the MCMC sampling worked efficiently, with good mixing and convergence. The posterior distributions are well-defined, and the parameter estimates are reliable. Additionally, the pair plots show that there are no strong correlations between the covariates, supporting the validity of the model structure and the sampling process.</p>
<h3 id="posterior-predictive-check-for-log-normal-survival-model-with-covariates">Posterior Predictive Check for Log-Normal Survival Model with Covariates</h3>
<p>We now perform a <strong>posterior predictive check</strong> to evaluate how well the log-normal survival model with covariates predicts the observed survival times. This helps assess the quality of the model’s fit by comparing the predicted and observed data distributions.</p>
<h4 id="code-explanation-9">Code Explanation</h4>
<ul>
<li><strong>Posterior Predictive Samples:</strong> We generate posterior predictive survival times using the posterior samples of the regression coefficients ($\beta$) and standard deviation ($\sigma$). This allows us to simulate the model&rsquo;s predictions based on the observed data and the learned parameters.</li>
<li><strong>Posterior Predictive Plot:</strong> The plot shows the posterior predictive distribution (blue lines), the observed data (black line), and the posterior predictive mean (dashed orange line).</li>
</ul>
<h3 id="posterior-predictive-check-for-log-normal-survival-model-with-covariates-1">Posterior Predictive Check for Log-Normal Survival Model with Covariates</h3>
<p>We now perform a <strong>posterior predictive check</strong> to evaluate how well the log-normal survival model with covariates predicts the observed survival times. This helps assess the quality of the model’s fit by comparing the predicted and observed data distributions.</p>
<h4 id="code-explanation-10">Code Explanation</h4>
<ul>
<li><strong>Posterior Predictive Samples:</strong> We generate posterior predictive survival times using the posterior samples of the regression coefficients ($\beta$) and standard deviation ($\sigma$). This allows us to simulate the model&rsquo;s predictions based on the observed data and the learned parameters.</li>
<li><strong>Posterior Predictive Plot:</strong> The plot shows the posterior predictive distribution (blue lines), the observed data (black line), and the posterior predictive mean (dashed orange line).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">survival_posterior_samples_lognormal</span> <span class="o">=</span> <span class="n">mcmc_survival_with_covariates_lognormal</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_posterior_predictive_lognormal_with_covariates</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lognormal_survival_model_with_covariates</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">survival_posterior_samples_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_times_predicted_lognormal_with_covariates</span> <span class="o">=</span> <span class="n">survival_posterior_predictive_lognormal_with_covariates</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                                                                                                    <span class="n">data</span> <span class="o">=</span> <span class="n">survival_times_observed</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                                                    <span class="n">covariates</span> <span class="o">=</span> <span class="n">covariates_survival_lognormal</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">survival_posterior_data_lognormal_with_covariates</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_survival_with_covariates_lognormal</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                          <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">survival_times_predicted_lognormal_with_covariates</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">survival_posterior_data_lognormal_with_covariates</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">group</span> <span class="o">=</span> <span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="kc">True</span><span class="p">);</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_94_0.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-posterior-predictive-plot-1">Interpretation of the Posterior Predictive Plot</h4>
<ul>
<li>
<p><strong>Blue Lines (Posterior Predictive Samples):</strong> These represent the model’s predictions for survival times based on the posterior distribution. The spread of the lines illustrates the uncertainty in the predictions.</p>
</li>
<li>
<p><strong>Solid Black Line (Observed Data):</strong> This represents the actual observed survival times.</p>
</li>
<li>
<p><strong>Dashed Orange Line (Posterior Predictive Mean):</strong> The dashed line shows the mean of the posterior predictive samples.</p>
</li>
</ul>
<p>The posterior predictive samples align well with the observed data, suggesting that the log-normal survival model with covariates captures the underlying survival patterns in the data. The predicted survival times fall within a reasonable range around the observed data, and the posterior predictive mean closely follows the trend of the observed survival times. This result indicates that the model has fit the data well and is capable of generating realistic survival time predictions.</p>
<h3 id="plot-interpretation-for-lognormal-posterior-predictive-vs-observed-data">Plot Interpretation for Lognormal Posterior Predictive vs. Observed Data</h3>
<p>In this step, we visualize the posterior predictive check for the <strong>lognormal model with covariates</strong> using kernel density estimation (KDE) plots. These plots help us understand how well the model captures the distribution of the survival data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_lognormal_with_covariates</span><span class="p">[</span><span class="s1">&#39;posterior_predictive&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span> <span class="p">:</span> <span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Lognormal Posterior Predictive&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">survival_posterior_data_lognormal_with_covariates</span><span class="p">[</span><span class="s1">&#39;observed_data&#39;</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span> <span class="p">:</span> <span class="s1">&#39;Observed Data&#39;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_97_0.png" type="" alt="png"  /></p>
<h4 id="explanation-of-the-plot-1">Explanation of the Plot:</h4>
<ul>
<li>
<p><strong>X-axis (Survival Times):</strong></p>
<ul>
<li>The x-axis represents the <strong>survival times</strong> (in days), ranging from 0 to about 4500 days.</li>
</ul>
</li>
<li>
<p><strong>Y-axis (Density):</strong></p>
<ul>
<li>The y-axis represents the <strong>density</strong> of survival times, or how frequently individuals survive for certain durations based on the observed data and model predictions.</li>
</ul>
</li>
</ul>
<h4 id="lines-in-the-plot-1">Lines in the Plot:</h4>
<ul>
<li>
<p><strong>Blue Line (Lognormal Posterior Predictive Uncensored):</strong></p>
<ul>
<li>This line represents the model&rsquo;s prediction of uncensored survival times using the <strong>lognormal distribution</strong>. It shows how the model expects individuals to survive over time, based on the posterior samples.</li>
</ul>
</li>
<li>
<p><strong>Green Line (Observed Data):</strong></p>
<ul>
<li>This line represents the <strong>actual observed data</strong> of uncensored survival times. It shows the true distribution of survival times for individuals where the event (e.g., death) has occurred.</li>
</ul>
</li>
</ul>
<h4 id="interpretation-2">Interpretation:</h4>
<ul>
<li>
<p><strong>Alignment of the Lines:</strong></p>
<ul>
<li>The blue and green lines show some overlap, especially in the range from <strong>500 to 2000 days</strong>. This indicates that the model has captured the trend of survival times relatively well in this region.</li>
</ul>
</li>
<li>
<p><strong>Discrepancies at the Extremes:</strong></p>
<ul>
<li>The blue line fluctuates more at the start of the timeline (before 1000 days) and at the far right (after 3000 days). This suggests that the model has higher variability in predicting shorter survival times and potentially struggles to match the observed data precisely in those ranges.</li>
</ul>
</li>
<li>
<p><strong>Overall Fit:</strong></p>
<ul>
<li>Despite some variability, the general trend of the posterior predictive distribution (blue) aligns with the observed data (green). This indicates that the <strong>lognormal model with covariates</strong> is reasonably accurate in predicting uncensored survival times but could be improved in certain time ranges, especially for early and late survival times.</li>
</ul>
</li>
</ul>
<p>By including this plot, we visually validate that the <strong>lognormal model</strong> captures key aspects of the survival data, though some refinement may be needed for more extreme cases.</p>
<h2 id="45-log-normal-survival-model-with-covariates-and-censored-data">4.5 Log-Normal Survival Model with Covariates and Censored Data</h2>
<p>In this final part, we extend the log-normal survival model to account for <strong>censored data</strong>. In survival analysis, <strong>censoring</strong> occurs when we do not observe the exact event time for some individuals. For example, if a patient is still alive at the end of the study or drops out before experiencing the event (death), their survival time is <strong>right-censored</strong>—we only know that the event did not occur before a certain time.</p>
<h3 id="modeling-censored-data">Modeling Censored Data</h3>
<p>To account for censored data, we modify the likelihood function to handle both <strong>observed</strong> and <strong>censored</strong> survival times. Specifically, for censored data, the likelihood is based on the <strong>survival function</strong> rather than the probability density function (PDF).</p>
<h3 id="likelihood-for-censored-and-observed-data">Likelihood for Censored and Observed Data</h3>
<p>For observed survival times $t_i$ (i.e., the event occurred):
$$ f(t_i | \mu_i, \sigma) = \frac{1}{t_i \sigma \sqrt{2\pi}} \exp\left(-\frac{(\log t_i - \mu_i)^2}{2\sigma^2}\right) $$</p>
<p>For censored survival times $t_i$ (i.e., the event did not occur by time $t_i$):
$$ S(t_i | \mu_i, \sigma) = 1 - F(t_i | \mu_i, \sigma) $$</p>
<p>Where:</p>
<ul>
<li>$f(t_i | \mu_i, \sigma)$ is the <strong>log-normal probability density function</strong> (PDF) for observed data,</li>
<li>$S(t_i | \mu_i, \sigma)$ is the <strong>survival function</strong>, representing the probability of survival beyond time $t_i$ for censored data,</li>
<li>$F(t_i | \mu_i, \sigma)$ is the <strong>cumulative distribution function</strong> (CDF) of the log-normal distribution,</li>
<li>$\mu_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki}$ is the mean of the log-survival times for individual $i$, based on their covariates,</li>
<li>$\sigma$ is the standard deviation of the log-survival times.</li>
</ul>
<h3 id="handling-censoring">Handling Censoring</h3>
<p>In the likelihood, we condition on whether the data is censored or not:</p>
<ul>
<li>For <strong>observed data</strong>, we use the PDF of the log-normal distribution.</li>
<li>For <strong>censored data</strong>, we use the survival function (1 minus the CDF).</li>
</ul>
<p>By incorporating both observed and censored survival times, this model can better capture the full picture of the dataset, including individuals who did not experience the event during the study period.</p>
<h3 id="steps-for-the-model">Steps for the Model:</h3>
<ol>
<li>Define the log-normal survival model with covariates, including censored data.</li>
<li>Prepare the dataset, distinguishing between censored and uncensored observations.</li>
<li>Fit the model using MCMC and analyze the results.</li>
</ol>
<h3 id="defining-the-log-normal-survival-model-with-censored-data">Defining the Log-Normal Survival Model with Censored Data</h3>
<p>In this model, we incorporate <strong>censored data</strong> into the log-normal survival model with covariates. Censoring allows us to handle cases where the exact survival time is unknown because the event (e.g., death) has not yet occurred by the end of the observation period.</p>
<h4 id="code-explanation-11">Code Explanation</h4>
<ul>
<li>
<p><strong>Priors:</strong></p>
<ul>
<li><strong>$\sigma$ (standard deviation parameter):</strong> We place an <strong>Exponential(1.0)</strong> prior on $\sigma$. This parameter controls the spread of the log-normal distribution for survival times, representing the variability in the log-transformed survival times.</li>
<li><strong>$\beta$ (covariate coefficients):</strong> If covariates are provided, we define a normal prior on $\beta$, which adjusts the mean of the log-survival times $\mu_i$ for each individual based on their characteristics.</li>
</ul>
</li>
<li>
<p><strong>Handling Censored Data:</strong></p>
<ul>
<li>We use a <strong>mask</strong> to differentiate between uncensored (event occurred) and censored (event did not occur) data.</li>
<li>For <strong>uncensored data</strong>, the likelihood is based on the <strong>log-normal distribution</strong>: $f(t_i | \mu_i, \sigma)$, where $\mu_i$ is the mean log-survival time (a linear combination of the covariates).</li>
<li>For <strong>censored data</strong>, the likelihood is based on the <strong>survival function</strong> of the log-normal distribution: $S(t_i | \mu_i, \sigma) = 1 - F(t_i | \mu_i, \sigma)$, where $F(t_i | \mu_i, \sigma)$ is the cumulative distribution function (CDF) of the log-normal distribution.</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong></p>
<ul>
<li>We define the likelihood separately for uncensored and censored data:
<ul>
<li>For <strong>uncensored data</strong>, we use the <strong>log-normal distribution</strong> to model the observed survival times.</li>
<li>For <strong>censored data</strong>, we use a Bernoulli likelihood to model the survival probabilities, representing the probability that the event has not yet occurred by the observed time. This is based on the <strong>survival function</strong> of the log-normal distribution.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>By integrating both the <strong>observed events</strong> and <strong>censored data</strong>, this model more accurately reflects the reality of survival analysis, where not all individuals experience the event within the study period. The log-normal model offers flexibility, particularly for data where the hazard rate may change over time, making it a more suitable choice for complex survival data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">survival_model_lognormal_with_censored</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">survival_times</span><span class="p">,</span> <span class="n">event_occurred</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># define priors</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>  <span class="c1"># standard deviation for the log-normal distribution</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">covariates</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">([</span><span class="n">covariates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>  <span class="c1"># regression coefficients</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_mu</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">covariates</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>  <span class="c1"># linear model for log(mu)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_mu</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># if no covariates, set log(mu) to 0 (default)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># likelihood - handle based on censoring status</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">survival_times</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># uncensored data (event occurred, i.e. event_occurred == 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">uncensored_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">event_occurred</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs_uncensored&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="n">log_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">uncensored_mask</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">survival_times</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># censored data (event did not occur, i.e. event_occurred == 0)</span>
</span></span><span class="line"><span class="cl">        <span class="n">censored_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">event_occurred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">survival_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="n">log_mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">survival_times</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs_censored&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">survival_prob</span><span class="p">)</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">censored_mask</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">event_occurred</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="preparing-the-data-for-the-log-normal-survival-model-with-censored-data">Preparing the Data for the Log-Normal Survival Model with Censored Data</h3>
<p>To fit the log-normal survival model with censored data, we need to prepare the following components:</p>
<ul>
<li>
<p><strong>Survival Times (<code>survival_times</code>):</strong> This represents the number of days each patient survived. These are the time-to-event data, which include both censored and uncensored observations.</p>
</li>
<li>
<p><strong>Event Occurred (<code>event_occurred</code>):</strong> This variable indicates whether the event (death) occurred or not. We create a binary indicator where <strong>1</strong> represents that the event occurred (i.e., death), and <strong>0</strong> represents that the data is censored (i.e., the patient was alive at the end of the study).</p>
</li>
<li>
<p><strong>Covariates (<code>covariates_survival</code>):</strong> We select a set of covariates that will be used to model the individual-specific survival times. In this case, we choose <code>'Bilirubin'</code>, <code>'Edema'</code>, <code>'Copper'</code>, <code>'Prothrombin'</code>, <code>'Albumin'</code>, and <code>'Age'</code> as the covariates, which are expected to influence the survival times, as we had already observed in EDA.</p>
</li>
</ul>
<h4 id="code-explanation-12">Code Explanation:</h4>
<ul>
<li><strong><code>survival_times</code>:</strong> Extracts the survival times (in days) from the dataset.</li>
<li><strong><code>event_occurred</code>:</strong> Converts the status variable into a binary format where 1 means the event occurred, and 0 means the data is censored.</li>
<li><strong><code>covariates_survival</code>:</strong> Selects the covariates of interest, which are used to predict survival times.</li>
</ul>
<p>This step ensures that we have the necessary data formatted correctly to fit the survival model, accounting for both censored and uncensored data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prepare the data</span>
</span></span><span class="line"><span class="cl"><span class="n">survival_times</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;N_Days&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># the survival times (days)</span>
</span></span><span class="line"><span class="cl"><span class="n">event_occurred</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Status&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># 1 if event (death), 0 if censored</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># select your covariates</span>
</span></span><span class="line"><span class="cl"><span class="n">covariates_survival</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Bilirubin&#39;</span><span class="p">,</span> <span class="s1">&#39;Edema&#39;</span><span class="p">,</span> <span class="s1">&#39;Copper&#39;</span><span class="p">,</span> <span class="s1">&#39;Prothrombin&#39;</span><span class="p">,</span> <span class="s1">&#39;Albumin&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>  
</span></span></code></pre></div><h3 id="fitting-the-weibull-survival-model-with-censored-data">Fitting the Weibull Survival Model with Censored Data</h3>
<p>We now fit the <strong>Weibull survival model</strong> with covariates and censored data using <strong>Markov Chain Monte Carlo (MCMC)</strong> with the <strong>No-U-Turn Sampler (NUTS)</strong>. The model accounts for both censored and uncensored survival times, making it suitable for real-world survival analysis where not all events occur within the observation window.</p>
<h4 id="code-explanation-13">Code Explanation</h4>
<ul>
<li><strong>NUTS Sampler:</strong> We use the NUTS kernel to efficiently explore the posterior distribution of the parameters.</li>
<li><strong>MCMC Setup:</strong> The model is run with 1,000 warm-up iterations and 1,000 sampling iterations, with 4 independent chains to ensure proper convergence and mixing.</li>
<li><strong>Input Data:</strong>
<ul>
<li><strong>Covariates:</strong> The selected covariates are passed to the model.</li>
<li><strong>Survival Times:</strong> The observed or censored survival times are provided to the model.</li>
<li><strong>Event Occurred:</strong> This binary indicator differentiates between censored (0) and uncensored (1) data.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># fit the Weibull model with censored data</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_lognormal_censored</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">NUTS</span><span class="p">(</span><span class="n">survival_model_lognormal_with_censored</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                             <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_lognormal_censored</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">covariates</span><span class="o">=</span><span class="n">covariates_survival</span><span class="p">,</span> <span class="n">survival_times</span><span class="o">=</span><span class="n">survival_times</span><span class="p">,</span> <span class="n">event_occurred</span><span class="o">=</span><span class="n">event_occurred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mcmc_lognormal_censored</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



  0%|          | 0/2000 [00:00&lt;?, ?it/s]



                mean       std    median      5.0%     95.0%     n_eff     r_hat
   beta[0]      0.91      0.30      0.91      0.44      1.42   5068.22      1.00
   beta[1]      1.32      0.52      1.31      0.50      2.20   5117.46      1.00
   beta[2]      0.61      0.32      0.61      0.11      1.14   6312.73      1.00
   beta[3]      0.68      0.34      0.68      0.09      1.20   6077.50      1.00
   beta[4]     -0.02      0.35     -0.02     -0.62      0.52   6067.08      1.00
   beta[5]      1.16      0.35      1.16      0.59      1.74   6004.50      1.00
     sigma      5.25      0.24      5.24      4.84      5.62   6263.06      1.00

Number of divergences: 0
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"></code></pre></div><h3 id="results-interpretation-for-the-log-normal-survival-model-with-censored-data">Results Interpretation for the Log-Normal Survival Model with Censored Data</h3>
<p>The output provides the posterior estimates for the <strong>regression coefficients</strong> ($\beta$) and the <strong>standard deviation ($\sigma$)</strong> for the log-normal survival model with censored data. The model has converged well, as indicated by the <strong>$\hat{R}$ values</strong> and <strong>effective sample sizes ($n_{\text{eff}}$)</strong>, with no divergences reported during sampling.</p>
<ul>
<li>
<p><strong>$\beta$ Coefficients (Covariates):</strong></p>
<ul>
<li><strong>beta[0] (Bilirubin):</strong> Mean of <strong>0.91</strong>, indicating a positive association between Bilirubin levels and survival time. Higher Bilirubin levels are associated with increased survival times, and the 90% credible interval does not cross zero, suggesting a strong effect.</li>
<li><strong>beta[1] (Albumin):</strong> Mean of <strong>1.32</strong>, showing a significant positive effect of Albumin on survival time. Higher Albumin levels appear to correlate with longer survival times, and the effect is pronounced.</li>
<li><strong>beta[2] (Stage):</strong> Mean of <strong>0.61</strong>, suggesting a positive effect of Stage on survival time. The 90% credible interval remains above zero, implying that individuals at higher stages have slightly increased survival times, though the effect size is moderate.</li>
<li><strong>beta[3] and beta[4]:</strong> These coefficients represent additional covariates in the model. Both show some positive effects, with <strong>beta[3]</strong> having a mean of <strong>0.68</strong> and <strong>beta[4]</strong> showing a very slight negative effect with a mean of <strong>-0.02</strong>. However, the credible interval for <strong>beta[4]</strong> crosses zero, indicating less certainty about this effect.</li>
<li><strong>beta[5]:</strong> A mean of <strong>1.16</strong>, suggesting another strong positive association with survival time, with a credible interval that does not cross zero.</li>
</ul>
</li>
<li>
<p><strong>Standard Deviation ($\sigma$):</strong></p>
<ul>
<li>The parameter <strong>$\sigma$</strong> has a mean of <strong>5.25</strong>, indicating the variability in the log-transformed survival times. This suggests that there is a moderate amount of variance in the log-survival times across individuals, likely due to the influence of covariates.</li>
</ul>
</li>
<li>
<p><strong>Effective Sample Size ($n_{\text{eff}}$) and $\hat{R}$ Values:</strong></p>
<ul>
<li>The effective sample sizes are large, suggesting good mixing of the chains.</li>
<li>All $\hat{R}$ values are equal to <strong>1.00</strong>, confirming that the model has converged correctly.</li>
</ul>
</li>
</ul>
<p>The log-normal survival model with censored data has fit the data well, with meaningful estimates for the regression coefficients and the standard deviation. The positive effects of <strong>Bilirubin</strong> and <strong>Albumin</strong> are particularly strong, suggesting these covariates play a key role in predicting survival times. Additionally, the inclusion of censored data has improved the model&rsquo;s robustness by accounting for individuals whose survival times are unknown by the end of the study.</p>
<h3 id="posterior-predictive-check-for-the-log-normal-survival-model-with-censored-data">Posterior Predictive Check for the Log-Normal Survival Model with Censored Data</h3>
<p>The final step involves performing a <strong>posterior predictive check (PPC)</strong> to evaluate how well the log-normal survival model with censored data fits the observed data. This process involves generating predictions from the model based on the posterior samples and comparing them to the actual observed data.</p>
<h4 id="code-explanation-14">Code Explanation</h4>
<ul>
<li>
<p><strong>Posterior Predictive Samples:</strong></p>
<ul>
<li>We use the <code>Predictive</code> class to sample from the posterior distribution of the model parameters (obtained from the MCMC) and generate new predicted survival times.</li>
</ul>
</li>
<li>
<p><strong>Inference Data:</strong></p>
<ul>
<li>The generated posterior predictive samples are converted into <strong>InferenceData</strong> format using <code>ArviZ</code> to facilitate easy analysis and visualization.</li>
</ul>
</li>
<li>
<p><strong>Trace Plot:</strong></p>
<ul>
<li>The trace plot visualizes the posterior distributions of the model parameters and the convergence of the MCMC chains.</li>
</ul>
</li>
<li>
<p><strong>Posterior Predictive Check (PPC):</strong></p>
<ul>
<li>The PPC compares the predicted survival times (both censored and uncensored) with the observed survival times. This allows us to assess the model&rsquo;s fit by evaluating how closely the predicted values match the actual data.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># posterior predictive check for the Weibull model</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_samples_lognormal_censored</span> <span class="o">=</span> <span class="n">mcmc_lognormal_censored</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_predictive_lognormal_censored</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">survival_model_lognormal_with_censored</span><span class="p">,</span> <span class="n">posterior_samples_lognormal_censored</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_predictive_samples_lognormal_censored</span> <span class="o">=</span> <span class="n">posterior_predictive_lognormal_censored</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                                                                                      <span class="n">covariates</span><span class="o">=</span><span class="n">covariates_survival</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                                      <span class="n">survival_times</span><span class="o">=</span><span class="n">survival_times</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                                                                      <span class="n">event_occurred</span><span class="o">=</span><span class="n">event_occurred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># convert to InferenceData and plot PPC</span>
</span></span><span class="line"><span class="cl"><span class="n">az_data_lognormal_censored</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc_lognormal_censored</span><span class="p">,</span> <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive_samples_lognormal_censored</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_survival_with_covariates_lognormal</span><span class="p">,</span> <span class="n">compact</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">az_data_lognormal_censored</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&#34;posterior&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_110_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_110_1.png" type="" alt="png"  /></p>
<h4 id="interpretation-of-the-results">Interpretation of the Results</h4>
<ol>
<li>
<p><strong>Trace Plot:</strong></p>
<ul>
<li>The trace plot shows well-mixed chains for each parameter (the $\beta$ coefficients and $\sigma$). The chains appear to have converged properly, with no visible signs of autocorrelation or poor mixing, further confirmed by the high effective sample sizes and $\hat{R}$ values from the summary.</li>
</ul>
</li>
<li>
<p><strong>Posterior Predictive Plot:</strong></p>
<ul>
<li>The PPC plot shows the predicted survival times (for both censored and uncensored data) overlaid with the actual observed data. The <strong>uncensored data</strong> (right plot) shows that the model does a reasonable job of capturing the distribution of the observed survival times, with the <strong>posterior predictive mean</strong> aligning well with the data.</li>
<li>For the <strong>censored data</strong> (left plot), the model also appears to fit well, indicating that the log-normal model appropriately captures the uncertainty introduced by censoring.</li>
</ul>
</li>
</ol>
<p>The posterior predictive check indicates that the log-normal model with censored data provides a good fit to the data. The model successfully handles both censored and uncensored survival times, making accurate predictions for the observed survival times. This confirms the model&rsquo;s robustness in dealing with time-to-event data in the presence of censoring.</p>
<p>The two plots presented below show the posterior predictive checks (PPC) for both <strong>uncensored</strong> and <strong>censored</strong> data from our log-normal survival model with censored observations.</p>
<h3 id="uncensored-data-plot">Uncensored Data Plot:</h3>
<p>The first plot compares the <strong>observed uncensored data</strong> (green line) with the <strong>posterior predictive distribution</strong> (blue line).</p>
<ul>
<li><strong>Observed Uncensored Data</strong>: These are individuals for whom the event (e.g., death) has occurred within the study period, and the exact survival times are known.</li>
<li><strong>Posterior Predictive Uncensored</strong>: The blue line represents the model&rsquo;s predicted distribution for these uncensored individuals, given the posterior distribution of parameters.</li>
</ul>
<p>In this plot, we observe a reasonable fit between the observed and predicted distributions. The general shape of both lines aligns, with some differences in fluctuation, which could be attributed to noise or model limitations. The model captures the overall distribution of survival times for individuals who experienced the event.</p>
<h3 id="censored-data-plot">Censored Data Plot:</h3>
<p>The second plot compares the <strong>observed censored data</strong> (green line) with the <strong>posterior predictive distribution</strong> (blue line).</p>
<ul>
<li><strong>Observed Censored Data</strong>: This represents individuals whose event did not occur by the end of the study period, so we only know they survived at least until a certain time. The x-axis shows the proportion of time for which the event had not occurred (between 0 and 1).</li>
<li><strong>Posterior Predictive Censored</strong>: The blue line shows the predicted distribution for censored individuals based on the posterior samples from the model.</li>
</ul>
<p>The fit is close, with both lines following similar trends. The model correctly predicts that most censored individuals have survival probabilities close to either 0 or 1, reflecting the individuals who were not likely to experience the event by the end of the study. This shows that the model is appropriately accounting for the censored data.</p>
<p>Overall, both PPC plots demonstrate that the model is capable of predicting survival times effectively, accounting for both censored and uncensored data, and matching the general distribution patterns in the dataset.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;observed_data&#34;</span><span class="p">][</span><span class="s2">&#34;obs_uncensored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C2&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Observed Data Uncensored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;posterior_predictive&#34;</span><span class="p">][</span><span class="s2">&#34;obs_uncensored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C0&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Posterior Predictive Uncensored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;observed_data&#34;</span><span class="p">][</span><span class="s2">&#34;obs_censored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C2&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Observed Data Censored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">az_data_lognormal_censored</span><span class="p">[</span><span class="s2">&#34;posterior_predictive&#34;</span><span class="p">][</span><span class="s2">&#34;obs_censored&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;color&#34;</span><span class="p">:</span> <span class="s2">&#34;C0&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">:</span> <span class="s2">&#34;Posterior Predictive Censored&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_113_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240924_NumPyro_LogReg_Surv_Analysis/output_113_1.png" type="" alt="png"  /></p>
<h1 id="5-conclusion">5. Conclusion</h1>
<p>In this notebook, we conducted a comprehensive analysis using <strong>Bayesian survival modeling</strong> techniques to predict survival times in a dataset of cirrhosis patients. We began with exploratory data analysis (EDA), examining key variables and correlations to gain an understanding of the dataset. We then moved to <strong>classification modeling</strong> using a <strong>Bayesian logistic regression model</strong> to predict survival status, where we explored the relationships between covariates and the binary outcome.</p>
<p>Afterwards, we shifted our focus to <strong>survival analysis</strong>. We first implemented a <strong>Weibull survival model</strong> to model survival times based solely on the observed uncensored data. We explored the shortcomings of this model, particularly when considering the flexibility of the data. Subsequently, we introduced a <strong>log-normal survival model</strong>, which proved more appropriate for capturing the distribution of survival times.</p>
<p>To account for individual-specific factors, we introduced <strong>covariates</strong> into the survival models, enabling more personalized predictions based on factors such as <strong>bilirubin levels</strong>, <strong>age</strong>, and <strong>albumin</strong>. This allowed us to model survival times as a function of these covariates, further improving the predictive power of the model.</p>
<p>Finally, we tackled the challenge of <strong>censored data</strong>, which occurs when the exact event time is unknown for some individuals. We adjusted the <strong>log-normal model</strong> to account for both censored and uncensored observations, demonstrating that Bayesian modeling can handle these complexities effectively. We used <strong>posterior predictive checks (PPC)</strong> to validate the models and showed that the predicted survival times align well with the observed data, both for censored and uncensored cases.</p>
<p>Through this journey, we demonstrated how <strong>Bayesian inference</strong> and <strong>probabilistic programming</strong> provide powerful tools for survival analysis. By leveraging <strong>MCMC sampling</strong>, we were able to estimate the posterior distributions of key parameters and perform robust predictions, while accounting for the uncertainty inherent in the data. Overall, we illustrated the flexibility and capability of Bayesian methods for complex survival analysis tasks.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Transfer Learning Classifier Again... with Julia!</title>
      <link>http://localhost:1313/posts/20240521_julia_transfer_learning_v5/20240521_julia_transfer_learning_v5/</link>
      <pubDate>Tue, 21 May 2024 22:38:29 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240521_julia_transfer_learning_v5/20240521_julia_transfer_learning_v5/</guid>
      <description>Replicating the cat mood classifier, this time using Julia and Flux.jl.</description>
      <content:encoded><![CDATA[<hr>
<p><img loading="lazy" src="/images/20240521_julia_transfer_learning_v5/intro.png" type="" alt="image"  /></p>
<h2 id="introduction">Introduction</h2>
<p>This guide demonstrates how to apply transfer learning using a pre-trained vision model to classify cat moods based on their facila expressions. We&rsquo;ll learn how to handle custom data setups.</p>
<p>In this demonstration, we recreate the exercise done in PyTorch, <a href="https://vflores-io.github.io/posts/20240515_cat_mood_classification/">available here</a>. Since that demonstration is quite detailed, we keep it pretty straightforward here.</p>
<h4 id="motivation--credit">Motivation &amp; Credit</h4>
<p>When I thought about learning how to implement a computer vision classification model for transfer learning in Julia and <code>Flux</code>, I immediately came upon two roadblocks:</p>
<ol>
<li>Since I am not an expert in Julia, I found the documentation to be a bit difficult to access (again, this is just me!).</li>
<li>There are not many tutorials or resources to illustrate this particular case.</li>
</ol>
<p>Therefore I took it upon myself to put things together and make a demonstration that would hopefully be useful for someone who might not be an expert in Flux (or Julia).</p>
<p>This particular demo was inspired by a combination of the following resources:</p>
<ul>
<li><a href="https://towardsdatascience.com/transfer-learning-and-twin-network-for-image-classification-using-flux-jl-cbe012ced146">Transfer Learning and Twin Network for Image Classification using <code>Flux.jl</code></a></li>
<li><a href="https://github.com/FluxML/model-zoo/tree/master/tutorials/transfer_learning"><code>Flux.jl</code>&rsquo;s Model Zoo Tutorial</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"><code>PyTorch</code> Transfer Learning for Computer Vision Tutorial</a></li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<p>We will use a pre-trained <code>ResNet18</code> model, initially trained on a general dataset, and fine-tune it for our specific task of classifying cat moods.</p>
<h3 id="initialization">Initialization</h3>
<p>First, we activate the current directory as our project environment by calling the package manager <code>Pkg</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Pkg</span>
</span></span><span class="line"><span class="cl"><span class="n">Pkg</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="s">&#34;.&#34;</span><span class="p">)</span> 
</span></span></code></pre></div><p>Then we will import the required packages. Of course, this is also assuming that one has already added the relevant packages into the environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Pkg</span>
</span></span><span class="line"><span class="cl"><span class="n">Pkg</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="s">&#34;.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Random</span><span class="o">:</span> <span class="n">shuffle!</span>
</span></span><span class="line"><span class="cl"><span class="k">import</span> <span class="n">Base</span><span class="o">:</span> <span class="n">length</span><span class="p">,</span> <span class="n">getindex</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Images</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Flux</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Flux</span><span class="o">:</span> <span class="n">update!</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">DataAugmentation</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Metalhead</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">MLUtils</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">DataFrames</span><span class="p">,</span> <span class="n">CSV</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Plots</span>
</span></span></code></pre></div><pre><code>[32m[1m  Activating[22m[39m project at `H:\My Drive\Projects\Coding\Portfolio\Machine Learning\Julia\Transfer Learning with Flux`
</code></pre>
<h3 id="retrieve-the-data-and-initial-setup">Retrieve the Data and Initial Setup</h3>
<p>First, we specify the paths to the dataset and labels CSV files for training, validation, and test sets. Then, we load these CSV files into <code>DataFrames</code>. Finally, we create vectors of absolute file paths for each image in the dataset.</p>
<p>This setup is essential for organizing the data and ensuring that our model can access the correct images and labels during training and evaluation.</p>
<h4 id="label-structure">Label Structure</h4>
<p>The data set we are using consists of three folders: <code>train</code>, <code>val</code>, <code>test</code>. Each of them contain a set of images of cats. The labels in this case, are in the form of a CSV file that maps the filename with a one-hot encoding to label the classification of the image, i.e. the cat&rsquo;s mood - alarmed, angry, calm, pleased.</p>
<p>The dataset was obtained <a href="https://universe.roboflow.com/mubbarryz/domestic-cats-facial-expressions">here</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># specify the paths to the dataset and labels CSV</span>
</span></span><span class="line"><span class="cl"><span class="n">train_data_path</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/train&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">train_data_csv</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/train/_classes.csv&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_data_path</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/val&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">val_data_csv</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/val/_classes.csv&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">test_data_path</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/test&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">test_data_csv</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/test/_classes.csv&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># load the CSV file containing the labels</span>
</span></span><span class="line"><span class="cl"><span class="n">train_labels_df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">train_data_csv</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_labels_df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">test_data_csv</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_labels_df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">val_data_csv</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># setup filepaths to the files as vectors</span>
</span></span><span class="line"><span class="cl"><span class="n">train_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">abspath</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="n">train_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">for</span> <span class="n">filename</span> <span class="k">in</span> <span class="n">train_labels_df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">abspath</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="n">test_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">for</span> <span class="n">filename</span> <span class="k">in</span> <span class="n">test_labels_df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">val_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">abspath</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="n">val_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">for</span> <span class="n">filename</span> <span class="k">in</span> <span class="n">val_labels_df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
</span></span></code></pre></div><pre><code>110-element Vector{String}:
 
 ⋮
</code></pre>
<h3 id="data-exploration">Data Exploration</h3>
<p>As usual, we take a look at the data to understand what we are working with.</p>
<p>Below we make a couple of functions to visualize the data.</p>
<p>Note that the helper function <code>label_from_row</code> will come in handy later on.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># -----------------------------------------------------------------------#</span>
</span></span><span class="line"><span class="cl"><span class="c"># helper function to extract label from the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">label_from_row</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># retrieve the label for the image from the DataFrame</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_row</span> <span class="o">=</span> <span class="n">filter</span><span class="p">(</span><span class="n">row</span> <span class="o">-&gt;</span> <span class="n">row</span><span class="o">.</span><span class="n">filename</span> <span class="o">==</span> <span class="n">filename</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_index</span> <span class="o">=</span> <span class="n">findfirst</span><span class="p">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">label_row</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">names</span><span class="p">(</span><span class="n">labels_df</span><span class="p">)[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">label_dict</span><span class="p">[</span><span class="n">label_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="c"># -----------------------------------------------------------------------#</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># function to display a selection of images and their labels</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">show_sample_images_and_labels</span><span class="p">(</span><span class="n">labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">;</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># randomly pick indices for sampling images</span>
</span></span><span class="line"><span class="cl">    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">labels_df</span><span class="p">),</span> <span class="n">num_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sample_filenames</span> <span class="o">=</span> <span class="n">labels_df</span><span class="o">.</span><span class="n">filename</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># calculate number of rows and columns for the grid layuot</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_cols</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="kt">Int</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_rows</span> <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c"># prepare a plot with a grid layout for the images</span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">layout</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">),</span> <span class="n">size</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span> <span class="n">grid</span> <span class="o">=</span> <span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># load and plot each sampled image</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">sample_filenames</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">img_path</span> <span class="o">=</span> <span class="n">joinpath</span><span class="p">(</span><span class="n">train_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">img</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>   <span class="c"># load the image from the file</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c"># retrieve the label for the image from the DataFrame</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span> <span class="o">=</span> <span class="n">label_from_row</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">plot!</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">img</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="n">label</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">display</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>   <span class="c"># display the plot</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># define a dictionary for label descriptions:</span>
</span></span><span class="line"><span class="cl"><span class="n">label_dict</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">(</span><span class="mi">1</span> <span class="o">=&gt;</span> <span class="s">&#34;alarmed&#34;</span><span class="p">,</span> <span class="mi">2</span> <span class="o">=&gt;</span> <span class="s">&#34;angry&#34;</span><span class="p">,</span> <span class="mi">3</span> <span class="o">=&gt;</span> <span class="s">&#34;calm&#34;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">=&gt;</span> <span class="s">&#34;pleased&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># run the function to show images</span>
</span></span><span class="line"><span class="cl"><span class="n">show_sample_images_and_labels</span><span class="p">(</span><span class="n">train_labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240521_julia_transfer_learning_v5/output_6_0.svg" type="" alt="svg"  /></p>
<h3 id="working-with-custom-datasets">Working with Custom Datasets</h3>
<p>When working with custom datasets in Julia, the concepts are similar as in PyTorch, but obviously following Julia&rsquo;s syntax.</p>
<p>In essence, we read the CSV files containing image file paths and their corresponding labels into DataFrames. We then create functions to handle data loading and transformations, such as resizing and normalizing images. This approach is similar to PyTorch&rsquo;s <code>Dataset</code>.</p>
<p>Let&rsquo;s have a quick look.</p>
<h3 id="create-a-custom-dataset">Create a Custom Dataset</h3>
<p>We define a custom dataset using a <code>struct</code>, which is similar to using a <code>class</code> in Python. The <code>ImageContainer</code> struct stores the image file paths and their corresponding labels in a DataFrame. We then create instances of this <code>struct</code> for the training, validation, and test datasets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">struct</span> <span class="kt">ImageContainer</span><span class="p">{</span><span class="kt">T</span><span class="o">&lt;:</span><span class="kt">Vector</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">img</span><span class="o">::</span><span class="kt">T</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels_df</span><span class="o">::</span><span class="kt">DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># generate dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ImageContainer</span><span class="p">(</span><span class="n">train_filepaths</span><span class="p">,</span> <span class="n">train_labels_df</span><span class="p">);</span>   
</span></span><span class="line"><span class="cl"><span class="n">val_dataset</span> <span class="o">=</span> <span class="n">ImageContainer</span><span class="p">(</span><span class="n">val_filepaths</span><span class="p">,</span> <span class="n">val_labels_df</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">ImageContainer</span><span class="p">(</span><span class="n">test_filepaths</span><span class="p">,</span> <span class="n">test_labels_df</span><span class="p">);</span>
</span></span></code></pre></div><h4 id="create-the-data-loaders">Create the Data Loaders</h4>
<p>In this section, we set up data loaders for our custom dataset in Julia, similar to how data loaders are used in PyTorch to manage batching and shuffling of data.</p>
<ol>
<li>
<p>Call helper Function: <code>label_from_row()</code> : This function extracts the label from the DataFrame for a given image file. It finds the index of the column with a value of 1, indicating the class.</p>
</li>
<li>
<p>Length and Indexing:</p>
</li>
</ol>
<ul>
<li><code>length(data::ImageContainer)</code>: Defines the length method to return the number of images in the dataset. Similar to PyTorch&rsquo;s <code>__len__</code>.</li>
<li><code>getindex(data::ImageContainer, idx::Int)</code>: This method is similar to PyTorch’s <code>__getitem__</code>. It loads an image, applies transformations, and returns the processed image along with its label.</li>
</ul>
<ol start="3">
<li>Data Augmentation and Transformations:</li>
</ol>
<ul>
<li>pipeline: Defines a transformation pipeline for scaling and cropping images.</li>
<li>transforms(image, labels_df): Inside getindex, this function applies the transformations to the image and normalizes it using the predefined mean and standard deviation values.</li>
</ul>
<ol start="4">
<li>DataLoaders:</li>
</ol>
<ul>
<li><code>train_loader</code> and <code>val_loader</code>: These DataLoader objects manage batching, shuffling, and parallel processing of the training and validation datasets, similar to <code>torch.utils.data.DataLoader</code> in PyTorch</li>
</ul>
<h5 id="notes-on-implementing-custom-data-containers">Notes on Implementing Custom Data Containers</h5>
<p>According to the documentation for MLUtils.DataLoader (<a href="https://fluxml.ai/Flux.jl/stable/data/mlutils/">see here</a>), custom data containers should implement Base.length instead of  <code>numobs</code>, and Base.getindex instead of <code>getobs</code>, unless there&rsquo;s a difference between these functions and the base methods for multi-dimensional arrays.</p>
<p>Base.length: Should be implemented to return the number of observations. This is akin to PyTorch&rsquo;s <code>__len__</code>.
Base.getindex: Should be implemented to handle indexing of the dataset, similar to PyTorch&rsquo;s <code>__getitem__</code>.
These methods ensure that the data is returned in a form suitable for the learning algorithm, maintaining consistency whether the index is a scalar or vector.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">length</span><span class="p">(</span><span class="n">data</span><span class="o">::</span><span class="kt">ImageContainer</span><span class="p">)</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">im_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">DATA_MEAN</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485f0</span><span class="p">,</span> <span class="mf">0.456f0</span><span class="p">,</span> <span class="mf">0.406f0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">DATA_STD</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229f0</span><span class="p">,</span> <span class="mf">0.224f0</span><span class="p">,</span> <span class="mf">0.225f0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># define a transformation pipeline</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline</span> <span class="o">=</span> <span class="n">DataAugmentation</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">ScaleKeepAspect</span><span class="p">(</span><span class="n">im_size</span><span class="p">),</span> <span class="n">CenterCrop</span><span class="p">(</span><span class="n">im_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">getindex</span><span class="p">(</span><span class="n">data</span><span class="o">::</span><span class="kt">ImageContainer</span><span class="p">,</span> <span class="n">idx</span><span class="o">::</span><span class="kt">Int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">img</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels_df</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">labels_df</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">function</span> <span class="n">transforms</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline</span> <span class="o">=</span> <span class="n">ScaleKeepAspect</span><span class="p">(</span><span class="n">im_size</span><span class="p">)</span> <span class="o">|&gt;</span> <span class="n">CenterCrop</span><span class="p">(</span><span class="n">im_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_img</span> <span class="o">=</span> <span class="n">Images</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_img</span> <span class="o">=</span> <span class="n">apply</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">Image</span><span class="p">(</span><span class="n">_img</span><span class="p">))</span> <span class="o">|&gt;</span> <span class="n">itemdata</span>
</span></span><span class="line"><span class="cl">        <span class="n">img</span> <span class="o">=</span> <span class="n">collect</span><span class="p">(</span><span class="n">channelview</span><span class="p">(</span><span class="n">float32</span><span class="o">.</span><span class="p">(</span><span class="n">RGB</span><span class="o">.</span><span class="p">(</span><span class="n">_img</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">        <span class="n">img</span> <span class="o">=</span> <span class="n">permutedims</span><span class="p">((</span><span class="n">img</span> <span class="o">.-</span> <span class="n">DATA_MEAN</span><span class="p">)</span> <span class="o">./</span> <span class="n">DATA_STD</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">label</span> <span class="o">=</span> <span class="n">label_from_row</span><span class="p">(</span><span class="n">labels_df</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">transforms</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">batchsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">collate</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">parallel</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">batchsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">collate</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">parallel</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">);</span>
</span></span></code></pre></div><h2 id="model-definition">Model Definition</h2>
<p>Here we will load the model with <code>Metalhead.jl</code> and change the classifier &ldquo;head&rdquo; of the architecture to suit our classification need.</p>
<p>We will use this to select the classifier head of the model and change it.</p>
<p>For the fine-tuning portion of this exercise will follow the <a href="https://github.com/FluxML/model-zoo/tree/master/tutorials%2Ftransfer_learning">model zoo documentation</a>:</p>
<hr>
<p><img loading="lazy" src="/images/20240521_julia_transfer_learning_v5/109ebfef-0cea-49b5-98d5-fcd19f0f9596.png" type="" alt="image.png"  /></p>
<hr>
<p>Let&rsquo;s try it out with the <code>ResNet18</code> model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># load the pre-trained model</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_model</span> <span class="o">=</span> <span class="n">ResNet</span><span class="p">(</span><span class="mi">18</span><span class="p">;</span> <span class="n">pretrain</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span><span class="o">.</span><span class="n">layers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># let&#39;s look at the model</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_model</span>
</span></span></code></pre></div><pre><code>Chain(
  Chain(
    Chain(
      Conv((7, 7), 3 =&gt; 64, pad=3, stride=2, bias=false),  [90m# 9_408 parameters[39m
      BatchNorm(64, relu),              [90m# 128 parameters[39m[90m, plus 128[39m
      MaxPool((3, 3), pad=1, stride=2),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
          NNlib.relu,
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
          NNlib.relu,
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
        ),
      ),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        Chain(
          Conv((1, 1), 64 =&gt; 128, stride=2, bias=false),  [90m# 8_192 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
        ),
        Chain(
          Conv((3, 3), 64 =&gt; 128, pad=1, stride=2, bias=false),  [90m# 73_728 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
          NNlib.relu,
          Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
          NNlib.relu,
          Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
        ),
      ),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        Chain(
          Conv((1, 1), 128 =&gt; 256, stride=2, bias=false),  [90m# 32_768 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
        ),
        Chain(
          Conv((3, 3), 128 =&gt; 256, pad=1, stride=2, bias=false),  [90m# 294_912 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
          NNlib.relu,
          Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
          NNlib.relu,
          Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
        ),
      ),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        Chain(
          Conv((1, 1), 256 =&gt; 512, stride=2, bias=false),  [90m# 131_072 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        ),
        Chain(
          Conv((3, 3), 256 =&gt; 512, pad=1, stride=2, bias=false),  [90m# 1_179_648 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
          NNlib.relu,
          Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
          NNlib.relu,
          Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        ),
      ),
    ),
  ),
  Chain(
    AdaptiveMeanPool((1, 1)),
    MLUtils.flatten,
    Dense(512 =&gt; 1000),                 [90m# 513_000 parameters[39m
  ),
) [90m        # Total: 62 trainable arrays, [39m11_689_512 parameters,
[90m          # plus 40 non-trainable, 9_600 parameters, summarysize [39m44.654 MiB.
</code></pre>
<p>Now we modify the head, by chaning the last <code>Chain</code> in the model. We change the last layer to output 4 classes (as opposed to the original 1000 classes).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># modify the model</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_infer</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">resnet_model</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_tune</span> <span class="o">=</span> <span class="n">Chain</span><span class="p">(</span><span class="n">AdaptiveMeanPool</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">Flux</span><span class="o">.</span><span class="n">flatten</span><span class="p">,</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span> <span class="o">=&gt;</span> <span class="mi">4</span><span class="p">))</span>
</span></span></code></pre></div><pre><code>Chain(
  AdaptiveMeanPool((1, 1)),
  Flux.flatten,
  Dense(512 =&gt; 4),                      [90m# 2_052 parameters[39m
) 
</code></pre>
<p><strong>And that&rsquo;s it!</strong> Now, let&rsquo;s just explore both portions of the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">resnet_infer</span>
</span></span></code></pre></div><pre><code>Chain(
  Chain(
    Conv((7, 7), 3 =&gt; 64, pad=3, stride=2, bias=false),  [90m# 9_408 parameters[39m
    BatchNorm(64, relu),                [90m# 128 parameters[39m[90m, plus 128[39m
    MaxPool((3, 3), pad=1, stride=2),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
        NNlib.relu,
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
        NNlib.relu,
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
      ),
    ),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      Chain(
        Conv((1, 1), 64 =&gt; 128, stride=2, bias=false),  [90m# 8_192 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
      ),
      Chain(
        Conv((3, 3), 64 =&gt; 128, pad=1, stride=2, bias=false),  [90m# 73_728 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
        NNlib.relu,
        Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
        NNlib.relu,
        Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
      ),
    ),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      Chain(
        Conv((1, 1), 128 =&gt; 256, stride=2, bias=false),  [90m# 32_768 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
      ),
      Chain(
        Conv((3, 3), 128 =&gt; 256, pad=1, stride=2, bias=false),  [90m# 294_912 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
        NNlib.relu,
        Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
        NNlib.relu,
        Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
      ),
    ),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      Chain(
        Conv((1, 1), 256 =&gt; 512, stride=2, bias=false),  [90m# 131_072 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
      ),
      Chain(
        Conv((3, 3), 256 =&gt; 512, pad=1, stride=2, bias=false),  [90m# 1_179_648 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        NNlib.relu,
        Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        NNlib.relu,
        Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
      ),
    ),
  ),
) [90m        # Total: 60 trainable arrays, [39m11_176_512 parameters,
[90m          # plus 40 non-trainable, 9_600 parameters, summarysize [39m42.693 MiB.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">resnet_tune</span>
</span></span></code></pre></div><pre><code>Chain(
  AdaptiveMeanPool((1, 1)),
  Flux.flatten,
  Dense(512 =&gt; 4),                      [90m# 2_052 parameters[39m
) 
</code></pre>
<h3 id="define-evaluation-and-training-functions">Define evaluation and training functions</h3>
<p>Again, will follow the model zoo documentation. Small adaptations will be needed. (These two functions were taken directly from the documentation).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">m_infer</span><span class="p">,</span> <span class="n">m_tune</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">good</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">in</span> <span class="n">val_loader</span>
</span></span><span class="line"><span class="cl">        <span class="n">good</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">(</span><span class="n">Flux</span><span class="o">.</span><span class="n">onecold</span><span class="p">(</span><span class="n">m_tune</span><span class="p">(</span><span class="n">m_infer</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span> <span class="o">.==</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">count</span> <span class="o">+=</span> <span class="n">length</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">round</span><span class="p">(</span><span class="n">good</span> <span class="o">/</span> <span class="n">count</span><span class="p">,</span> <span class="n">digits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">acc</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>eval_f (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">train_epoch!</span><span class="p">(</span><span class="n">model_infer</span><span class="p">,</span> <span class="n">model_tune</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">in</span> <span class="n">loader</span>
</span></span><span class="line"><span class="cl">        <span class="n">infer</span> <span class="o">=</span> <span class="n">model_infer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">model_tune</span><span class="p">)</span> <span class="k">do</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">            <span class="n">Flux</span><span class="o">.</span><span class="n">Losses</span><span class="o">.</span><span class="n">logitcrossentropy</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">infer</span><span class="p">),</span> <span class="n">Flux</span><span class="o">.</span><span class="n">onehotbatch</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="o">:</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">        <span class="n">update!</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">model_tune</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>train_epoch! (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">resnet_opt</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">Flux</span><span class="o">.</span><span class="n">Optimisers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">resnet_tune</span><span class="p">);</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@time</span> <span class="n">train_epoch!</span><span class="p">(</span><span class="n">resnet_infer</span><span class="p">,</span> <span class="n">resnet_tune</span><span class="p">,</span> <span class="n">resnet_opt</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_train</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">resnet_infer</span><span class="p">,</span> <span class="n">resnet_tune</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_eval</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">resnet_infer</span><span class="p">,</span> <span class="n">resnet_tune</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;train&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_train</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;eval&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_eval</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>176.283332 seconds (37.11 M allocations: 98.153 GiB, 6.06% gc time, 143.87% compilation time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.5744
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.5455


 70.815518 seconds (2.42 M allocations: 95.936 GiB, 11.25% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.6823
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6273


 90.463025 seconds (2.42 M allocations: 95.936 GiB, 11.21% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.7032
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6455


 94.362892 seconds (2.42 M allocations: 95.936 GiB, 10.91% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.7433
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6727


116.526515 seconds (2.42 M allocations: 95.936 GiB, 9.62% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.7885
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6909
</code></pre>
<hr>
<h2 id="vision-transformers">Vision Transformers</h2>
<hr>
<p>Similar to the PyTorch demonstration, we can do transfer learning by changing a different computer vision model (Vision Transformer).</p>
<p>Let&rsquo;s get into it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">vit_model</span> <span class="o">=</span> <span class="n">ViT</span><span class="p">(</span><span class="ss">:base</span><span class="p">;</span> <span class="n">pretrain</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span><span class="o">.</span><span class="n">layers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># let&#39;s have a look at the model head, to see how many inputs the head needs</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span></code></pre></div><pre><code>Chain(
  LayerNorm(768),                       [90m# 1_536 parameters[39m
  Dense(768 =&gt; 1000),                   [90m# 769_000 parameters[39m
) [90m                  # Total: 4 arrays, [39m770_536 parameters, 2.940 MiB.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># modify the head</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_infer</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">vit_model</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># notice how we keep the input to the model head</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_tune</span> <span class="o">=</span> <span class="n">Chain</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">LayerNorm</span><span class="p">(</span><span class="mi">768</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Dense</span><span class="p">(</span><span class="mi">768</span> <span class="o">=&gt;</span> <span class="mi">4</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><pre><code>Chain(
  LayerNorm(768),                       [90m# 1_536 parameters[39m
  Dense(768 =&gt; 4),                      [90m# 3_076 parameters[39m
) [90m                  # Total: 4 arrays, [39m4_612 parameters, 18.352 KiB.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">vit_opt</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">Flux</span><span class="o">.</span><span class="n">Optimisers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">vit_tune</span><span class="p">);</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@time</span> <span class="n">train_epoch!</span><span class="p">(</span><span class="n">vit_infer</span><span class="p">,</span> <span class="n">vit_tune</span><span class="p">,</span> <span class="n">vit_opt</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_train</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">vit_infer</span><span class="p">,</span> <span class="n">vit_tune</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_eval</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">vit_infer</span><span class="p">,</span> <span class="n">vit_tune</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;train&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_train</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;eval&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_eval</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>627.303072 seconds (17.32 M allocations: 291.924 GiB, 4.61% gc time, 3.66% compilation time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.7058
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6273


565.986959 seconds (2.54 M allocations: 291.028 GiB, 4.71% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.8042
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6273


516.041945 seconds (2.54 M allocations: 291.028 GiB, 4.92% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.866
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6818


515.415614 seconds (2.54 M allocations: 291.028 GiB, 4.80% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.8973
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6818


427.423410 seconds (2.54 M allocations: 291.028 GiB, 5.01% gc time)


[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1m└ [22m[39m  metric = 0.9199
[36m[1m┌ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1m└ [22m[39m  metric = 0.6727
</code></pre>
<h3 id="save-the-models">Save the Models</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">JLD2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">resnet_model_state</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">resnet_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_model_state</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">vit_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">jldsave</span><span class="p">(</span><span class="s">&#34;resnet_model.jld2&#34;</span><span class="p">;</span> <span class="n">resnet_model_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">jldsave</span><span class="p">(</span><span class="s">&#34;vit_model.jld2&#34;</span><span class="p">;</span> <span class="n">vit_model_state</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[33m[1m┌ [22m[39m[33m[1mWarning: [22m[39mOpening file with JLD2.MmapIO failed, falling back to IOStream
[33m[1m└ [22m[39m[90m@ JLD2 C:\Users\ingvi\.julia\packages\JLD2\7uAqU\src\JLD2.jl:300[39m
[33m[1m┌ [22m[39m[33m[1mWarning: [22m[39mOpening file with JLD2.MmapIO failed, falling back to IOStream
[33m[1m└ [22m[39m[90m@ JLD2 C:\Users\ingvi\.julia\packages\JLD2\7uAqU\src\JLD2.jl:300[39m
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">BSON</span><span class="o">:</span> <span class="nd">@save</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@save</span> <span class="s">&#34;resnet_model_sate.bson&#34;</span> <span class="n">resnet_model</span>
</span></span><span class="line"><span class="cl"><span class="nd">@save</span> <span class="s">&#34;vit_model_state.bson&#34;</span> <span class="n">vit_model</span>
</span></span></code></pre></div><h2 id="thank-you">Thank you!</h2>
<p>I hope this demonstration on using Julia and <code>Flux</code> for transfer learning was helpful!</p>
<p>Victor</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Transfer Learning Classifier Using PyTorch</title>
      <link>http://localhost:1313/posts/20240515_cat_mood_classification/20240515_cat_mood_classification/</link>
      <pubDate>Wed, 15 May 2024 14:53:29 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240515_cat_mood_classification/20240515_cat_mood_classification/</guid>
      <description>Things we learn here include image data exploration, transfer learning, custom datasets, comparing ML models, saving/loading models and model data, conditional setup for different work environments.</description>
      <content:encoded><![CDATA[<p><a href="https://colab.research.google.com/github/vflores-io/cat_mood/blob/main/cat_mood_classification.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="cat-expression-classifier-using-convolutional-neural-networks">Cat Expression Classifier Using Convolutional Neural Networks</h1>
<hr>
<p>This project aims to build a cat expression classifier with convolutional neural networks (CNNs) using PyTorch. This project serves as an introduction to image classification and also dives into the nuances of handling a specific, custom dataset and adapting pre-trained models for our purposes.</p>
<h3 id="objective">Objective</h3>
<p>The primary objective of this project is to develop a model capable of classifying images of cat faces into one of four moods: alarmed, angry, calm, and pleased. By the end of this tutorial, you will learn how to preprocess image data, leverage transfer learning for image classification, and evaluate a model&rsquo;s performance.</p>
<h3 id="tools-and-techniques">Tools and Techniques</h3>
<p>We will employ PyTorch, a powerful and versatile deep learning library, to construct our CNN. The model of choice for this tutorial is ResNet18, a robust architecture that is commonly used in image recognition tasks. Given the straightforward nature of our classificaiton problem, ResNet18 provides an ecellent balance between complexity and performance.</p>
<h3 id="why-transfer-learning">Why <em>Transfer Learning</em>?</h3>
<p>In this tutorial, we utilize <em>transfer learning</em> to take advantage of a pre-trained ResNet18 model. This approach allows us to use a model that has already learned a significant amount of relevant features from a vast and diverse dataset (ImageNet). By fine-tuning this model to our specific task, we can achieve high accuracy with relatively little data and reduce the computational cost typycally associated with training a deep neural network from scratch.</p>
<h3 id="dataset">Dataset</h3>
<p>The dataset comprises images of cat faces, labeled according to their expressed mood. These images are organized into training, validation, and testing sets, each with a corresponding CSV file which maps filenames to mood labels. This guide will walk you through the process of loading, preprocessing, and augmenting this data to suit the needs of our CNN.</p>
<p>The dataset was obtained <a href="https://universe.roboflow.com/mubbarryz/domestic-cats-facial-expressions">here</a>.</p>
<p>Let&rsquo;s get started!</p>
<h2 id="dataset-exploration">Dataset Exploration</h2>
<h3 id="listing-the-number-of-images-in-each-set-and-visualizing-the-set">Listing the Number of Images in Each Set and Visualizing The Set</h3>
<p>Below we will mount the drive to retrieve the data set files.
Then, will use Python&rsquo;s <code>os</code> module to list the number of images in the dataset. This will give us an idea of the size of the set.</p>
<p>Additionally, we will include a flag to tell the model whether we want to train it or to load a previously saved model&hellip; this will become clear later.</p>
<p>Finally, we will set up a function to visualize some sample images from each set.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># flag to control whether to train the model or load a saved model</span>
</span></span><span class="line"><span class="cl"><span class="n">should_train_resnet</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="n">should_train_vit</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">set_path</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># check if the notebook is running on google colab</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running on Google Colab.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
</span></span><span class="line"><span class="cl">    <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;/content/drive/PATH-TO-YOUR-DATA&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running locally.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;./data/cat_expression_data&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">path</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">base_dir</span> <span class="o">=</span> <span class="n">set_path</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>Running on Google Colab.
Mounted at /content/drive
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># base directories</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">list_images</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34; list folders and count image files in each folder &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s1">&#39;.jpg&#39;</span><span class="p">,</span> <span class="s1">&#39;jpeg&#39;</span><span class="p">,</span> <span class="s1">&#39;.bmp&#39;</span><span class="p">,</span> <span class="s1">&#39;.gif&#39;</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total image files in </span><span class="si">{</span><span class="n">dir_name</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">list_images</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">list_images</span><span class="p">(</span><span class="n">val_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">list_images</span><span class="p">(</span><span class="n">test_dir</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Total image files in train: 1149
Total image files in val: 110
Total image files in test: 55
</code></pre>
<p>Finally, we will make a dictionary that maps the classes to index-based labels, from the CSV file. We will need this way later, but we will define the dictionary this early on.</p>
<h3 id="visualizing-some-of-the-data">Visualizing Some of the Data</h3>
<p>Let&rsquo;s visualize a few images from each folder to ensure to have a better feel of the data.</p>
<p>We will do this by making a dataframe out of the annotations file where the labels are stored. We will use the test annotations, since this is the smallest dataset, and the other two have the same labels anyway.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define the annotations file</span>
</span></span><span class="line"><span class="cl"><span class="n">annotations_filename</span> <span class="o">=</span> <span class="s1">&#39;_classes.csv&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define the full path to the annotations file</span>
</span></span><span class="line"><span class="cl"><span class="n">test_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># load the annotations file</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_annotations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">class_names</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">col</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">:])}</span>  <span class="c1"># Adjust slicing if there are other columns</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>{0: ' alarmed', 1: ' angry', 2: ' calm', 3: ' pleased'}
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">show_sample_images</span><span class="p">(</span><span class="n">main_directory</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Display sample images from each subfolder within the main directory.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">subfolders</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">path</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">scandir</span><span class="p">(</span><span class="n">main_directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">directory</span> <span class="ow">in</span> <span class="n">subfolders</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s1">&#39;.jpg&#39;</span><span class="p">,</span> <span class="s1">&#39;.jpeg&#39;</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">chosen_samples</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">image_files</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">),</span> <span class="n">num_samples</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Plot settings</span>
</span></span><span class="line"><span class="cl">            <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">),</span> <span class="n">num_samples</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Images from </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">chosen_samples</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># ax.set_title(os.path.basename(img_path))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;No images to display in </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="si">}</span><span class="s2">.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Example usage with the base directory containing train, validation, and test subfolders</span>
</span></span><span class="line"><span class="cl"><span class="n">show_sample_images</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_10_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_10_1.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_10_2.png" type="" alt="png"  /></p>
<h2 id="data-preprocessing">Data Preprocessing</h2>
<hr>
<p>This steps involves preparing the dataset for training a PyTorch model by resizing, normalizing, and applying data augmentation.</p>
<p><strong>NOTE:</strong> At this point, it is important to know what is the model or CNN architecture we will be using. Important aspects to consider include the image size, any data transformations for training and validation, data augmentation techniques, and setting up data loaders later.</p>
<p>In this example, we will use <code>ResNet18 </code>. The inputs must follow a specific format, as per the PyTorch ResNet documentation found <a href="https://pytorch.org/hub/pytorch_vision_resnet/">here</a>:</p>
<blockquote>
<p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape <code>(3 x H x W)</code>, where <code>H</code> and <code>W</code> are expected to be at least 224. The images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>.</p>
</blockquote>
<h3 id="tools-for-data-preprocessing-in-pytorch">Tools for Data Preprocessing in PyTorch</h3>
<ul>
<li><code>torchvision.transforms</code>: Provides common image transformations like resizing, normalization, and augmentation.</li>
<li><code>torch.utils.data.Dataset</code>: A base class for creating custom datasets.</li>
<li><code>torch.utils.data.DataLoader</code>: Loads and batches data for training.</li>
</ul>
<h3 id="the-data">The Data</h3>
<p>The data set we are using consists of three folders: <code>train</code>, <code>val</code>, <code>test</code>. Each of them contain a set of images of cats. The labels in this case, are in the form of a CSV file that maps the filename with a one-hot encoding to label the classification of the image, i.e. the cat&rsquo;s mood - alarmed, angry, calm, pleased.</p>
<p>Because this dataset structure is not exactly suitable for the <code>ImageFolder</code> module in PyTorch, whereby labelling is made easier and based on the folder structure, we need to create a custom dataset and loader. Let&rsquo;s get started!</p>
<h3 id="define-image-transformations">Define Image Transformations</h3>
<ul>
<li>Specify resizing dimensions, normalization parameters, and augmentation techniques (like random rotation, flips, etc.).</li>
<li>Create separate transformations for training and validation datasets.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># import transforms</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define the image size</span>
</span></span><span class="line"><span class="cl"><span class="n">image_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># adjusted for ResNet18</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define transformations for the training dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>  <span class="c1"># resize to ensure minimum size</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="c1"># center crop to 224x224</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span> <span class="c1"># data augmentation</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">15</span><span class="p">),</span> <span class="c1"># data augmentation</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>  <span class="c1"># important, because the read_image reads as uint8, needs to be float</span>
</span></span><span class="line"><span class="cl">                                                <span class="c1"># given that below we apply normalization</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                         <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                         <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span></code></pre></div><p>With these transformations, the data pipeline will align with common practices for pre-trained models like ResNet18.</p>
<h3 id="create-custom-datasets-and-data-loaders">Create Custom Datasets and Data Loaders</h3>
<p>Given the structure of our dataset, where labels are provided in a CSV file rather than through directory structure, we need to use a custom dataset class. This will allow us to link echc image with its respective label based on our CSV file&rsquo;s structure.</p>
<h4 id="creating-custom-dataset">Creating Custom Dataset</h4>
<p>We will extend the <code>torch.utils.data.Dataset</code> class to create our custom dataset. this class will override the necessary methods to handle our specific dataset setup:</p>
<ol>
<li>Initialization: Load the CSV file and set up the path to the images</li>
<li>Length: Return the total number of images</li>
<li>Get item: Load each image by index, apply the specified transformations, and parse the label from the CSV data</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CustomImageDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34; a custom dataset class that loads images and their labels from a CSV &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">annotations_file</span><span class="p">,</span> <span class="n">img_dir</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    args:
</span></span></span><span class="line"><span class="cl"><span class="s2">      annotations_file (string): path to the CSV file with annotations
</span></span></span><span class="line"><span class="cl"><span class="s2">      img_dir (str): directory with all the images
</span></span></span><span class="line"><span class="cl"><span class="s2">      transform (callable, optional): transform to be applied on a sample
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">annotations_file</span><span class="p">)</span> <span class="c1"># load annotations</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">img_dir</span> <span class="o">=</span> <span class="n">img_dir</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; returns the number of items in the dataset &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; fetches the image and label at the index idx from the dataset &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_dir</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># convert one-hot encoded labels to a categorical label</span>
</span></span><span class="line"><span class="cl">    <span class="n">one_hot_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># next find the index of the element in the slice which contains the &#39;1&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># since all other numbers will be 0; this will correspond to the label</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 0, 1, 2, 3</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">one_hot_label</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>  <span class="c1"># apply transformations</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</span></span></code></pre></div><p>Now that we have defined the data classes, we can create objects for each of our datasets, as a <code>CustomImageDataset</code> class.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># annotations_filename = &#39;_classes.csv&#39;    # previously defined</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># paths to annotation files</span>
</span></span><span class="line"><span class="cl"><span class="n">train_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">val_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>    <span class="c1"># previously defined</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create dataset objects</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CustomImageDataset</span><span class="p">(</span><span class="n">train_annotations</span><span class="p">,</span> <span class="n">train_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">transform</span> <span class="o">=</span> <span class="n">train_transforms</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_dataset</span> <span class="o">=</span> <span class="n">CustomImageDataset</span><span class="p">(</span><span class="n">val_annotations</span><span class="p">,</span> <span class="n">val_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">transform</span> <span class="o">=</span> <span class="n">val_transforms</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">CustomImageDataset</span><span class="p">(</span><span class="n">test_annotations</span><span class="p">,</span> <span class="n">test_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">transform</span> <span class="o">=</span> <span class="n">val_transforms</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="creating-data-loaders">Creating Data Loaders</h4>
<p>Data loaders in PyTorch provide the necessary functionality to batch, shuffle, and feed the data to your model during training in an efficient manner. They also handle parallel processing using multiple worker threads, which can significantly speed up data loading.</p>
<p>In short, data loaders take the dataset objects and handle the process of creating batches, shuffling the data, and parallelizing the data loading process.</p>
<p>Below we will create a data loaders for our datasets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>   <span class="c1"># defines how many samples per batch to load</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span>    <span class="c1"># shuffles the dataset at every epoch</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span>   <span class="c1"># no need to shuffle validation data</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p>In the loader above, we have the following main parts:</p>
<ul>
<li>Batch size: typycally set based on the system&rsquo;s memory capacity and how large the model is. A larger batch size can speed up training but requires more memory.</li>
<li>Shuffle: shuffling helps ensure that each batch sees a varierty of data across epochs, which can improve model generalization.</li>
<li>Number of workers: this controls how many subproceses to use for data loading. More workers can lead to faster data preprocessing and reduced time to train each epoch but also increases memory usage.</li>
</ul>
<h3 id="integration-with-the-training-loop">Integration with the Training Loop</h3>
<p>With the data loaders set up, we are now ready to integrate them into the model&rsquo;s training and validation loops.</p>
<p>The code <strong>snippet</strong> below shows how this would be done. We will implement the actual integration when we get to the training section.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Forward pass, backward pass, and optimize</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Validation step at the end of each epoch</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Calculate validation accuracy, loss, etc.</span>
</span></span></code></pre></div><h2 id="model-training">Model Training</h2>
<hr>
<p>Now that the data is ready and properly formatted for input into a neural network, the next step involves setting up and training the <code>ResNet18</code> model. We will configure the model, define the loss function and optimizer, and implement the training and validation loops.</p>
<h3 id="next-steps">Next Steps</h3>
<ol>
<li>Model setup:</li>
</ol>
<ul>
<li>Load the pre-trained <code>ResNet18</code> model and modify it for our specific classification task (number of classes based on cat facial expressions)</li>
</ul>
<ol start="2">
<li>Loss function and optimizer:</li>
</ol>
<ul>
<li>Define a loss function suitable for classification, e.g. <code>CrossEntropyLoss</code></li>
<li>Set up an optimizer (like <code>Adam</code> or <code>SGD</code>) to adjust the model weights during training based on the computed gradients</li>
</ul>
<ol start="3">
<li>Training loop:</li>
</ol>
<ul>
<li>Implement the loop that processes the data through the model, computes the loss, updates the model parameters, and evaluates the model performance on the validation dataset periodically</li>
</ul>
<ol start="4">
<li>Monitoring and saving the model:</li>
</ol>
<ul>
<li>Track performance metrics such as loss and accuracy</li>
<li>Implement functionality to save the trained model for later use or further evaluation</li>
</ul>
<h3 id="model-setup">Model Setup</h3>
<p>In this section, we&rsquo;ll configure a ResNet18 model to suit our specific classification task. Since the model is originally designed for ImageNet with 1000 classes, we&rsquo;ll adapt it for our use case, which involves classifying images into four mood categories (alarmed, angry, calm, pleased).</p>
<h4 id="import-the-necessary-libraries">Import the Necessary Libraries</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># import torch  # this has already been imported before</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
</span></span></code></pre></div><h4 id="load-and-modify-the-pre-trained-resnet18">Load and Modify the Pre-trained ResNet18</h4>
<p>We will load a pre-trained ResNet18 model and modify its final layer to suit our classification needs. This is known as <strong>transfer learning</strong>, and it is a technique that uses a pre-trained model and leverages its learned parameters to focus on a similar, more specific task. This is a powerful technique, since it uses the existing knowledge (such as edges and features) so that the new classification task is more robust, and faster to tune to the specific task.</p>
<h4 id="understanding-transfer-learning">Understanding Transfer Learning</h4>
<p><strong>Transfer Learning</strong> is a powerful technique in machine learning where a model developed for a particular task is reused as the starting point for a model on a second task. It&rsquo;s especially popular in deep learning given the vast compute and time resources required to develop neural network models on large datasets and from scratch.</p>
<h4 id="why-use-transfer-learning">Why Use Transfer Learning?</h4>
<ol>
<li>Efficiency: transfer learning allows us to leverage pre-trained networks that have already learned a good amount of features on large datasets. This is beneficial as it can drastically reduce the time and computational cost to achieve high performance.</li>
<li>Performance: models trained on large-scalr datasets like ImageNet havve proven to generalize well to other datasets. Starting with these can provide a significand head-start in terms of performance.</li>
</ol>
<h5 id="applying-transfer-learning">Applying Transfer Learning</h5>
<ul>
<li>Model adaptation: for our specific task fo classifying cat moods, we take a pre-trained ResNet18 model and tailor it to our needs. The pre-trained model brings the advantage of learned features from ImageNet, a vast and diverse dataset.</li>
<li>Feature extraction: by <strong>freezing</strong> (i.e. keeping the weight values as they are) the pre-trained layers, we utilize them as a feature extractor. Only the final layers are trained to adapt those features to our specific classification task.</li>
</ul>
<h4 id="model-setup-with-a-custom-classifier">Model Setup with a Custom Classifier</h4>
<p>We have mentioned replacing the funal layer(s) as a transfer learning techniques. In this case, we replace the final fully connected (fc) layer of ResNet18 with a different layer which will suit our need to have 4 classes. Additionally, we will replace this fc layer with a more complex classifier portion, which involves adding additional layers such as ReLU for non-linearity, and dropout for regularization to prevent overfitting.</p>
<ul>
<li>ReLU Activation: introduces non-linearity into the model, allowing it to learn more complex patterns.</li>
<li>Dropout: Randomly zeros some of the elements of the input (to the layer, not input to the model) tensor with probability $p$ during training, which helps prevent overfitting.</li>
</ul>
<p>Let&rsquo;s implement this classifier in our transfer learning setup.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># assign the model weights</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create the model object with pre-trained weights</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># freeze all the layers in the network</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># replace the fc layer with a more complex classifier</span>
</span></span><span class="line"><span class="cl"><span class="n">num_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>    <span class="c1"># first linear layer</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>                       <span class="c1"># non-linearity</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>                 <span class="c1"># dropout for regularization</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span>                <span class="c1"># output layer, 4 classes for cat moods</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># move model to GPU if available</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 117MB/s]
</code></pre>
<h3 id="loss-function-and-optimizer">Loss Function and Optimizer</h3>
<p>For our classification task, we need a loss function that effectively measures the discrepancy between the predicted labels and the actual labels. Since we&rsquo;ve configured out model outputs to be class indices (from our dataset&rsquo;s one-hot encoded labels), we&rsquo;ll use <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code>CrossEntropyLoss</code></a>, which is ideal for such clasification tasks.</p>
<p>We&rsquo;ll par this with the <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code>Adam</code></a> optimizer, which is known for its efficiency in handling sparse gradients and adaptive learning rate capabilities, making it well-suited for this task.</p>
<p>Let&rsquo;s set up our loss function and optimizer.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># loss function</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># optimizer</span>
</span></span><span class="line"><span class="cl"><span class="c1"># optimize only the final classifier layers</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="training-and-validation-loops">Training and Validation Loops</h3>
<p>Now, let&rsquo;s write the code to train and validate our model. This involves running the model over several epochs, making predictions, calculating loss, updating the model parameters, and evaluating the model&rsquo;s perfomance on the validation dataset.</p>
<ul>
<li>Training loop: here, the model learns by adjusting its weights based on the calculated loss from the training data</li>
<li>Validation loop: validation occurs post the training phase in each epoch and helps in evaluating the model&rsquo;s performance on unseen data, ensuring it generalizes well and doesn&rsquo;t overfit</li>
</ul>
<h3 id="savingloading-the-model">Saving/Loading the Model</h3>
<h4 id="save-the-trained-model">Save the Trained Model</h4>
<p>If we have performed training, we can save the model to use next time, so that we can avoid re-training everytime we run the notebook.</p>
<p>We will implement this part as an <code>if</code> statement, that would run the training loop and save the model if we choose to, otherwise, we would just load the model weights.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pickle</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define a function to set the save model path</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">model_save_path</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># check if the notebook is running on google colab</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running on Google Colab.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;/content/drive/PATH-TO-YOUR-SAVE-FOLDER&#39;</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running locally.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;./saved_models&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">path</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">flag</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">num_epochs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">flag</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># num_epochs = 25   # define the number of epochs for training</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>   <span class="c1"># set the model to training mode</span>
</span></span><span class="line"><span class="cl">      <span class="n">total_train_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># forward pass to get outputs</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># backpropagation and optimization</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">total_train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># calculate average training loss for the epoch</span>
</span></span><span class="line"><span class="cl">      <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">total_train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_train_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># print average training loss per epoch&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">, Training Loss: </span><span class="si">{</span><span class="n">avg_train_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">#------------------#</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># validation phase #</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#------------------#</span>
</span></span><span class="line"><span class="cl">      <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>   <span class="c1"># set the model to evaluation mode</span>
</span></span><span class="line"><span class="cl">      <span class="n">total_val_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">      <span class="n">total_val_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">total_val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">total_val_accuracy</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># calculate average validation loss for the epoch</span>
</span></span><span class="line"><span class="cl">      <span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">total_val_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># calculate validation accuracy</span>
</span></span><span class="line"><span class="cl">      <span class="n">val_accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">total_val_accuracy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># print validation accuracy</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; Validation loss: </span><span class="si">{</span><span class="n">avg_val_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Validation accuracy: </span><span class="si">{</span><span class="n">val_accuracy</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># save model and training data</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_model_path</span> <span class="o">=</span> <span class="n">model_save_path</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># model_filename = &#39;vit_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model saved to&#39;</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># save the training and validation losses</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">save_training_data</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">({</span><span class="s1">&#39;train_losses&#39;</span><span class="p">:</span> <span class="n">train_losses</span><span class="p">,</span> <span class="s1">&#39;val_losses&#39;</span><span class="p">:</span> <span class="n">val_losses</span><span class="p">},</span> <span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Training data saved to </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Specify the filename for saving training data</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_data_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_training_data</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">training_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># load the trained model</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_model_path</span> <span class="o">=</span> <span class="n">model_save_path</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># model_filename = &#39;vit_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span> <span class="o">=</span> <span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model loaded and set to evaluation mode.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># load training data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Load the training and validation losses</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">load_training_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">training_data_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_data</span> <span class="o">=</span> <span class="n">load_training_data</span><span class="p">(</span><span class="n">training_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_losses</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;train_losses&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_losses</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;val_losses&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training data loaded successfully.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model_filename</span> <span class="o">=</span> <span class="s1">&#39;resnet18_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">model_data_filename</span> <span class="o">=</span> <span class="s1">&#39;resnet18_training_data.pkl&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="n">training_loop</span><span class="p">(</span><span class="n">should_train_resnet</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Running on Google Colab.
Model loaded and set to evaluation mode.
Training data loaded successfully.
</code></pre>
<hr>
<h2 id="model-evaluation">Model Evaluation</h2>
<p>Now that the model is trained, the next step is to evaluate its performance more thoroughly, and possibly improve it based on the insights gained.</p>
<p>Evaluating the model involves checking the accuracy and also looking at other metrics like precision, recall, and F1-score, especially if the dataset is imbalanced or if specific classes are more important than others.</p>
<h3 id="model-evaluation-on-validation-set">Model Evaluation on Validation Set</h3>
<p>After training a machine learning model, it&rsquo;s crucial to evaluate its performance comprehensively. Here, we will detail three key diagnostic tools&quot;</p>
<ol>
<li>Confusion matrix</li>
<li>Plotting training and validation losses</li>
<li>Visualization of the predictions</li>
</ol>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<p>A confusion matrix provides a detailed breakdown of the model&rsquo;s predictions, showing exactly how many samples from each class were correctly or incorrectly predicted as each other class. This is crucial for understanding the model&rsquo;s performance across different categories.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">true_labels</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">      <span class="n">true_labels</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># compute the confusion matrix</span>
</span></span><span class="line"><span class="cl">  <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">clf_report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Convert classification report dictionary to DataFrame</span>
</span></span><span class="line"><span class="cl">  <span class="n">clf_report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clf_report</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">cm</span><span class="p">,</span> <span class="n">clf_report_df</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">cm</span><span class="p">,</span> <span class="n">clf_report_df</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print classification report</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">clf_report_df</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_40_0.png" type="" alt="png"  /></p>
<pre><code>Classification Report:
              precision    recall  f1-score     support
0              0.333333  0.285714  0.307692   14.000000
1              0.862069  0.714286  0.781250   35.000000
2              0.487179  0.678571  0.567164   28.000000
3              0.866667  0.787879  0.825397   33.000000
accuracy       0.672727  0.672727  0.672727    0.672727
macro avg      0.637312  0.616613  0.620376  110.000000
weighted avg   0.700728  0.672727  0.679728  110.000000
</code></pre>
<h4 id="plotting-training-and-validation-losses">Plotting Training and Validation Losses</h4>
<p>Plotting the training and validation losses over epochs allows us to monitor the learning process, identifying issues such as overfitting or underfitting.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">plot_losses</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Plot the training and validation losses.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Parameters:
</span></span></span><span class="line"><span class="cl"><span class="s2">  - train_losses: list of training loss values per epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">  - val_losses: list of validation loss values per epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Training Loss&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Validation Loss&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and Validation Losses Over Epochs&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># take the tracked losses from thet training loop</span>
</span></span><span class="line"><span class="cl"><span class="n">plot_losses</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_42_0.png" type="" alt="png"  /></p>
<h4 id="visualization-of-the-predictions">Visualization of the Predictions</h4>
<p>Visualizing model predictions on actual data points provides immediate qualitative feedback about model behavior. It helps identify paterns in which the model performs well or poorly, revealing potential biases, underfitting, or overfitting.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visualize_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">images_so_far</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="n">rows</span> <span class="o">=</span> <span class="n">num_images</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">num_images</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">num_images</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span> <span class="o">=</span> <span class="n">rows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># define the mean and std deviation used for normalization</span>
</span></span><span class="line"><span class="cl">  <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">images_so_far</span> <span class="o">&lt;</span> <span class="n">num_images</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">images_so_far</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">images_so_far</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">num_images</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">axes</span> <span class="c1"># arrange in grid</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># convert tensors to integers</span>
</span></span><span class="line"><span class="cl">          <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">          <span class="n">actual_label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># reverse normalization transform</span>
</span></span><span class="line"><span class="cl">          <span class="n">img</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># change CxHxW to HxWxC</span>
</span></span><span class="line"><span class="cl">          <span class="n">img</span> <span class="o">=</span> <span class="n">std</span> <span class="o">*</span> <span class="n">img</span> <span class="o">+</span> <span class="n">mean</span>   <span class="c1"># reverse normalization</span>
</span></span><span class="line"><span class="cl">          <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># clip values to ensure they fall between 0 and 1</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># use converted integers to access class names</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">]</span><span class="si">}</span><span class="s1"> | actual: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">actual_label</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">images_so_far</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>   <span class="c1"># adjust layout</span>
</span></span><span class="line"><span class="cl">          <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">          <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># make new loader for random samples</span>
</span></span><span class="line"><span class="cl"><span class="n">vis_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">visualize_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vis_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_44_0.png" type="" alt="png"  /></p>
<h2 id="evaluation-of-model-performance">Evaluation of Model Performance</h2>
<p>Our Cat Expression Classifier, built on a modified ResNet18 architecture, demonstrates a promising ability to classify cat expressions into four categories: alarmed, angry, calm, and pleased. Here, we provide a detailed analysis of the model&rsquo;s performance based on our the training and validation efforts.</p>
<h3 id="overall-performance-metrics">Overall Performance Metrics</h3>
<p>The model achieves an overall accuracy of 68.18% on the validation set. This is a decent foundation but indicates room for further refinement, especially in distinguishing between expressions that share subtle features. Here is a breakdown of the key performance metrics:</p>
<ul>
<li>Precision: Measures the accuracy of positive predictions. for example, the <code>pleased</code> category shows high precision, indicating that the model reliably identifies this expression.</li>
<li>Recall: Reflects the model&rsquo;s ability to identify all relevant instances of a class. The <code>angry</code> category has a high recall, suggesting that the model effectively captures most of the <code>angry</code> expressions.</li>
<li>F1-Score: Balances precision and recall and is particularly useful in scenarios where class distribution is uneven.</li>
</ul>
<h3 id="confusion-matrix-insights">Confusion Matrix Insights</h3>
<p>the confusion matrix provides a granular view of the model&rsquo;s performance across the different classes. It highlights specific areas where the model performs well and others where it struggles, such as:</p>
<ul>
<li>Misclassifications between <code>alarmed</code> and <code>angry</code> suggest that the model may be conflating these expressions due to their similar features.</li>
<li>The high accuracy in identifying <code>pleased</code> expressions shows that distinct features of this mood are well captured by the model.</li>
</ul>
<h3 id="training-and-validation-losses">Training and Validation Losses</h3>
<p>The training and validation loss plots reveal the learning dynamics over the epochs:</p>
<ul>
<li>A steady decrease in training loss indicates that the model is effectively learning from the training data.</li>
<li>The pattern of validation loss provides insights into the model&rsquo;s generalization ability. Increases in validation loss suggest moments where the model might be overfitting to the training data.</li>
</ul>
<hr>
<h2 id="trying-out-visiontransformers">Trying out VisionTransformers</h2>
<h3 id="data-transformations-for-vision-transformer">Data Transformations for Vision Transformer</h3>
<p>When transitioning from a CNN like <code>ResNet18</code> to a Vision Transformer (ViT), it&rsquo;s essential to evaluate whether the existing preprocessing steps - particulary the data transformations - are suitable for the new model architecture. For ViT , wed must consider their unique handling of image data, which relies on dividing the image into fixed-size patches and understanding global dependencies through self-attention mechanisms.</p>
<p>For this exercise, we will maintain the same transformations we have previously defined.</p>
<p>The decision to retain the initial transformations is based on the principle of consistency and the minimal impact expected by changing model architectures regarding how images are scaled and augmented. The chosen transformations ensure that the images are adequately prepared for the neural network without introducing complexities or distortions that could hinder the learning of global patterns, which are vital for Vision Transformers due to their reliance on self-attention mechanisms.</p>
<p>Additionally, maintaining these transformations allows for a more straightforward comparison between the ResNet18 model and the Vision Transformer model, as any changes in model performance can more confidently be attributed to the architectural differences rather than changes in data preprocessing.</p>
<h3 id="load-pre-trained-vision-transformer-model">Load Pre-Trained Vision Transformer Model</h3>
<p>First, we need to load the ViT model that has been pre-trained on a large dataset. We&rsquo;ll then adapt the classifier <em>head</em> to our needs, which is classifying cat moods into four categories.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># load the ViT pre-trained model</span>
</span></span><span class="line"><span class="cl"><span class="n">weights_vit</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ViT_B_16_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_vit</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights_vit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># print the model structure to understand what needs to be replaced</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model_vit</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Downloading: &quot;https://download.pytorch.org/models/vit_b_16-c867db91.pth&quot; to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth
100%|██████████| 330M/330M [00:02&lt;00:00, 124MB/s]


VisionTransformer(
  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (head): Linear(in_features=768, out_features=1000, bias=True)
  )
)
</code></pre>
<h3 id="freezing-the-encoder-layers">Freezing the Encoder Layers</h3>
<p>Freezing the encoder layers prevents their weights from being updated during training, which means they retain the knowledge they have already gained from ImageNet. We only want to train the classifier that we will modify to our specific task.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># freeze all layers in the model by disabling gradient computation</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model_vit</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</span></span></code></pre></div><h3 id="modify-the-classifier">Modify the Classifier</h3>
<p>The standard ViT model includes a classifier at the end (usually named <code>heads</code> in <code>torchvision</code> models), which is a linear layer designed for the original classification task, e.g. 1000 classes for ImageNet. We will replace this with a new classifier suited for our task (4 cat moods).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># replace the classifier head</span>
</span></span><span class="line"><span class="cl"><span class="c1"># as we saw in the architecture above, the classifier is called `heads`</span>
</span></span><span class="line"><span class="cl"><span class="n">num_features</span> <span class="o">=</span> <span class="n">model_vit</span><span class="o">.</span><span class="n">heads</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">in_features</span>   <span class="c1"># ge tthe number of input features</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># replace with a new head for len(class_names) = 4</span>
</span></span><span class="line"><span class="cl"><span class="n">model_vit</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># move model to appropriate device</span>
</span></span><span class="line"><span class="cl"><span class="n">model_vit</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>VisionTransformer(
  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (0): Linear(in_features=768, out_features=4, bias=True)
  )
)
</code></pre>
<h3 id="define-loss-function-and-optimizer">Define Loss Function and Optimizer</h3>
<p>Now, define the loss function and an optimizer. Since we are only training the classifier layer, ensure the optimizer is set to only update the parameters of the classifier.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># optimizer will not change, but still show it here:</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># optimizer - only optimize the classifier parameters</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_vit</span><span class="o">.</span><span class="n">heads</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="training-loop">Training Loop</h3>
<p>Here, we revisit the training process, adapting our previously established procedures to the Vision Transformer (ViT) model. Much like our approach with Resnet18, we utilize a similar training loop structure to ensure consistency and comparability. the core steps of training - forward pass, loss computation, backward pass, and parameters update - are maintained, but they are now applied to a differently structured model that leverages self-attention mechanisms rather than convolutional layers. This section briefly outlines these steps, focusing on any adjustments specific to the ViT to optimize it for our cat mood classification task.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model_filename</span> <span class="o">=</span> <span class="s1">&#39;vit_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">model_data_filename</span> <span class="o">=</span> <span class="s1">&#39;vit_training_data.pkl&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="n">training_loop</span><span class="p">(</span><span class="n">should_train_vit</span><span class="p">,</span> <span class="n">model_vit</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Running on Google Colab.
Model loaded and set to evaluation mode.
Training data loaded successfully.
</code></pre>
<h3 id="model-evaluation-1">Model Evaluation</h3>
<h4 id="confusion-matrix-1">Confusion Matrix</h4>
<p>The confusion matrix below shows the following results:</p>
<ul>
<li>Class <code>alarmed</code>: moderate confusion with other classes, indicating difficulty in distinguishing <code>alarmed</code> from other moods.</li>
<li>Class <code>angry</code>: high accuracy, showing that `angry is well-recognized, with few misclassifications</li>
<li>Class <code>calm</code>: some confusion, particularly with <code>pleased</code>, suggesting similar features or expressions between these moods that the model confuses</li>
<li>Class <code>pleased</code>: best performance, indicating clear distinguishing features that the model learnes effectively</li>
</ul>
<h3 id="considerations-on-data-quality">Considerations on Data Quality</h3>
<p>Throughout the development and evaluation of our models, it has become evident that the quality of the dataset significantly impacts the classification accuracy. Certain misclassifications observed, such as the confusion between &lsquo;pleased&rsquo; and &lsquo;calm&rsquo; or &lsquo;alarmed&rsquo; and &lsquo;angry,&rsquo; suggest that the labels may not always align perfectly with the visual cues present in the images. This discrepancy can stem from subjective interpretations of cat expressions during labeling. Improving the dataset by refining the labeling process, possibly with the assistance of animal behavior experts, or by curating a more consistently labeled dataset could enhance model performance. Enhancing data quality would help in training more accurate and reliable models, thereby increasing the robustness of the classification outcomes.</p>
<h4 id="classification-report">Classification Report</h4>
<p>From the classification report, we can draw the following conclusions:</p>
<ul>
<li>Class <code>pleased</code> shows the highest precision, indicating a high rate of true positive predictions</li>
<li>Class <code>angry</code> has the highes recall, suggesting effective identification of this mood</li>
<li>Classes <code>angry</code> and <code>pleased</code> show high F1-scores, indicating robust performance.</li>
</ul>
<h4 id="overall-accuracy">Overall Accuracy</h4>
<p>The model achieves and accuracy of 74.55%, which is a solid performance but suggests room for improvement, particularly in reducing misclassifications among less distinct moods.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">cm</span><span class="p">,</span> <span class="n">clf_report_df</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model_vit</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print classification report</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">clf_report_df</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_62_0.png" type="" alt="png"  /></p>
<pre><code>Classification Report:
              precision    recall  f1-score     support
0              0.461538  0.428571  0.444444   14.000000
1              0.815789  0.885714  0.849315   35.000000
2              0.625000  0.535714  0.576923   28.000000
3              0.857143  0.909091  0.882353   33.000000
accuracy       0.745455  0.745455  0.745455    0.745455
macro avg      0.689868  0.689773  0.688259  110.000000
weighted avg   0.734544  0.745455  0.738361  110.000000
</code></pre>
<h3 id="training-and-validation-losses-1">Training and Validation Losses</h3>
<p>The plot of training loss shows a consistent decrease, indicating that the model is effectively learning from the data. The vlaidation loss decreases alongside the training loss but begins to plateau, suggesting that the model might be nearing its learning capacity with the current configuration and dataset.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># take the tracked losses from thet training loop</span>
</span></span><span class="line"><span class="cl"><span class="n">plot_losses</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_64_0.png" type="" alt="png"  /></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">visualize_predictions</span><span class="p">(</span><span class="n">model_vit</span><span class="p">,</span> <span class="n">vis_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_65_0.png" type="" alt="png"  /></p>
<h2 id="using-the-model-for-inference-on-new-data">Using the Model for Inference on New Data</h2>
<p>Let&rsquo;s try out the model on new, unseen data. This photo did not come from a dataset, but rather from a friend of mine who wants to know her cat&rsquo;s mood.</p>
<p>For inference, we first need to apply some simple transformation (not augmentation). In this case, we can use our previously defined <code>val_transforms</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">val_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                         <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span></code></pre></div><p>Then, we can define a simple function to open, transform and add a batch dimension to the file we want to pass.</p>
<p>Then, the function will pass the image to the model to make inference. This latter step is done in a way so that the gradients are not computed, and the image data is passed as a forward pass only.</p>
<p>Finally, we will include a de-normalization to the transformed image, so that we can display it along with the predicted label.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Function to classify a single image and display it with the predicted label</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">classify_and_display_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">val_transforms</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">image_tensor</span> <span class="o">=</span> <span class="n">transformed_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># Disable gradient calculation</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">[</span><span class="n">predicted</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Convert the transformed image tensor back to a PIL image for display</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transformed_image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Change from (C, H, W) to (H, W, C)</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transformed_image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Denormalize the image</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">std</span> <span class="o">*</span> <span class="n">transformed_image</span> <span class="o">+</span> <span class="n">mean</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Display the image with the predicted label</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Predicted Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Example usage</span>
</span></span><span class="line"><span class="cl"><span class="n">classify_and_display_image</span><span class="p">(</span><span class="s1">&#39;gato.jpg&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_67_0.png" type="" alt="png"  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>This tutorial guides you through creating a cat expression classifier using convolutional neural networks with ResNet18 and later with a Vision Transformer (ViT). It demonstrates how to apply transfer learning to improve efficiency and accuracy with limited data.</p>
<p>The guide is structured to provide clear steps and practical examples for each phase of the project, from data preprocessing and model training to evaluation. By breaking down complex concepts and processes into manageable parts, it ensures that readers can easily follow along and apply these techniques to their projects.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
