<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Bayesian Linear Regression on Victor Flores, PhD</title>
    <link>http://localhost:1313/tags/bayesian-linear-regression/</link>
    <description>Recent content in Bayesian Linear Regression on Victor Flores, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 10 Nov 2023 14:53:29 +0800</lastBuildDate><atom:link href="http://localhost:1313/tags/bayesian-linear-regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bayesian Linear Regression with Julia and Turing.jl</title>
      <link>http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/</link>
      <pubDate>Fri, 10 Nov 2023 14:53:29 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20231110_bayesian_linear_regression_julia/20231110_bayesian_linear_regression_julia/</guid>
      <description>Learn the basics of Bayesian linear regression using Julia and Turing.jl. This tutorial covers model formulation, implementation, and interpretation through a practical example.</description>
      <content:encoded><![CDATA[<hr>
<h2 id="finding-a-linear-relationship-between-height-and-weight-using-bayesian-methods">Finding a Linear Relationship Between Height and Weight Using Bayesian Methods</h2>
<h3 id="problem-statement">Problem Statement</h3>
<p>You have some data on the relationship between the height and weight of some people, and you want to fit a linear model of the form:</p>
<p>$$y = \alpha + \beta x + \varepsilon$$</p>
<p>where $y$ is the weight, $x$ is the height, $\alpha$ is the intercept, $\beta$ is the slope, and $\varepsilon$ is the error term. You want to use Bayesian inference to estimate the posterior distributions of $\alpha$ and $\beta$ given the data and some prior assumptions. You also want to use probabilistic programming to implement the Bayesian model and perform inference using a package like <code>Turing.jl</code>.</p>
<p>Your task is to write the code in Julia that can generate some synthetic data (or use an existing data set), define the Bayesian linear regression model, and sample from the posterior distributions using Hamiltonian Monte Carlo (HMC).</p>
<h6 id="credit">Credit</h6>
<p>This exercise is heavily inspired, and mostly taken from, the doggo&rsquo;s tutorial. Please visit his <a href="https://www.youtube.com/@doggodotjl">Youtube channel here</a>, it&rsquo;s an amazing starting point for Julia programming!</p>
<h3 id="import-the-necessary-packages">Import the Necessary Packages</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="p">,</span> <span class="n">Turing</span><span class="p">,</span> <span class="n">CSV</span><span class="p">,</span> <span class="n">DataFrames</span><span class="p">,</span> <span class="n">Plots</span><span class="p">,</span> <span class="n">StatsPlots</span><span class="p">,</span> <span class="n">LaTeXStrings</span>
</span></span></code></pre></div><h3 id="bayesian-workflow">Bayesian Workflow</h3>
<p>For this exercise, I will implement the following workflow:</p>
<ul>
<li>Collect data: this will be implemented by downloading the relevant data</li>
<li>Build a Bayesian model: will use <code>Turing.jl</code> to build the model</li>
<li>Infer the posterior distributions of the parameters $\alpha$ and $\beta$</li>
<li>Evaluate the fit of the model</li>
</ul>
<h4 id="collecting-the-data">Collecting the data</h4>
<p>The data to be analyzed will be the height vs. weight data from:
<a href="https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset">https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset</a>.</p>
<p>Since the dataset is too large, we will select only the first 1000 entries.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># collect data</span>
</span></span><span class="line"><span class="cl"><span class="c"># this data set was downloaded from kaggle:</span>
</span></span><span class="line"><span class="cl"><span class="c"># https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="s">&#34;data&#34;</span><span class="p">,</span> <span class="s">&#34;SOCR-HeightWeight.csv&#34;</span><span class="p">),</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># select only 100 entries</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="mi">1000</span><span class="p">,</span> <span class="o">:</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">first</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span></code></pre></div><div><div style = "float: left;"><span>5Ã—3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">Index</th><th style = "text-align: left;">Height(Inches)</th><th style = "text-align: left;">Weight(Pounds)</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">1</td><td style = "text-align: right;">65.7833</td><td style = "text-align: right;">112.993</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">2</td><td style = "text-align: right;">71.5152</td><td style = "text-align: right;">136.487</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">3</td><td style = "text-align: right;">69.3987</td><td style = "text-align: right;">153.027</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">4</td><td style = "text-align: right;">68.2166</td><td style = "text-align: right;">142.335</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">5</td><td style = "text-align: right;">67.7878</td><td style = "text-align: right;">144.297</td></tr></tbody></table></div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># change the column headers for easier access</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#34;index&#34;</span><span class="p">,</span><span class="s">&#34;height&#34;</span><span class="p">,</span><span class="s">&#34;weight&#34;</span><span class="p">];</span> <span class="n">rename!</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="kt">Symbol</span><span class="o">.</span><span class="p">(</span><span class="n">colnames</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">first</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span></code></pre></div><div><div style = "float: left;"><span>5Ã—3 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "header"><th class = "rowNumber" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">index</th><th style = "text-align: left;">height</th><th style = "text-align: left;">weight</th></tr><tr class = "subheader headerLastRow"><th class = "rowNumber" style = "font-weight: bold; text-align: right;"></th><th title = "Int64" style = "text-align: left;">Int64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th></tr></thead><tbody><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">1</td><td style = "text-align: right;">65.7833</td><td style = "text-align: right;">112.993</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">2</td><td style = "text-align: right;">71.5152</td><td style = "text-align: right;">136.487</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">3</td><td style = "text-align: right;">69.3987</td><td style = "text-align: right;">153.027</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">4</td><td style = "text-align: right;">68.2166</td><td style = "text-align: right;">142.335</td></tr><tr><td class = "rowNumber" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">5</td><td style = "text-align: right;">67.7878</td><td style = "text-align: right;">144.297</td></tr></tbody></table></div>
<h4 id="visualizing-the-data">Visualizing the Data</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot_data</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Height vs. Weight&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">xlabel</span> <span class="o">=</span> <span class="s">&#34;Height (in)&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">ylabel</span> <span class="o">=</span> <span class="s">&#34;Weight (lb)&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20231110_Bayesian_Linear_Regression_Julia/output_9_0.svg" type="" alt="svg"  /></p>
<h4 id="building-a-bayesian-model-with-turingjl">Building a Bayesian model with <code>Turing.jl</code>.</h4>
<p>First, we assume that the weight is a variable dependent on the height. Thus, we can express the Bayesian model as:</p>
<p>$$y\sim N(\alpha + \beta^{T}\mathbf{X}, \sigma^2)$$</p>
<p>The above means that we assume that the data follows a normal distribution (in this case, a multivariate normal distribution), whose standard deviation is Ïƒ and its mean is the linear relationship $\alpha + \beta^{T}\mathbf{X}$.</p>
<p>Next, we need to assign priors to the variables $\alpha$, $\beta$ and $\sigma^2$. The latter is a measure of the uncertainty in <em>the model</em>.</p>
<p>So, the priors will be assigned as follows:</p>
<p>$$\alpha \sim N(0,10)$$
$$\beta \sim U(0,50)$$
$$\sigma^{2} \sim TN(0,100;0,\infty)$$</p>
<p>The last distribution is a <em>truncated normal distribution</em> bounded from 0 to $\infty$.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">blr</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c"># priors:</span>
</span></span><span class="line"><span class="cl">	<span class="n">Î±</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="c"># intercept</span>
</span></span><span class="line"><span class="cl">	<span class="n">Î²</span> <span class="o">~</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">Ïƒ</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># variance standard distribution</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c"># likelihood</span>
</span></span><span class="line"><span class="cl">	<span class="c"># the likelihood in this case means that I assume that the data follows a</span>
</span></span><span class="line"><span class="cl">	<span class="c"># multivariate normal distribution, whose uncertainty is Ïƒ, and its mean is the linear relationship:</span>
</span></span><span class="line"><span class="cl">	<span class="n">avg_weight</span> <span class="o">=</span> <span class="n">Î±</span> <span class="o">.+</span> <span class="p">(</span><span class="n">Î²</span><span class="o">.*</span><span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c"># build the model</span>
</span></span><span class="line"><span class="cl">	<span class="n">weight</span> <span class="o">~</span> <span class="n">MvNormal</span><span class="p">(</span><span class="n">avg_weight</span><span class="p">,</span> <span class="n">Ïƒ</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>blr (generic function with 2 methods)
</code></pre>
<p>The next step is to perform Bayesian inference. <em>Crank up the Bayes!</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># crank up the bayes!</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">samples</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(),</span> <span class="n">samples</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mFound initial step size
[36m[1mâ”” [22m[39m  Ïµ = 9.765625e-5
[32mSampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:11[39m9m





Chains MCMC chain (1000Ã—15Ã—1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 31.4 seconds
Compute duration  = 31.4 seconds
parameters        = Î±, Î², Ïƒ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m [0m â‹¯
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m [0m â‹¯

           Î±   -34.8414    7.6414    0.4117   344.5155   365.1189    1.0038    â‹¯
           Î²     2.3859    0.1124    0.0060   345.5269   345.0618    1.0039    â‹¯
           Ïƒ    10.3030    0.2239    0.0100   509.4680   389.9078    1.0016    â‹¯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m     2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

           Î±   -49.8948   -39.7950   -34.9188   -29.8116   -19.8403
           Î²     2.1673     2.3108     2.3872     2.4580     2.6100
           Ïƒ     9.8649    10.1550    10.3018    10.4554    10.7449
</code></pre>
<h4 id="visualizing-the-mcmc-diagnostics-and-summarizing-the-results">Visualizing the MCMC Diagnostics and Summarizing the Results</h4>
<p>Now that we have performed Bayesian inference using the <code>NUTS()</code> algorithm, we can visualize the results. Addisionally, call for a summary of the statistics of the inferred posterior distributions of $\theta$.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">summarize</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><pre><code> [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m [0m â‹¯
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m [0m â‹¯

           Î±   -34.8414    7.6414    0.4117   344.5155   365.1189    1.0038    â‹¯
           Î²     2.3859    0.1124    0.0060   345.5269   345.0618    1.0039    â‹¯
           Ïƒ    10.3030    0.2239    0.0100   509.4680   389.9078    1.0016    â‹¯
[36m                                                                1 column omitted[0m
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20231110_Bayesian_Linear_Regression_Julia/output_16_0.svg" type="" alt="svg"  /></p>
<h5 id="visualizing-the-results">Visualizing the results</h5>
<p>It is worth noting that the results from a Bayesian Linear Regression is not one single regression line, but many. From PyMC&rsquo;s <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html">Generalized Linear Regression tutorial</a>:</p>
<blockquote>
<p>In GLMs, we do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. We can manually generate these regression lines using the posterior samples directly.</p>
</blockquote>
<p>What this means is that if we want to visualize all the lines that are generated by the parameter posterior distribution sample pool, we need to generate one line per sample set, and then we can plot them all. This procedure is executed next.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># plot all the sample regressions</span>
</span></span><span class="line"><span class="cl"><span class="c"># this method was taken from: https://www.youtube.com/watch?v=EgrrtZEVOv0&amp;t=1113s</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">samples</span>
</span></span><span class="line"><span class="cl">	<span class="n">Î±</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>    <span class="c">#chain[row, column, chain_ID]</span>
</span></span><span class="line"><span class="cl">	<span class="n">Î²</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="n">ÏƒÂ²</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="n">plot!</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="n">x</span> <span class="o">-&gt;</span> <span class="n">Î±</span> <span class="o">+</span> <span class="n">Î²</span><span class="o">*</span><span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="c"># samples</span>
</span></span><span class="line"><span class="cl">		<span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="ss">:orange</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="c"># error</span>
</span></span><span class="line"><span class="cl">        <span class="n">ribbon</span> <span class="o">=</span> <span class="n">ÏƒÂ²</span><span class="p">,</span> <span class="n">fillalpha</span> <span class="o">=</span> <span class="mf">0.002</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>	
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plot_data</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20231110_Bayesian_Linear_Regression_Julia/output_18_0.svg" type="" alt="svg"  /></p>
<h3 id="using-the-regression-model-to-make-predictions">Using the Regression Model to Make Predictions</h3>
<p>Select the heights for which we want to predict the weights and then run the prediction command from <code>Turing</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">pred_height</span> <span class="o">=</span> <span class="p">[</span><span class="mi">62</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">71</span><span class="p">,</span> <span class="mi">67</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">blr</span><span class="p">(</span><span class="n">pred_height</span><span class="p">,</span> <span class="nb">missing</span><span class="p">),</span> <span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Chains MCMC chain (1000Ã—6Ã—1 Array{Float64, 3}):

Iterations        = 1:1:1000
Number of chains  = 1
Samples per chain = 1000
parameters        = weight[1], weight[2], weight[3], weight[4], weight[5], weight[6]
internals         = 

Summary Statistics
 [1m parameters [0m [1m     mean [0m [1m     std [0m [1m    mcse [0m [1m  ess_bulk [0m [1m ess_tail [0m [1m    rhat [0m [1m[0m â‹¯
 [90m     Symbol [0m [90m  Float64 [0m [90m Float64 [0m [90m Float64 [0m [90m   Float64 [0m [90m  Float64 [0m [90m Float64 [0m [90m[0m â‹¯

   weight[1]   113.6815   10.3344    0.3270    997.5393   947.2109    0.9993   â‹¯
   weight[2]   165.3164   10.8352    0.3744    832.5405   818.6640    1.0008   â‹¯
   weight[3]   143.8911   10.5355    0.3461    929.5467   874.2977    0.9993   â‹¯
   weight[4]   132.3417   10.4836    0.3448    921.6347   943.0320    1.0007   â‹¯
   weight[5]   134.7606   10.7046    0.3350   1023.8876   977.6814    1.0025   â‹¯
   weight[6]   124.9423   10.2245    0.3247    993.9282   867.7391    0.9991   â‹¯
[36m                                                                1 column omitted[0m

Quantiles
 [1m parameters [0m [1m     2.5% [0m [1m    25.0% [0m [1m    50.0% [0m [1m    75.0% [0m [1m    97.5% [0m
 [90m     Symbol [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m [90m  Float64 [0m

   weight[1]    93.9378   106.3972   113.6943   120.8093   134.9264
   weight[2]   142.4871   158.4933   165.5406   172.7313   184.7437
   weight[3]   122.8292   137.0108   144.0339   151.1920   164.2645
   weight[4]   111.8872   125.3733   132.1726   139.2690   153.7222
   weight[5]   113.9147   127.4356   135.0149   142.1375   154.5537
   weight[6]   105.3221   118.0098   125.1640   131.6011   145.2976
</code></pre>
<h4 id="visualize-the-distributions-of-the-predicted-weights">Visualize the Distributions of the Predicted Weights</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20231110_Bayesian_Linear_Regression_Julia/output_22_0.svg" type="" alt="svg"  /></p>
<p>Finally, to obtain a point estimate, compute the mean weight prediction for each height.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">mean_predictions</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Mean
 [1m parameters [0m [1m     mean [0m
 [90m     Symbol [0m [90m  Float64 [0m

   weight[1]   113.6815
   weight[2]   165.3164
   weight[3]   143.8911
   weight[4]   132.3417
   weight[5]   134.7606
   weight[6]   124.9423
</code></pre>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
