<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on Victor Flores, PhD</title>
    <link>http://localhost:1313/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Victor Flores, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 07 Feb 2025 19:52:15 +0700</lastBuildDate><atom:link href="http://localhost:1313/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building a Virtual Theremin with MediaPipe and Pure Data</title>
      <link>http://localhost:1313/posts/20250208_theremin_mediapipe/theremin_mediapipe/</link>
      <pubDate>Fri, 07 Feb 2025 19:52:15 +0700</pubDate>
      
      <guid>http://localhost:1313/posts/20250208_theremin_mediapipe/theremin_mediapipe/</guid>
      <description>Using hand tracking to create a virtual theremin with MediaPipe and Pure Data.</description>
      <content:encoded><![CDATA[<p>I recently worked on a fun project where I used <strong>MediaPipe</strong> for finger tracking and interfaced it with <strong>Pure Data</strong> to create a simple virtual theremin. The idea was to control pitch and volume using hand movements, without touching any physical object.</p>
<p>This blog post provides an overview of the project, the steps I followed, and a few code snippets to illustrate key aspects of the implementation.</p>
<hr>
<h2 id="demo-video">Demo Video</h2>
<p>Before diving into the details, check out a quick demo of the theremin in action:</p>
<video width="100%" controls>
  <source src="/videos/theremin_puredata.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<h2 id="project-overview">Project Overview</h2>
<p>A theremin is a touchless musical instrument that produces sound based on the position of the player&rsquo;s hands. For this project, I used:</p>
<ul>
<li><strong>MediaPipe Hand Tracking</strong> to detect finger positions</li>
<li><strong>OpenCV</strong> to visualize the hand movements</li>
<li><strong>OSC (Open Sound Control)</strong> to send data to <strong>Pure Data</strong>, which handled the sound synthesis</li>
</ul>
<p>The result is a simple but effective virtual instrument that lets you manipulate sound using only hand gestures.</p>
<hr>
<h2 id="how-it-works">How It Works</h2>
<ol>
<li><strong>Track Hand Landmarks:</strong> Using MediaPipe, we detect hands and extract the positions of key landmarks (fingertips, wrist, etc.).</li>
<li><strong>Define Control Areas:</strong> We set up an <strong>ON/OFF button</strong> on the screen to enable or disable sound.</li>
<li><strong>Map Hand Movements to Sound Parameters:</strong>
<ul>
<li>Left hand controls <strong>volume</strong> (vertical movement).</li>
<li>Right hand controls <strong>pitch</strong> (horizontal movement).</li>
</ul>
</li>
<li><strong>Send Data to Pure Data:</strong> We use OSC messages to send pitch and volume values to a Pure Data patch, where they are converted into sound. The patch takes these values, processes them, and routes them to an oscillator and an amplitude controller, translating hand gestures into musical notes. This setup mimics the behavior of a real theremin, producing pitch and volume variations.</li>
</ol>
<hr>
<h2 id="key-code-snippets">Key Code Snippets</h2>
<h3 id="1-tracking-hand-landmarks-with-mediapipe">1. Tracking Hand Landmarks with MediaPipe</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">mediapipe</span> <span class="k">as</span> <span class="nn">mp</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mp_hands</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">solutions</span><span class="o">.</span><span class="n">hands</span>
</span></span><span class="line"><span class="cl"><span class="n">hands</span> <span class="o">=</span> <span class="n">mp_hands</span><span class="o">.</span><span class="n">Hands</span><span class="p">(</span><span class="n">min_detection_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_tracking_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">while</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="n">frame_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span> <span class="o">=</span> <span class="n">hands</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">frame_rgb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">results</span><span class="o">.</span><span class="n">multi_hand_landmarks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">hand_landmarks</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">multi_hand_landmarks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Process landmarks here</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s2">&#34;Hand Tracking&#34;</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s2">&#34;q&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">break</span>
</span></span><span class="line"><span class="cl"><span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</span></span></code></pre></div><p>This snippet initializes the webcam, processes frames, and detects hands in real time.</p>
<hr>
<h3 id="2-detecting-button-presses-for-onoff-controls">2. Detecting Button Presses for ON/OFF Controls</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">check_button_press</span><span class="p">(</span><span class="n">landmarks</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="n">radius</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">index_tip</span> <span class="o">=</span> <span class="n">landmarks</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">middle_tip</span> <span class="o">=</span> <span class="n">landmarks</span><span class="p">[</span><span class="mi">12</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">index_coords</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">index_tip</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">width</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">index_tip</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">height</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">middle_coords</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">middle_tip</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">width</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">middle_tip</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">height</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">index_coords</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">center</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist_middle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">middle_coords</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">center</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dist_index</span> <span class="o">&lt;=</span> <span class="n">radius</span> <span class="ow">and</span> <span class="n">dist_middle</span> <span class="o">&lt;=</span> <span class="n">radius</span>
</span></span></code></pre></div><p>This function checks whether both the index and middle fingers are inside a circular region, acting as an ON/OFF switch.</p>
<hr>
<h3 id="3-mapping-hand-movements-to-sound-parameters">3. Mapping Hand Movements to Sound Parameters</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">map_hand_to_pitch_or_volume</span><span class="p">(</span><span class="n">handedness</span><span class="p">,</span> <span class="n">index_tip_coords</span><span class="p">,</span> <span class="n">ring_tip_coords</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">handedness</span> <span class="o">==</span> <span class="s1">&#39;Right&#39;</span><span class="p">:</span>  <span class="c1"># Left hand for volume</span>
</span></span><span class="line"><span class="cl">        <span class="n">normalized_y</span> <span class="o">=</span> <span class="n">ring_tip_coords</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">height</span>
</span></span><span class="line"><span class="cl">        <span class="n">volume</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">max</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">normalized_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="n">volume</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">handedness</span> <span class="o">==</span> <span class="s1">&#39;Left&#39;</span><span class="p">:</span>  <span class="c1"># Right hand for pitch</span>
</span></span><span class="line"><span class="cl">        <span class="n">normalized_x</span> <span class="o">=</span> <span class="n">index_tip_coords</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">width</span>
</span></span><span class="line"><span class="cl">        <span class="n">pitch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">127</span> <span class="o">*</span> <span class="p">(</span><span class="n">normalized_x</span><span class="p">))</span>  <span class="c1"># Map to MIDI range</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">pitch</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span></code></pre></div><p>This function maps <strong>vertical movement</strong> of the left hand to <strong>volume</strong> and <strong>horizontal movement</strong> of the right hand to <strong>pitch</strong>.</p>
<hr>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>This was a blast to build! Thereâ€™s something very satisfying about making noise by just waving your hands around like some kind of musical wizard. Iâ€™m keeping the details of the Pure Data setup out of this post for brevity (and hey, a little mystery never hurt anyone), but the core idea is simple: detect hands, map motion to sound, and have fun.</p>
<p>If you have any thoughts, ideas, or want to collaborate to expand this project, reach out! Iâ€™d love to hear what you think and see where we can take this next.</p>
<hr>
]]></content:encoded>
    </item>
    
    <item>
      <title>Transfer Learning Classifier Again... with Julia!</title>
      <link>http://localhost:1313/posts/20240521_julia_transfer_learning_v5/20240521_julia_transfer_learning_v5/</link>
      <pubDate>Tue, 21 May 2024 22:38:29 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240521_julia_transfer_learning_v5/20240521_julia_transfer_learning_v5/</guid>
      <description>Replicating the cat mood classifier, this time using Julia and Flux.jl.</description>
      <content:encoded><![CDATA[<hr>
<p><img loading="lazy" src="/images/20240521_julia_transfer_learning_v5/intro.png" type="" alt="image"  /></p>
<h2 id="introduction">Introduction</h2>
<p>This guide demonstrates how to apply transfer learning using a pre-trained vision model to classify cat moods based on their facila expressions. We&rsquo;ll learn how to handle custom data setups.</p>
<p>In this demonstration, we recreate the exercise done in PyTorch, <a href="https://vflores-io.github.io/posts/20240515_cat_mood_classification/">available here</a>. Since that demonstration is quite detailed, we keep it pretty straightforward here.</p>
<h4 id="motivation--credit">Motivation &amp; Credit</h4>
<p>When I thought about learning how to implement a computer vision classification model for transfer learning in Julia and <code>Flux</code>, I immediately came upon two roadblocks:</p>
<ol>
<li>Since I am not an expert in Julia, I found the documentation to be a bit difficult to access (again, this is just me!).</li>
<li>There are not many tutorials or resources to illustrate this particular case.</li>
</ol>
<p>Therefore I took it upon myself to put things together and make a demonstration that would hopefully be useful for someone who might not be an expert in Flux (or Julia).</p>
<p>This particular demo was inspired by a combination of the following resources:</p>
<ul>
<li><a href="https://towardsdatascience.com/transfer-learning-and-twin-network-for-image-classification-using-flux-jl-cbe012ced146">Transfer Learning and Twin Network for Image Classification using <code>Flux.jl</code></a></li>
<li><a href="https://github.com/FluxML/model-zoo/tree/master/tutorials/transfer_learning"><code>Flux.jl</code>&rsquo;s Model Zoo Tutorial</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"><code>PyTorch</code> Transfer Learning for Computer Vision Tutorial</a></li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<p>We will use a pre-trained <code>ResNet18</code> model, initially trained on a general dataset, and fine-tune it for our specific task of classifying cat moods.</p>
<h3 id="initialization">Initialization</h3>
<p>First, we activate the current directory as our project environment by calling the package manager <code>Pkg</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Pkg</span>
</span></span><span class="line"><span class="cl"><span class="n">Pkg</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="s">&#34;.&#34;</span><span class="p">)</span> 
</span></span></code></pre></div><p>Then we will import the required packages. Of course, this is also assuming that one has already added the relevant packages into the environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Pkg</span>
</span></span><span class="line"><span class="cl"><span class="n">Pkg</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="s">&#34;.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Random</span><span class="o">:</span> <span class="n">shuffle!</span>
</span></span><span class="line"><span class="cl"><span class="k">import</span> <span class="n">Base</span><span class="o">:</span> <span class="n">length</span><span class="p">,</span> <span class="n">getindex</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Images</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Flux</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Flux</span><span class="o">:</span> <span class="n">update!</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">DataAugmentation</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Metalhead</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">MLUtils</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">DataFrames</span><span class="p">,</span> <span class="n">CSV</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Plots</span>
</span></span></code></pre></div><pre><code>[32m[1m  Activating[22m[39m project at `H:\My Drive\Projects\Coding\Portfolio\Machine Learning\Julia\Transfer Learning with Flux`
</code></pre>
<h3 id="retrieve-the-data-and-initial-setup">Retrieve the Data and Initial Setup</h3>
<p>First, we specify the paths to the dataset and labels CSV files for training, validation, and test sets. Then, we load these CSV files into <code>DataFrames</code>. Finally, we create vectors of absolute file paths for each image in the dataset.</p>
<p>This setup is essential for organizing the data and ensuring that our model can access the correct images and labels during training and evaluation.</p>
<h4 id="label-structure">Label Structure</h4>
<p>The data set we are using consists of three folders: <code>train</code>, <code>val</code>, <code>test</code>. Each of them contain a set of images of cats. The labels in this case, are in the form of a CSV file that maps the filename with a one-hot encoding to label the classification of the image, i.e. the cat&rsquo;s mood - alarmed, angry, calm, pleased.</p>
<p>The dataset was obtained <a href="https://universe.roboflow.com/mubbarryz/domestic-cats-facial-expressions">here</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># specify the paths to the dataset and labels CSV</span>
</span></span><span class="line"><span class="cl"><span class="n">train_data_path</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/train&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">train_data_csv</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/train/_classes.csv&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_data_path</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/val&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">val_data_csv</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/val/_classes.csv&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">test_data_path</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/test&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">test_data_csv</span> <span class="o">=</span> <span class="s">&#34;data/cat_expression_data/test/_classes.csv&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># load the CSV file containing the labels</span>
</span></span><span class="line"><span class="cl"><span class="n">train_labels_df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">train_data_csv</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_labels_df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">test_data_csv</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_labels_df</span> <span class="o">=</span> <span class="n">CSV</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">val_data_csv</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># setup filepaths to the files as vectors</span>
</span></span><span class="line"><span class="cl"><span class="n">train_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">abspath</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="n">train_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">for</span> <span class="n">filename</span> <span class="k">in</span> <span class="n">train_labels_df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">abspath</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="n">test_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">for</span> <span class="n">filename</span> <span class="k">in</span> <span class="n">test_labels_df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">val_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">abspath</span><span class="p">(</span><span class="n">joinpath</span><span class="p">(</span><span class="n">val_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">for</span> <span class="n">filename</span> <span class="k">in</span> <span class="n">val_labels_df</span><span class="p">[</span><span class="o">!</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
</span></span></code></pre></div><pre><code>110-element Vector{String}:
 
 â‹®
</code></pre>
<h3 id="data-exploration">Data Exploration</h3>
<p>As usual, we take a look at the data to understand what we are working with.</p>
<p>Below we make a couple of functions to visualize the data.</p>
<p>Note that the helper function <code>label_from_row</code> will come in handy later on.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># -----------------------------------------------------------------------#</span>
</span></span><span class="line"><span class="cl"><span class="c"># helper function to extract label from the DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">label_from_row</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># retrieve the label for the image from the DataFrame</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_row</span> <span class="o">=</span> <span class="n">filter</span><span class="p">(</span><span class="n">row</span> <span class="o">-&gt;</span> <span class="n">row</span><span class="o">.</span><span class="n">filename</span> <span class="o">==</span> <span class="n">filename</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_index</span> <span class="o">=</span> <span class="n">findfirst</span><span class="p">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">label_row</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">names</span><span class="p">(</span><span class="n">labels_df</span><span class="p">)[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">label_dict</span><span class="p">[</span><span class="n">label_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="c"># -----------------------------------------------------------------------#</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># function to display a selection of images and their labels</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">show_sample_images_and_labels</span><span class="p">(</span><span class="n">labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">;</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># randomly pick indices for sampling images</span>
</span></span><span class="line"><span class="cl">    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">labels_df</span><span class="p">),</span> <span class="n">num_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sample_filenames</span> <span class="o">=</span> <span class="n">labels_df</span><span class="o">.</span><span class="n">filename</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># calculate number of rows and columns for the grid layuot</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_cols</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="kt">Int</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_rows</span> <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c"># prepare a plot with a grid layout for the images</span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">layout</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">),</span> <span class="n">size</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">legend</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span> <span class="n">grid</span> <span class="o">=</span> <span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># load and plot each sampled image</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span> <span class="k">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">sample_filenames</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">img_path</span> <span class="o">=</span> <span class="n">joinpath</span><span class="p">(</span><span class="n">train_data_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">img</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>   <span class="c"># load the image from the file</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c"># retrieve the label for the image from the DataFrame</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span> <span class="o">=</span> <span class="n">label_from_row</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">plot!</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">img</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="n">label</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">display</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>   <span class="c"># display the plot</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># define a dictionary for label descriptions:</span>
</span></span><span class="line"><span class="cl"><span class="n">label_dict</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">(</span><span class="mi">1</span> <span class="o">=&gt;</span> <span class="s">&#34;alarmed&#34;</span><span class="p">,</span> <span class="mi">2</span> <span class="o">=&gt;</span> <span class="s">&#34;angry&#34;</span><span class="p">,</span> <span class="mi">3</span> <span class="o">=&gt;</span> <span class="s">&#34;calm&#34;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">=&gt;</span> <span class="s">&#34;pleased&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># run the function to show images</span>
</span></span><span class="line"><span class="cl"><span class="n">show_sample_images_and_labels</span><span class="p">(</span><span class="n">train_labels_df</span><span class="p">,</span> <span class="n">label_dict</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240521_julia_transfer_learning_v5/output_6_0.svg" type="" alt="svg"  /></p>
<h3 id="working-with-custom-datasets">Working with Custom Datasets</h3>
<p>When working with custom datasets in Julia, the concepts are similar as in PyTorch, but obviously following Julia&rsquo;s syntax.</p>
<p>In essence, we read the CSV files containing image file paths and their corresponding labels into DataFrames. We then create functions to handle data loading and transformations, such as resizing and normalizing images. This approach is similar to PyTorch&rsquo;s <code>Dataset</code>.</p>
<p>Let&rsquo;s have a quick look.</p>
<h3 id="create-a-custom-dataset">Create a Custom Dataset</h3>
<p>We define a custom dataset using a <code>struct</code>, which is similar to using a <code>class</code> in Python. The <code>ImageContainer</code> struct stores the image file paths and their corresponding labels in a DataFrame. We then create instances of this <code>struct</code> for the training, validation, and test datasets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">struct</span> <span class="kt">ImageContainer</span><span class="p">{</span><span class="kt">T</span><span class="o">&lt;:</span><span class="kt">Vector</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">img</span><span class="o">::</span><span class="kt">T</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels_df</span><span class="o">::</span><span class="kt">DataFrame</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># generate dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ImageContainer</span><span class="p">(</span><span class="n">train_filepaths</span><span class="p">,</span> <span class="n">train_labels_df</span><span class="p">);</span>   
</span></span><span class="line"><span class="cl"><span class="n">val_dataset</span> <span class="o">=</span> <span class="n">ImageContainer</span><span class="p">(</span><span class="n">val_filepaths</span><span class="p">,</span> <span class="n">val_labels_df</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">ImageContainer</span><span class="p">(</span><span class="n">test_filepaths</span><span class="p">,</span> <span class="n">test_labels_df</span><span class="p">);</span>
</span></span></code></pre></div><h4 id="create-the-data-loaders">Create the Data Loaders</h4>
<p>In this section, we set up data loaders for our custom dataset in Julia, similar to how data loaders are used in PyTorch to manage batching and shuffling of data.</p>
<ol>
<li>
<p>Call helper Function: <code>label_from_row()</code> : This function extracts the label from the DataFrame for a given image file. It finds the index of the column with a value of 1, indicating the class.</p>
</li>
<li>
<p>Length and Indexing:</p>
</li>
</ol>
<ul>
<li><code>length(data::ImageContainer)</code>: Defines the length method to return the number of images in the dataset. Similar to PyTorch&rsquo;s <code>__len__</code>.</li>
<li><code>getindex(data::ImageContainer, idx::Int)</code>: This method is similar to PyTorchâ€™s <code>__getitem__</code>. It loads an image, applies transformations, and returns the processed image along with its label.</li>
</ul>
<ol start="3">
<li>Data Augmentation and Transformations:</li>
</ol>
<ul>
<li>pipeline: Defines a transformation pipeline for scaling and cropping images.</li>
<li>transforms(image, labels_df): Inside getindex, this function applies the transformations to the image and normalizes it using the predefined mean and standard deviation values.</li>
</ul>
<ol start="4">
<li>DataLoaders:</li>
</ol>
<ul>
<li><code>train_loader</code> and <code>val_loader</code>: These DataLoader objects manage batching, shuffling, and parallel processing of the training and validation datasets, similar to <code>torch.utils.data.DataLoader</code> in PyTorch</li>
</ul>
<h5 id="notes-on-implementing-custom-data-containers">Notes on Implementing Custom Data Containers</h5>
<p>According to the documentation for MLUtils.DataLoader (<a href="https://fluxml.ai/Flux.jl/stable/data/mlutils/">see here</a>), custom data containers should implement Base.length instead of  <code>numobs</code>, and Base.getindex instead of <code>getobs</code>, unless there&rsquo;s a difference between these functions and the base methods for multi-dimensional arrays.</p>
<p>Base.length: Should be implemented to return the number of observations. This is akin to PyTorch&rsquo;s <code>__len__</code>.
Base.getindex: Should be implemented to handle indexing of the dataset, similar to PyTorch&rsquo;s <code>__getitem__</code>.
These methods ensure that the data is returned in a form suitable for the learning algorithm, maintaining consistency whether the index is a scalar or vector.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">length</span><span class="p">(</span><span class="n">data</span><span class="o">::</span><span class="kt">ImageContainer</span><span class="p">)</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">im_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">DATA_MEAN</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485f0</span><span class="p">,</span> <span class="mf">0.456f0</span><span class="p">,</span> <span class="mf">0.406f0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">DATA_STD</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229f0</span><span class="p">,</span> <span class="mf">0.224f0</span><span class="p">,</span> <span class="mf">0.225f0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># define a transformation pipeline</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline</span> <span class="o">=</span> <span class="n">DataAugmentation</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">ScaleKeepAspect</span><span class="p">(</span><span class="n">im_size</span><span class="p">),</span> <span class="n">CenterCrop</span><span class="p">(</span><span class="n">im_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">getindex</span><span class="p">(</span><span class="n">data</span><span class="o">::</span><span class="kt">ImageContainer</span><span class="p">,</span> <span class="n">idx</span><span class="o">::</span><span class="kt">Int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">img</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels_df</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">labels_df</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">function</span> <span class="n">transforms</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline</span> <span class="o">=</span> <span class="n">ScaleKeepAspect</span><span class="p">(</span><span class="n">im_size</span><span class="p">)</span> <span class="o">|&gt;</span> <span class="n">CenterCrop</span><span class="p">(</span><span class="n">im_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_img</span> <span class="o">=</span> <span class="n">Images</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_img</span> <span class="o">=</span> <span class="n">apply</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">Image</span><span class="p">(</span><span class="n">_img</span><span class="p">))</span> <span class="o">|&gt;</span> <span class="n">itemdata</span>
</span></span><span class="line"><span class="cl">        <span class="n">img</span> <span class="o">=</span> <span class="n">collect</span><span class="p">(</span><span class="n">channelview</span><span class="p">(</span><span class="n">float32</span><span class="o">.</span><span class="p">(</span><span class="n">RGB</span><span class="o">.</span><span class="p">(</span><span class="n">_img</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">        <span class="n">img</span> <span class="o">=</span> <span class="n">permutedims</span><span class="p">((</span><span class="n">img</span> <span class="o">.-</span> <span class="n">DATA_MEAN</span><span class="p">)</span> <span class="o">./</span> <span class="n">DATA_STD</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">label</span> <span class="o">=</span> <span class="n">label_from_row</span><span class="p">(</span><span class="n">labels_df</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">transforms</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">labels_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">batchsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">collate</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">parallel</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">batchsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">collate</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">parallel</span> <span class="o">=</span> <span class="nb">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">);</span>
</span></span></code></pre></div><h2 id="model-definition">Model Definition</h2>
<p>Here we will load the model with <code>Metalhead.jl</code> and change the classifier &ldquo;head&rdquo; of the architecture to suit our classification need.</p>
<p>We will use this to select the classifier head of the model and change it.</p>
<p>For the fine-tuning portion of this exercise will follow the <a href="https://github.com/FluxML/model-zoo/tree/master/tutorials%2Ftransfer_learning">model zoo documentation</a>:</p>
<hr>
<p><img loading="lazy" src="/images/20240521_julia_transfer_learning_v5/109ebfef-0cea-49b5-98d5-fcd19f0f9596.png" type="" alt="image.png"  /></p>
<hr>
<p>Let&rsquo;s try it out with the <code>ResNet18</code> model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># load the pre-trained model</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_model</span> <span class="o">=</span> <span class="n">ResNet</span><span class="p">(</span><span class="mi">18</span><span class="p">;</span> <span class="n">pretrain</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span><span class="o">.</span><span class="n">layers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># let&#39;s look at the model</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_model</span>
</span></span></code></pre></div><pre><code>Chain(
  Chain(
    Chain(
      Conv((7, 7), 3 =&gt; 64, pad=3, stride=2, bias=false),  [90m# 9_408 parameters[39m
      BatchNorm(64, relu),              [90m# 128 parameters[39m[90m, plus 128[39m
      MaxPool((3, 3), pad=1, stride=2),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
          NNlib.relu,
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
          NNlib.relu,
          Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
          BatchNorm(64),                [90m# 128 parameters[39m[90m, plus 128[39m
        ),
      ),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        Chain(
          Conv((1, 1), 64 =&gt; 128, stride=2, bias=false),  [90m# 8_192 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
        ),
        Chain(
          Conv((3, 3), 64 =&gt; 128, pad=1, stride=2, bias=false),  [90m# 73_728 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
          NNlib.relu,
          Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
          NNlib.relu,
          Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
          BatchNorm(128),               [90m# 256 parameters[39m[90m, plus 256[39m
        ),
      ),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        Chain(
          Conv((1, 1), 128 =&gt; 256, stride=2, bias=false),  [90m# 32_768 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
        ),
        Chain(
          Conv((3, 3), 128 =&gt; 256, pad=1, stride=2, bias=false),  [90m# 294_912 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
          NNlib.relu,
          Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
          NNlib.relu,
          Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
          BatchNorm(256),               [90m# 512 parameters[39m[90m, plus 512[39m
        ),
      ),
    ),
    Chain(
      Parallel(
        addact(NNlib.relu, ...),
        Chain(
          Conv((1, 1), 256 =&gt; 512, stride=2, bias=false),  [90m# 131_072 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        ),
        Chain(
          Conv((3, 3), 256 =&gt; 512, pad=1, stride=2, bias=false),  [90m# 1_179_648 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
          NNlib.relu,
          Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        ),
      ),
      Parallel(
        addact(NNlib.relu, ...),
        identity,
        Chain(
          Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
          NNlib.relu,
          Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
          BatchNorm(512),               [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        ),
      ),
    ),
  ),
  Chain(
    AdaptiveMeanPool((1, 1)),
    MLUtils.flatten,
    Dense(512 =&gt; 1000),                 [90m# 513_000 parameters[39m
  ),
) [90m        # Total: 62 trainable arrays, [39m11_689_512 parameters,
[90m          # plus 40 non-trainable, 9_600 parameters, summarysize [39m44.654 MiB.
</code></pre>
<p>Now we modify the head, by chaning the last <code>Chain</code> in the model. We change the last layer to output 4 classes (as opposed to the original 1000 classes).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># modify the model</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_infer</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">resnet_model</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">resnet_tune</span> <span class="o">=</span> <span class="n">Chain</span><span class="p">(</span><span class="n">AdaptiveMeanPool</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">Flux</span><span class="o">.</span><span class="n">flatten</span><span class="p">,</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span> <span class="o">=&gt;</span> <span class="mi">4</span><span class="p">))</span>
</span></span></code></pre></div><pre><code>Chain(
  AdaptiveMeanPool((1, 1)),
  Flux.flatten,
  Dense(512 =&gt; 4),                      [90m# 2_052 parameters[39m
) 
</code></pre>
<p><strong>And that&rsquo;s it!</strong> Now, let&rsquo;s just explore both portions of the model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">resnet_infer</span>
</span></span></code></pre></div><pre><code>Chain(
  Chain(
    Conv((7, 7), 3 =&gt; 64, pad=3, stride=2, bias=false),  [90m# 9_408 parameters[39m
    BatchNorm(64, relu),                [90m# 128 parameters[39m[90m, plus 128[39m
    MaxPool((3, 3), pad=1, stride=2),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
        NNlib.relu,
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
        NNlib.relu,
        Conv((3, 3), 64 =&gt; 64, pad=1, bias=false),  [90m# 36_864 parameters[39m
        BatchNorm(64),                  [90m# 128 parameters[39m[90m, plus 128[39m
      ),
    ),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      Chain(
        Conv((1, 1), 64 =&gt; 128, stride=2, bias=false),  [90m# 8_192 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
      ),
      Chain(
        Conv((3, 3), 64 =&gt; 128, pad=1, stride=2, bias=false),  [90m# 73_728 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
        NNlib.relu,
        Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
        NNlib.relu,
        Conv((3, 3), 128 =&gt; 128, pad=1, bias=false),  [90m# 147_456 parameters[39m
        BatchNorm(128),                 [90m# 256 parameters[39m[90m, plus 256[39m
      ),
    ),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      Chain(
        Conv((1, 1), 128 =&gt; 256, stride=2, bias=false),  [90m# 32_768 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
      ),
      Chain(
        Conv((3, 3), 128 =&gt; 256, pad=1, stride=2, bias=false),  [90m# 294_912 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
        NNlib.relu,
        Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
        NNlib.relu,
        Conv((3, 3), 256 =&gt; 256, pad=1, bias=false),  [90m# 589_824 parameters[39m
        BatchNorm(256),                 [90m# 512 parameters[39m[90m, plus 512[39m
      ),
    ),
  ),
  Chain(
    Parallel(
      addact(NNlib.relu, ...),
      Chain(
        Conv((1, 1), 256 =&gt; 512, stride=2, bias=false),  [90m# 131_072 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
      ),
      Chain(
        Conv((3, 3), 256 =&gt; 512, pad=1, stride=2, bias=false),  [90m# 1_179_648 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        NNlib.relu,
        Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
      ),
    ),
    Parallel(
      addact(NNlib.relu, ...),
      identity,
      Chain(
        Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
        NNlib.relu,
        Conv((3, 3), 512 =&gt; 512, pad=1, bias=false),  [90m# 2_359_296 parameters[39m
        BatchNorm(512),                 [90m# 1_024 parameters[39m[90m, plus 1_024[39m
      ),
    ),
  ),
) [90m        # Total: 60 trainable arrays, [39m11_176_512 parameters,
[90m          # plus 40 non-trainable, 9_600 parameters, summarysize [39m42.693 MiB.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">resnet_tune</span>
</span></span></code></pre></div><pre><code>Chain(
  AdaptiveMeanPool((1, 1)),
  Flux.flatten,
  Dense(512 =&gt; 4),                      [90m# 2_052 parameters[39m
) 
</code></pre>
<h3 id="define-evaluation-and-training-functions">Define evaluation and training functions</h3>
<p>Again, will follow the model zoo documentation. Small adaptations will be needed. (These two functions were taken directly from the documentation).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">m_infer</span><span class="p">,</span> <span class="n">m_tune</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">good</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">in</span> <span class="n">val_loader</span>
</span></span><span class="line"><span class="cl">        <span class="n">good</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">(</span><span class="n">Flux</span><span class="o">.</span><span class="n">onecold</span><span class="p">(</span><span class="n">m_tune</span><span class="p">(</span><span class="n">m_infer</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span> <span class="o">.==</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">count</span> <span class="o">+=</span> <span class="n">length</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">round</span><span class="p">(</span><span class="n">good</span> <span class="o">/</span> <span class="n">count</span><span class="p">,</span> <span class="n">digits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">acc</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>eval_f (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">train_epoch!</span><span class="p">(</span><span class="n">model_infer</span><span class="p">,</span> <span class="n">model_tune</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">in</span> <span class="n">loader</span>
</span></span><span class="line"><span class="cl">        <span class="n">infer</span> <span class="o">=</span> <span class="n">model_infer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">model_tune</span><span class="p">)</span> <span class="k">do</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">            <span class="n">Flux</span><span class="o">.</span><span class="n">Losses</span><span class="o">.</span><span class="n">logitcrossentropy</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">infer</span><span class="p">),</span> <span class="n">Flux</span><span class="o">.</span><span class="n">onehotbatch</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="o">:</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">        <span class="n">update!</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">model_tune</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>train_epoch! (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">resnet_opt</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">Flux</span><span class="o">.</span><span class="n">Optimisers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">resnet_tune</span><span class="p">);</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@time</span> <span class="n">train_epoch!</span><span class="p">(</span><span class="n">resnet_infer</span><span class="p">,</span> <span class="n">resnet_tune</span><span class="p">,</span> <span class="n">resnet_opt</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_train</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">resnet_infer</span><span class="p">,</span> <span class="n">resnet_tune</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_eval</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">resnet_infer</span><span class="p">,</span> <span class="n">resnet_tune</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;train&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_train</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;eval&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_eval</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>176.283332 seconds (37.11 M allocations: 98.153 GiB, 6.06% gc time, 143.87% compilation time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.5744
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.5455


 70.815518 seconds (2.42 M allocations: 95.936 GiB, 11.25% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.6823
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6273


 90.463025 seconds (2.42 M allocations: 95.936 GiB, 11.21% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.7032
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6455


 94.362892 seconds (2.42 M allocations: 95.936 GiB, 10.91% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.7433
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6727


116.526515 seconds (2.42 M allocations: 95.936 GiB, 9.62% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.7885
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6909
</code></pre>
<hr>
<h2 id="vision-transformers">Vision Transformers</h2>
<hr>
<p>Similar to the PyTorch demonstration, we can do transfer learning by changing a different computer vision model (Vision Transformer).</p>
<p>Let&rsquo;s get into it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">vit_model</span> <span class="o">=</span> <span class="n">ViT</span><span class="p">(</span><span class="ss">:base</span><span class="p">;</span> <span class="n">pretrain</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span><span class="o">.</span><span class="n">layers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># let&#39;s have a look at the model head, to see how many inputs the head needs</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span></code></pre></div><pre><code>Chain(
  LayerNorm(768),                       [90m# 1_536 parameters[39m
  Dense(768 =&gt; 1000),                   [90m# 769_000 parameters[39m
) [90m                  # Total: 4 arrays, [39m770_536 parameters, 2.940 MiB.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># modify the head</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_infer</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">vit_model</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># notice how we keep the input to the model head</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_tune</span> <span class="o">=</span> <span class="n">Chain</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">LayerNorm</span><span class="p">(</span><span class="mi">768</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">Dense</span><span class="p">(</span><span class="mi">768</span> <span class="o">=&gt;</span> <span class="mi">4</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><pre><code>Chain(
  LayerNorm(768),                       [90m# 1_536 parameters[39m
  Dense(768 =&gt; 4),                      [90m# 3_076 parameters[39m
) [90m                  # Total: 4 arrays, [39m4_612 parameters, 18.352 KiB.
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">vit_opt</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">Flux</span><span class="o">.</span><span class="n">Optimisers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">vit_tune</span><span class="p">);</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@time</span> <span class="n">train_epoch!</span><span class="p">(</span><span class="n">vit_infer</span><span class="p">,</span> <span class="n">vit_tune</span><span class="p">,</span> <span class="n">vit_opt</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_train</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">vit_infer</span><span class="p">,</span> <span class="n">vit_tune</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric_eval</span> <span class="o">=</span> <span class="n">eval_f</span><span class="p">(</span><span class="n">vit_infer</span><span class="p">,</span> <span class="n">vit_tune</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;train&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_train</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@info</span> <span class="s">&#34;eval&#34;</span> <span class="n">metric</span> <span class="o">=</span> <span class="n">metric_eval</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>627.303072 seconds (17.32 M allocations: 291.924 GiB, 4.61% gc time, 3.66% compilation time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.7058
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6273


565.986959 seconds (2.54 M allocations: 291.028 GiB, 4.71% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.8042
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6273


516.041945 seconds (2.54 M allocations: 291.028 GiB, 4.92% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.866
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6818


515.415614 seconds (2.54 M allocations: 291.028 GiB, 4.80% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.8973
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6818


427.423410 seconds (2.54 M allocations: 291.028 GiB, 5.01% gc time)


[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39mtrain
[36m[1mâ”” [22m[39m  metric = 0.9199
[36m[1mâ”Œ [22m[39m[36m[1mInfo: [22m[39meval
[36m[1mâ”” [22m[39m  metric = 0.6727
</code></pre>
<h3 id="save-the-models">Save the Models</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">JLD2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">resnet_model_state</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">resnet_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">vit_model_state</span> <span class="o">=</span> <span class="n">Flux</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">vit_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">jldsave</span><span class="p">(</span><span class="s">&#34;resnet_model.jld2&#34;</span><span class="p">;</span> <span class="n">resnet_model_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">jldsave</span><span class="p">(</span><span class="s">&#34;vit_model.jld2&#34;</span><span class="p">;</span> <span class="n">vit_model_state</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[33m[1mâ”Œ [22m[39m[33m[1mWarning: [22m[39mOpening file with JLD2.MmapIO failed, falling back to IOStream
[33m[1mâ”” [22m[39m[90m@ JLD2 C:\Users\ingvi\.julia\packages\JLD2\7uAqU\src\JLD2.jl:300[39m
[33m[1mâ”Œ [22m[39m[33m[1mWarning: [22m[39mOpening file with JLD2.MmapIO failed, falling back to IOStream
[33m[1mâ”” [22m[39m[90m@ JLD2 C:\Users\ingvi\.julia\packages\JLD2\7uAqU\src\JLD2.jl:300[39m
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">BSON</span><span class="o">:</span> <span class="nd">@save</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@save</span> <span class="s">&#34;resnet_model_sate.bson&#34;</span> <span class="n">resnet_model</span>
</span></span><span class="line"><span class="cl"><span class="nd">@save</span> <span class="s">&#34;vit_model_state.bson&#34;</span> <span class="n">vit_model</span>
</span></span></code></pre></div><h2 id="thank-you">Thank you!</h2>
<p>I hope this demonstration on using Julia and <code>Flux</code> for transfer learning was helpful!</p>
<p>Victor</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Transfer Learning Classifier Using PyTorch</title>
      <link>http://localhost:1313/posts/20240515_cat_mood_classification/20240515_cat_mood_classification/</link>
      <pubDate>Wed, 15 May 2024 14:53:29 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/20240515_cat_mood_classification/20240515_cat_mood_classification/</guid>
      <description>Things we learn here include image data exploration, transfer learning, custom datasets, comparing ML models, saving/loading models and model data, conditional setup for different work environments.</description>
      <content:encoded><![CDATA[<p><a href="https://colab.research.google.com/github/vflores-io/cat_mood/blob/main/cat_mood_classification.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="cat-expression-classifier-using-convolutional-neural-networks">Cat Expression Classifier Using Convolutional Neural Networks</h1>
<hr>
<p>This project aims to build a cat expression classifier with convolutional neural networks (CNNs) using PyTorch. This project serves as an introduction to image classification and also dives into the nuances of handling a specific, custom dataset and adapting pre-trained models for our purposes.</p>
<h3 id="objective">Objective</h3>
<p>The primary objective of this project is to develop a model capable of classifying images of cat faces into one of four moods: alarmed, angry, calm, and pleased. By the end of this tutorial, you will learn how to preprocess image data, leverage transfer learning for image classification, and evaluate a model&rsquo;s performance.</p>
<h3 id="tools-and-techniques">Tools and Techniques</h3>
<p>We will employ PyTorch, a powerful and versatile deep learning library, to construct our CNN. The model of choice for this tutorial is ResNet18, a robust architecture that is commonly used in image recognition tasks. Given the straightforward nature of our classificaiton problem, ResNet18 provides an ecellent balance between complexity and performance.</p>
<h3 id="why-transfer-learning">Why <em>Transfer Learning</em>?</h3>
<p>In this tutorial, we utilize <em>transfer learning</em> to take advantage of a pre-trained ResNet18 model. This approach allows us to use a model that has already learned a significant amount of relevant features from a vast and diverse dataset (ImageNet). By fine-tuning this model to our specific task, we can achieve high accuracy with relatively little data and reduce the computational cost typycally associated with training a deep neural network from scratch.</p>
<h3 id="dataset">Dataset</h3>
<p>The dataset comprises images of cat faces, labeled according to their expressed mood. These images are organized into training, validation, and testing sets, each with a corresponding CSV file which maps filenames to mood labels. This guide will walk you through the process of loading, preprocessing, and augmenting this data to suit the needs of our CNN.</p>
<p>The dataset was obtained <a href="https://universe.roboflow.com/mubbarryz/domestic-cats-facial-expressions">here</a>.</p>
<p>Let&rsquo;s get started!</p>
<h2 id="dataset-exploration">Dataset Exploration</h2>
<h3 id="listing-the-number-of-images-in-each-set-and-visualizing-the-set">Listing the Number of Images in Each Set and Visualizing The Set</h3>
<p>Below we will mount the drive to retrieve the data set files.
Then, will use Python&rsquo;s <code>os</code> module to list the number of images in the dataset. This will give us an idea of the size of the set.</p>
<p>Additionally, we will include a flag to tell the model whether we want to train it or to load a previously saved model&hellip; this will become clear later.</p>
<p>Finally, we will set up a function to visualize some sample images from each set.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># flag to control whether to train the model or load a saved model</span>
</span></span><span class="line"><span class="cl"><span class="n">should_train_resnet</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="n">should_train_vit</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">set_path</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># check if the notebook is running on google colab</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running on Google Colab.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
</span></span><span class="line"><span class="cl">    <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;/content/drive/PATH-TO-YOUR-DATA&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running locally.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;./data/cat_expression_data&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">path</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">base_dir</span> <span class="o">=</span> <span class="n">set_path</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>Running on Google Colab.
Mounted at /content/drive
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># base directories</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">list_images</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34; list folders and count image files in each folder &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s1">&#39;.jpg&#39;</span><span class="p">,</span> <span class="s1">&#39;jpeg&#39;</span><span class="p">,</span> <span class="s1">&#39;.bmp&#39;</span><span class="p">,</span> <span class="s1">&#39;.gif&#39;</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total image files in </span><span class="si">{</span><span class="n">dir_name</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">list_images</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">list_images</span><span class="p">(</span><span class="n">val_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">list_images</span><span class="p">(</span><span class="n">test_dir</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Total image files in train: 1149
Total image files in val: 110
Total image files in test: 55
</code></pre>
<p>Finally, we will make a dictionary that maps the classes to index-based labels, from the CSV file. We will need this way later, but we will define the dictionary this early on.</p>
<h3 id="visualizing-some-of-the-data">Visualizing Some of the Data</h3>
<p>Let&rsquo;s visualize a few images from each folder to ensure to have a better feel of the data.</p>
<p>We will do this by making a dataframe out of the annotations file where the labels are stored. We will use the test annotations, since this is the smallest dataset, and the other two have the same labels anyway.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define the annotations file</span>
</span></span><span class="line"><span class="cl"><span class="n">annotations_filename</span> <span class="o">=</span> <span class="s1">&#39;_classes.csv&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define the full path to the annotations file</span>
</span></span><span class="line"><span class="cl"><span class="n">test_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># load the annotations file</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_annotations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">class_names</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">col</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">:])}</span>  <span class="c1"># Adjust slicing if there are other columns</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>{0: ' alarmed', 1: ' angry', 2: ' calm', 3: ' pleased'}
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">show_sample_images</span><span class="p">(</span><span class="n">main_directory</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Display sample images from each subfolder within the main directory.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">subfolders</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">path</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">scandir</span><span class="p">(</span><span class="n">main_directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">directory</span> <span class="ow">in</span> <span class="n">subfolders</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s1">&#39;.jpg&#39;</span><span class="p">,</span> <span class="s1">&#39;.jpeg&#39;</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">chosen_samples</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">image_files</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">),</span> <span class="n">num_samples</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Plot settings</span>
</span></span><span class="line"><span class="cl">            <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">image_files</span><span class="p">),</span> <span class="n">num_samples</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Images from </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">chosen_samples</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># ax.set_title(os.path.basename(img_path))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;No images to display in </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="si">}</span><span class="s2">.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Example usage with the base directory containing train, validation, and test subfolders</span>
</span></span><span class="line"><span class="cl"><span class="n">show_sample_images</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_10_0.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_10_1.png" type="" alt="png"  /></p>
<p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_10_2.png" type="" alt="png"  /></p>
<h2 id="data-preprocessing">Data Preprocessing</h2>
<hr>
<p>This steps involves preparing the dataset for training a PyTorch model by resizing, normalizing, and applying data augmentation.</p>
<p><strong>NOTE:</strong> At this point, it is important to know what is the model or CNN architecture we will be using. Important aspects to consider include the image size, any data transformations for training and validation, data augmentation techniques, and setting up data loaders later.</p>
<p>In this example, we will use <code>ResNet18 </code>. The inputs must follow a specific format, as per the PyTorch ResNet documentation found <a href="https://pytorch.org/hub/pytorch_vision_resnet/">here</a>:</p>
<blockquote>
<p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape <code>(3 x H x W)</code>, where <code>H</code> and <code>W</code> are expected to be at least 224. The images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>.</p></blockquote>
<h3 id="tools-for-data-preprocessing-in-pytorch">Tools for Data Preprocessing in PyTorch</h3>
<ul>
<li><code>torchvision.transforms</code>: Provides common image transformations like resizing, normalization, and augmentation.</li>
<li><code>torch.utils.data.Dataset</code>: A base class for creating custom datasets.</li>
<li><code>torch.utils.data.DataLoader</code>: Loads and batches data for training.</li>
</ul>
<h3 id="the-data">The Data</h3>
<p>The data set we are using consists of three folders: <code>train</code>, <code>val</code>, <code>test</code>. Each of them contain a set of images of cats. The labels in this case, are in the form of a CSV file that maps the filename with a one-hot encoding to label the classification of the image, i.e. the cat&rsquo;s mood - alarmed, angry, calm, pleased.</p>
<p>Because this dataset structure is not exactly suitable for the <code>ImageFolder</code> module in PyTorch, whereby labelling is made easier and based on the folder structure, we need to create a custom dataset and loader. Let&rsquo;s get started!</p>
<h3 id="define-image-transformations">Define Image Transformations</h3>
<ul>
<li>Specify resizing dimensions, normalization parameters, and augmentation techniques (like random rotation, flips, etc.).</li>
<li>Create separate transformations for training and validation datasets.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># import transforms</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define the image size</span>
</span></span><span class="line"><span class="cl"><span class="n">image_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># adjusted for ResNet18</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define transformations for the training dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>  <span class="c1"># resize to ensure minimum size</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="c1"># center crop to 224x224</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span> <span class="c1"># data augmentation</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">15</span><span class="p">),</span> <span class="c1"># data augmentation</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>  <span class="c1"># important, because the read_image reads as uint8, needs to be float</span>
</span></span><span class="line"><span class="cl">                                                <span class="c1"># given that below we apply normalization</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                         <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                         <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span></code></pre></div><p>With these transformations, the data pipeline will align with common practices for pre-trained models like ResNet18.</p>
<h3 id="create-custom-datasets-and-data-loaders">Create Custom Datasets and Data Loaders</h3>
<p>Given the structure of our dataset, where labels are provided in a CSV file rather than through directory structure, we need to use a custom dataset class. This will allow us to link echc image with its respective label based on our CSV file&rsquo;s structure.</p>
<h4 id="creating-custom-dataset">Creating Custom Dataset</h4>
<p>We will extend the <code>torch.utils.data.Dataset</code> class to create our custom dataset. this class will override the necessary methods to handle our specific dataset setup:</p>
<ol>
<li>Initialization: Load the CSV file and set up the path to the images</li>
<li>Length: Return the total number of images</li>
<li>Get item: Load each image by index, apply the specified transformations, and parse the label from the CSV data</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CustomImageDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34; a custom dataset class that loads images and their labels from a CSV &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">annotations_file</span><span class="p">,</span> <span class="n">img_dir</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    args:
</span></span></span><span class="line"><span class="cl"><span class="s2">      annotations_file (string): path to the CSV file with annotations
</span></span></span><span class="line"><span class="cl"><span class="s2">      img_dir (str): directory with all the images
</span></span></span><span class="line"><span class="cl"><span class="s2">      transform (callable, optional): transform to be applied on a sample
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">annotations_file</span><span class="p">)</span> <span class="c1"># load annotations</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">img_dir</span> <span class="o">=</span> <span class="n">img_dir</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; returns the number of items in the dataset &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; fetches the image and label at the index idx from the dataset &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_dir</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># convert one-hot encoded labels to a categorical label</span>
</span></span><span class="line"><span class="cl">    <span class="n">one_hot_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_labels</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># next find the index of the element in the slice which contains the &#39;1&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># since all other numbers will be 0; this will correspond to the label</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 0, 1, 2, 3</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">one_hot_label</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>  <span class="c1"># apply transformations</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</span></span></code></pre></div><p>Now that we have defined the data classes, we can create objects for each of our datasets, as a <code>CustomImageDataset</code> class.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># annotations_filename = &#39;_classes.csv&#39;    # previously defined</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># paths to annotation files</span>
</span></span><span class="line"><span class="cl"><span class="n">train_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">val_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_annotations</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">annotations_filename</span><span class="p">)</span>    <span class="c1"># previously defined</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create dataset objects</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CustomImageDataset</span><span class="p">(</span><span class="n">train_annotations</span><span class="p">,</span> <span class="n">train_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">transform</span> <span class="o">=</span> <span class="n">train_transforms</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">val_dataset</span> <span class="o">=</span> <span class="n">CustomImageDataset</span><span class="p">(</span><span class="n">val_annotations</span><span class="p">,</span> <span class="n">val_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">transform</span> <span class="o">=</span> <span class="n">val_transforms</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">CustomImageDataset</span><span class="p">(</span><span class="n">test_annotations</span><span class="p">,</span> <span class="n">test_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">transform</span> <span class="o">=</span> <span class="n">val_transforms</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="creating-data-loaders">Creating Data Loaders</h4>
<p>Data loaders in PyTorch provide the necessary functionality to batch, shuffle, and feed the data to your model during training in an efficient manner. They also handle parallel processing using multiple worker threads, which can significantly speed up data loading.</p>
<p>In short, data loaders take the dataset objects and handle the process of creating batches, shuffling the data, and parallelizing the data loading process.</p>
<p>Below we will create a data loaders for our datasets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>   <span class="c1"># defines how many samples per batch to load</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span>    <span class="c1"># shuffles the dataset at every epoch</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span>   <span class="c1"># no need to shuffle validation data</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p>In the loader above, we have the following main parts:</p>
<ul>
<li>Batch size: typycally set based on the system&rsquo;s memory capacity and how large the model is. A larger batch size can speed up training but requires more memory.</li>
<li>Shuffle: shuffling helps ensure that each batch sees a varierty of data across epochs, which can improve model generalization.</li>
<li>Number of workers: this controls how many subproceses to use for data loading. More workers can lead to faster data preprocessing and reduced time to train each epoch but also increases memory usage.</li>
</ul>
<h3 id="integration-with-the-training-loop">Integration with the Training Loop</h3>
<p>With the data loaders set up, we are now ready to integrate them into the model&rsquo;s training and validation loops.</p>
<p>The code <strong>snippet</strong> below shows how this would be done. We will implement the actual integration when we get to the training section.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Forward pass, backward pass, and optimize</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Validation step at the end of each epoch</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Calculate validation accuracy, loss, etc.</span>
</span></span></code></pre></div><h2 id="model-training">Model Training</h2>
<hr>
<p>Now that the data is ready and properly formatted for input into a neural network, the next step involves setting up and training the <code>ResNet18</code> model. We will configure the model, define the loss function and optimizer, and implement the training and validation loops.</p>
<h3 id="next-steps">Next Steps</h3>
<ol>
<li>Model setup:</li>
</ol>
<ul>
<li>Load the pre-trained <code>ResNet18</code> model and modify it for our specific classification task (number of classes based on cat facial expressions)</li>
</ul>
<ol start="2">
<li>Loss function and optimizer:</li>
</ol>
<ul>
<li>Define a loss function suitable for classification, e.g. <code>CrossEntropyLoss</code></li>
<li>Set up an optimizer (like <code>Adam</code> or <code>SGD</code>) to adjust the model weights during training based on the computed gradients</li>
</ul>
<ol start="3">
<li>Training loop:</li>
</ol>
<ul>
<li>Implement the loop that processes the data through the model, computes the loss, updates the model parameters, and evaluates the model performance on the validation dataset periodically</li>
</ul>
<ol start="4">
<li>Monitoring and saving the model:</li>
</ol>
<ul>
<li>Track performance metrics such as loss and accuracy</li>
<li>Implement functionality to save the trained model for later use or further evaluation</li>
</ul>
<h3 id="model-setup">Model Setup</h3>
<p>In this section, we&rsquo;ll configure a ResNet18 model to suit our specific classification task. Since the model is originally designed for ImageNet with 1000 classes, we&rsquo;ll adapt it for our use case, which involves classifying images into four mood categories (alarmed, angry, calm, pleased).</p>
<h4 id="import-the-necessary-libraries">Import the Necessary Libraries</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># import torch  # this has already been imported before</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
</span></span></code></pre></div><h4 id="load-and-modify-the-pre-trained-resnet18">Load and Modify the Pre-trained ResNet18</h4>
<p>We will load a pre-trained ResNet18 model and modify its final layer to suit our classification needs. This is known as <strong>transfer learning</strong>, and it is a technique that uses a pre-trained model and leverages its learned parameters to focus on a similar, more specific task. This is a powerful technique, since it uses the existing knowledge (such as edges and features) so that the new classification task is more robust, and faster to tune to the specific task.</p>
<h4 id="understanding-transfer-learning">Understanding Transfer Learning</h4>
<p><strong>Transfer Learning</strong> is a powerful technique in machine learning where a model developed for a particular task is reused as the starting point for a model on a second task. It&rsquo;s especially popular in deep learning given the vast compute and time resources required to develop neural network models on large datasets and from scratch.</p>
<h4 id="why-use-transfer-learning">Why Use Transfer Learning?</h4>
<ol>
<li>Efficiency: transfer learning allows us to leverage pre-trained networks that have already learned a good amount of features on large datasets. This is beneficial as it can drastically reduce the time and computational cost to achieve high performance.</li>
<li>Performance: models trained on large-scalr datasets like ImageNet havve proven to generalize well to other datasets. Starting with these can provide a significand head-start in terms of performance.</li>
</ol>
<h5 id="applying-transfer-learning">Applying Transfer Learning</h5>
<ul>
<li>Model adaptation: for our specific task fo classifying cat moods, we take a pre-trained ResNet18 model and tailor it to our needs. The pre-trained model brings the advantage of learned features from ImageNet, a vast and diverse dataset.</li>
<li>Feature extraction: by <strong>freezing</strong> (i.e. keeping the weight values as they are) the pre-trained layers, we utilize them as a feature extractor. Only the final layers are trained to adapt those features to our specific classification task.</li>
</ul>
<h4 id="model-setup-with-a-custom-classifier">Model Setup with a Custom Classifier</h4>
<p>We have mentioned replacing the funal layer(s) as a transfer learning techniques. In this case, we replace the final fully connected (fc) layer of ResNet18 with a different layer which will suit our need to have 4 classes. Additionally, we will replace this fc layer with a more complex classifier portion, which involves adding additional layers such as ReLU for non-linearity, and dropout for regularization to prevent overfitting.</p>
<ul>
<li>ReLU Activation: introduces non-linearity into the model, allowing it to learn more complex patterns.</li>
<li>Dropout: Randomly zeros some of the elements of the input (to the layer, not input to the model) tensor with probability $p$ during training, which helps prevent overfitting.</li>
</ul>
<p>Let&rsquo;s implement this classifier in our transfer learning setup.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># assign the model weights</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create the model object with pre-trained weights</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># freeze all the layers in the network</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># replace the fc layer with a more complex classifier</span>
</span></span><span class="line"><span class="cl"><span class="n">num_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>    <span class="c1"># first linear layer</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>                       <span class="c1"># non-linearity</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>                 <span class="c1"># dropout for regularization</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span>                <span class="c1"># output layer, 4 classes for cat moods</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># move model to GPU if available</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00&lt;00:00, 117MB/s]
</code></pre>
<h3 id="loss-function-and-optimizer">Loss Function and Optimizer</h3>
<p>For our classification task, we need a loss function that effectively measures the discrepancy between the predicted labels and the actual labels. Since we&rsquo;ve configured out model outputs to be class indices (from our dataset&rsquo;s one-hot encoded labels), we&rsquo;ll use <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code>CrossEntropyLoss</code></a>, which is ideal for such clasification tasks.</p>
<p>We&rsquo;ll par this with the <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code>Adam</code></a> optimizer, which is known for its efficiency in handling sparse gradients and adaptive learning rate capabilities, making it well-suited for this task.</p>
<p>Let&rsquo;s set up our loss function and optimizer.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># loss function</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># optimizer</span>
</span></span><span class="line"><span class="cl"><span class="c1"># optimize only the final classifier layers</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="training-and-validation-loops">Training and Validation Loops</h3>
<p>Now, let&rsquo;s write the code to train and validate our model. This involves running the model over several epochs, making predictions, calculating loss, updating the model parameters, and evaluating the model&rsquo;s perfomance on the validation dataset.</p>
<ul>
<li>Training loop: here, the model learns by adjusting its weights based on the calculated loss from the training data</li>
<li>Validation loop: validation occurs post the training phase in each epoch and helps in evaluating the model&rsquo;s performance on unseen data, ensuring it generalizes well and doesn&rsquo;t overfit</li>
</ul>
<h3 id="savingloading-the-model">Saving/Loading the Model</h3>
<h4 id="save-the-trained-model">Save the Trained Model</h4>
<p>If we have performed training, we can save the model to use next time, so that we can avoid re-training everytime we run the notebook.</p>
<p>We will implement this part as an <code>if</code> statement, that would run the training loop and save the model if we choose to, otherwise, we would just load the model weights.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pickle</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># define a function to set the save model path</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">model_save_path</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># check if the notebook is running on google colab</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running on Google Colab.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;/content/drive/PATH-TO-YOUR-SAVE-FOLDER&#39;</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running locally.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;./saved_models&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">path</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">flag</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">num_epochs</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">flag</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># num_epochs = 25   # define the number of epochs for training</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>   <span class="c1"># set the model to training mode</span>
</span></span><span class="line"><span class="cl">      <span class="n">total_train_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># forward pass to get outputs</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># backpropagation and optimization</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">total_train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># calculate average training loss for the epoch</span>
</span></span><span class="line"><span class="cl">      <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">total_train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_train_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># print average training loss per epoch&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">, Training Loss: </span><span class="si">{</span><span class="n">avg_train_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">#------------------#</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># validation phase #</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#------------------#</span>
</span></span><span class="line"><span class="cl">      <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>   <span class="c1"># set the model to evaluation mode</span>
</span></span><span class="line"><span class="cl">      <span class="n">total_val_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">      <span class="n">total_val_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">total_val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">total_val_accuracy</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># calculate average validation loss for the epoch</span>
</span></span><span class="line"><span class="cl">      <span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">total_val_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># calculate validation accuracy</span>
</span></span><span class="line"><span class="cl">      <span class="n">val_accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">total_val_accuracy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># print validation accuracy</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; Validation loss: </span><span class="si">{</span><span class="n">avg_val_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Validation accuracy: </span><span class="si">{</span><span class="n">val_accuracy</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># save model and training data</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_model_path</span> <span class="o">=</span> <span class="n">model_save_path</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># model_filename = &#39;vit_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model saved to&#39;</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># save the training and validation losses</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">save_training_data</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">({</span><span class="s1">&#39;train_losses&#39;</span><span class="p">:</span> <span class="n">train_losses</span><span class="p">,</span> <span class="s1">&#39;val_losses&#39;</span><span class="p">:</span> <span class="n">val_losses</span><span class="p">},</span> <span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Training data saved to </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Specify the filename for saving training data</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_data_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_training_data</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">training_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># load the trained model</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_model_path</span> <span class="o">=</span> <span class="n">model_save_path</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># model_filename = &#39;vit_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span> <span class="o">=</span> <span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model loaded and set to evaluation mode.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># load training data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Load the training and validation losses</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">load_training_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">training_data_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_model_path</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_data</span> <span class="o">=</span> <span class="n">load_training_data</span><span class="p">(</span><span class="n">training_data_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_losses</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;train_losses&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_losses</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;val_losses&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training data loaded successfully.&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model_filename</span> <span class="o">=</span> <span class="s1">&#39;resnet18_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">model_data_filename</span> <span class="o">=</span> <span class="s1">&#39;resnet18_training_data.pkl&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="n">training_loop</span><span class="p">(</span><span class="n">should_train_resnet</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Running on Google Colab.
Model loaded and set to evaluation mode.
Training data loaded successfully.
</code></pre>
<hr>
<h2 id="model-evaluation">Model Evaluation</h2>
<p>Now that the model is trained, the next step is to evaluate its performance more thoroughly, and possibly improve it based on the insights gained.</p>
<p>Evaluating the model involves checking the accuracy and also looking at other metrics like precision, recall, and F1-score, especially if the dataset is imbalanced or if specific classes are more important than others.</p>
<h3 id="model-evaluation-on-validation-set">Model Evaluation on Validation Set</h3>
<p>After training a machine learning model, it&rsquo;s crucial to evaluate its performance comprehensively. Here, we will detail three key diagnostic tools&quot;</p>
<ol>
<li>Confusion matrix</li>
<li>Plotting training and validation losses</li>
<li>Visualization of the predictions</li>
</ol>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<p>A confusion matrix provides a detailed breakdown of the model&rsquo;s predictions, showing exactly how many samples from each class were correctly or incorrectly predicted as each other class. This is crucial for understanding the model&rsquo;s performance across different categories.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">true_labels</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">      <span class="n">true_labels</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># compute the confusion matrix</span>
</span></span><span class="line"><span class="cl">  <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">clf_report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Convert classification report dictionary to DataFrame</span>
</span></span><span class="line"><span class="cl">  <span class="n">clf_report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clf_report</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">cm</span><span class="p">,</span> <span class="n">clf_report_df</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">cm</span><span class="p">,</span> <span class="n">clf_report_df</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print classification report</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">clf_report_df</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_40_0.png" type="" alt="png"  /></p>
<pre><code>Classification Report:
              precision    recall  f1-score     support
0              0.333333  0.285714  0.307692   14.000000
1              0.862069  0.714286  0.781250   35.000000
2              0.487179  0.678571  0.567164   28.000000
3              0.866667  0.787879  0.825397   33.000000
accuracy       0.672727  0.672727  0.672727    0.672727
macro avg      0.637312  0.616613  0.620376  110.000000
weighted avg   0.700728  0.672727  0.679728  110.000000
</code></pre>
<h4 id="plotting-training-and-validation-losses">Plotting Training and Validation Losses</h4>
<p>Plotting the training and validation losses over epochs allows us to monitor the learning process, identifying issues such as overfitting or underfitting.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">plot_losses</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Plot the training and validation losses.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Parameters:
</span></span></span><span class="line"><span class="cl"><span class="s2">  - train_losses: list of training loss values per epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">  - val_losses: list of validation loss values per epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Training Loss&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Validation Loss&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and Validation Losses Over Epochs&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># take the tracked losses from thet training loop</span>
</span></span><span class="line"><span class="cl"><span class="n">plot_losses</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_42_0.png" type="" alt="png"  /></p>
<h4 id="visualization-of-the-predictions">Visualization of the Predictions</h4>
<p>Visualizing model predictions on actual data points provides immediate qualitative feedback about model behavior. It helps identify paterns in which the model performs well or poorly, revealing potential biases, underfitting, or overfitting.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visualize_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">images_so_far</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="n">rows</span> <span class="o">=</span> <span class="n">num_images</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">num_images</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">num_images</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span> <span class="o">=</span> <span class="n">rows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># define the mean and std deviation used for normalization</span>
</span></span><span class="line"><span class="cl">  <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">images_so_far</span> <span class="o">&lt;</span> <span class="n">num_images</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">images_so_far</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">images_so_far</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">num_images</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">axes</span> <span class="c1"># arrange in grid</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># convert tensors to integers</span>
</span></span><span class="line"><span class="cl">          <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">          <span class="n">actual_label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># reverse normalization transform</span>
</span></span><span class="line"><span class="cl">          <span class="n">img</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># change CxHxW to HxWxC</span>
</span></span><span class="line"><span class="cl">          <span class="n">img</span> <span class="o">=</span> <span class="n">std</span> <span class="o">*</span> <span class="n">img</span> <span class="o">+</span> <span class="n">mean</span>   <span class="c1"># reverse normalization</span>
</span></span><span class="line"><span class="cl">          <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># clip values to ensure they fall between 0 and 1</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># use converted integers to access class names</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">]</span><span class="si">}</span><span class="s1"> | actual: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">actual_label</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          <span class="n">images_so_far</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>   <span class="c1"># adjust layout</span>
</span></span><span class="line"><span class="cl">          <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">          <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># make new loader for random samples</span>
</span></span><span class="line"><span class="cl"><span class="n">vis_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">visualize_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vis_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_44_0.png" type="" alt="png"  /></p>
<h2 id="evaluation-of-model-performance">Evaluation of Model Performance</h2>
<p>Our Cat Expression Classifier, built on a modified ResNet18 architecture, demonstrates a promising ability to classify cat expressions into four categories: alarmed, angry, calm, and pleased. Here, we provide a detailed analysis of the model&rsquo;s performance based on our the training and validation efforts.</p>
<h3 id="overall-performance-metrics">Overall Performance Metrics</h3>
<p>The model achieves an overall accuracy of 68.18% on the validation set. This is a decent foundation but indicates room for further refinement, especially in distinguishing between expressions that share subtle features. Here is a breakdown of the key performance metrics:</p>
<ul>
<li>Precision: Measures the accuracy of positive predictions. for example, the <code>pleased</code> category shows high precision, indicating that the model reliably identifies this expression.</li>
<li>Recall: Reflects the model&rsquo;s ability to identify all relevant instances of a class. The <code>angry</code> category has a high recall, suggesting that the model effectively captures most of the <code>angry</code> expressions.</li>
<li>F1-Score: Balances precision and recall and is particularly useful in scenarios where class distribution is uneven.</li>
</ul>
<h3 id="confusion-matrix-insights">Confusion Matrix Insights</h3>
<p>the confusion matrix provides a granular view of the model&rsquo;s performance across the different classes. It highlights specific areas where the model performs well and others where it struggles, such as:</p>
<ul>
<li>Misclassifications between <code>alarmed</code> and <code>angry</code> suggest that the model may be conflating these expressions due to their similar features.</li>
<li>The high accuracy in identifying <code>pleased</code> expressions shows that distinct features of this mood are well captured by the model.</li>
</ul>
<h3 id="training-and-validation-losses">Training and Validation Losses</h3>
<p>The training and validation loss plots reveal the learning dynamics over the epochs:</p>
<ul>
<li>A steady decrease in training loss indicates that the model is effectively learning from the training data.</li>
<li>The pattern of validation loss provides insights into the model&rsquo;s generalization ability. Increases in validation loss suggest moments where the model might be overfitting to the training data.</li>
</ul>
<hr>
<h2 id="trying-out-visiontransformers">Trying out VisionTransformers</h2>
<h3 id="data-transformations-for-vision-transformer">Data Transformations for Vision Transformer</h3>
<p>When transitioning from a CNN like <code>ResNet18</code> to a Vision Transformer (ViT), it&rsquo;s essential to evaluate whether the existing preprocessing steps - particulary the data transformations - are suitable for the new model architecture. For ViT , wed must consider their unique handling of image data, which relies on dividing the image into fixed-size patches and understanding global dependencies through self-attention mechanisms.</p>
<p>For this exercise, we will maintain the same transformations we have previously defined.</p>
<p>The decision to retain the initial transformations is based on the principle of consistency and the minimal impact expected by changing model architectures regarding how images are scaled and augmented. The chosen transformations ensure that the images are adequately prepared for the neural network without introducing complexities or distortions that could hinder the learning of global patterns, which are vital for Vision Transformers due to their reliance on self-attention mechanisms.</p>
<p>Additionally, maintaining these transformations allows for a more straightforward comparison between the ResNet18 model and the Vision Transformer model, as any changes in model performance can more confidently be attributed to the architectural differences rather than changes in data preprocessing.</p>
<h3 id="load-pre-trained-vision-transformer-model">Load Pre-Trained Vision Transformer Model</h3>
<p>First, we need to load the ViT model that has been pre-trained on a large dataset. We&rsquo;ll then adapt the classifier <em>head</em> to our needs, which is classifying cat moods into four categories.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># load the ViT pre-trained model</span>
</span></span><span class="line"><span class="cl"><span class="n">weights_vit</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ViT_B_16_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_vit</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights_vit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># print the model structure to understand what needs to be replaced</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model_vit</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Downloading: &quot;https://download.pytorch.org/models/vit_b_16-c867db91.pth&quot; to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330M/330M [00:02&lt;00:00, 124MB/s]


VisionTransformer(
  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (head): Linear(in_features=768, out_features=1000, bias=True)
  )
)
</code></pre>
<h3 id="freezing-the-encoder-layers">Freezing the Encoder Layers</h3>
<p>Freezing the encoder layers prevents their weights from being updated during training, which means they retain the knowledge they have already gained from ImageNet. We only want to train the classifier that we will modify to our specific task.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># freeze all layers in the model by disabling gradient computation</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model_vit</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</span></span></code></pre></div><h3 id="modify-the-classifier">Modify the Classifier</h3>
<p>The standard ViT model includes a classifier at the end (usually named <code>heads</code> in <code>torchvision</code> models), which is a linear layer designed for the original classification task, e.g. 1000 classes for ImageNet. We will replace this with a new classifier suited for our task (4 cat moods).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># replace the classifier head</span>
</span></span><span class="line"><span class="cl"><span class="c1"># as we saw in the architecture above, the classifier is called `heads`</span>
</span></span><span class="line"><span class="cl"><span class="n">num_features</span> <span class="o">=</span> <span class="n">model_vit</span><span class="o">.</span><span class="n">heads</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">in_features</span>   <span class="c1"># ge tthe number of input features</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># replace with a new head for len(class_names) = 4</span>
</span></span><span class="line"><span class="cl"><span class="n">model_vit</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># move model to appropriate device</span>
</span></span><span class="line"><span class="cl"><span class="n">model_vit</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>VisionTransformer(
  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (0): Linear(in_features=768, out_features=4, bias=True)
  )
)
</code></pre>
<h3 id="define-loss-function-and-optimizer">Define Loss Function and Optimizer</h3>
<p>Now, define the loss function and an optimizer. Since we are only training the classifier layer, ensure the optimizer is set to only update the parameters of the classifier.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># optimizer will not change, but still show it here:</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># optimizer - only optimize the classifier parameters</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_vit</span><span class="o">.</span><span class="n">heads</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="training-loop">Training Loop</h3>
<p>Here, we revisit the training process, adapting our previously established procedures to the Vision Transformer (ViT) model. Much like our approach with Resnet18, we utilize a similar training loop structure to ensure consistency and comparability. the core steps of training - forward pass, loss computation, backward pass, and parameters update - are maintained, but they are now applied to a differently structured model that leverages self-attention mechanisms rather than convolutional layers. This section briefly outlines these steps, focusing on any adjustments specific to the ViT to optimize it for our cat mood classification task.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model_filename</span> <span class="o">=</span> <span class="s1">&#39;vit_cat_mood.pth&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">model_data_filename</span> <span class="o">=</span> <span class="s1">&#39;vit_training_data.pkl&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span> <span class="o">=</span> <span class="n">training_loop</span><span class="p">(</span><span class="n">should_train_vit</span><span class="p">,</span> <span class="n">model_vit</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">,</span> <span class="n">model_data_filename</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Running on Google Colab.
Model loaded and set to evaluation mode.
Training data loaded successfully.
</code></pre>
<h3 id="model-evaluation-1">Model Evaluation</h3>
<h4 id="confusion-matrix-1">Confusion Matrix</h4>
<p>The confusion matrix below shows the following results:</p>
<ul>
<li>Class <code>alarmed</code>: moderate confusion with other classes, indicating difficulty in distinguishing <code>alarmed</code> from other moods.</li>
<li>Class <code>angry</code>: high accuracy, showing that `angry is well-recognized, with few misclassifications</li>
<li>Class <code>calm</code>: some confusion, particularly with <code>pleased</code>, suggesting similar features or expressions between these moods that the model confuses</li>
<li>Class <code>pleased</code>: best performance, indicating clear distinguishing features that the model learnes effectively</li>
</ul>
<h3 id="considerations-on-data-quality">Considerations on Data Quality</h3>
<p>Throughout the development and evaluation of our models, it has become evident that the quality of the dataset significantly impacts the classification accuracy. Certain misclassifications observed, such as the confusion between &lsquo;pleased&rsquo; and &lsquo;calm&rsquo; or &lsquo;alarmed&rsquo; and &lsquo;angry,&rsquo; suggest that the labels may not always align perfectly with the visual cues present in the images. This discrepancy can stem from subjective interpretations of cat expressions during labeling. Improving the dataset by refining the labeling process, possibly with the assistance of animal behavior experts, or by curating a more consistently labeled dataset could enhance model performance. Enhancing data quality would help in training more accurate and reliable models, thereby increasing the robustness of the classification outcomes.</p>
<h4 id="classification-report">Classification Report</h4>
<p>From the classification report, we can draw the following conclusions:</p>
<ul>
<li>Class <code>pleased</code> shows the highest precision, indicating a high rate of true positive predictions</li>
<li>Class <code>angry</code> has the highes recall, suggesting effective identification of this mood</li>
<li>Classes <code>angry</code> and <code>pleased</code> show high F1-scores, indicating robust performance.</li>
</ul>
<h4 id="overall-accuracy">Overall Accuracy</h4>
<p>The model achieves and accuracy of 74.55%, which is a solid performance but suggests room for improvement, particularly in reducing misclassifications among less distinct moods.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">cm</span><span class="p">,</span> <span class="n">clf_report_df</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model_vit</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot the confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Label&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print classification report</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">clf_report_df</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_62_0.png" type="" alt="png"  /></p>
<pre><code>Classification Report:
              precision    recall  f1-score     support
0              0.461538  0.428571  0.444444   14.000000
1              0.815789  0.885714  0.849315   35.000000
2              0.625000  0.535714  0.576923   28.000000
3              0.857143  0.909091  0.882353   33.000000
accuracy       0.745455  0.745455  0.745455    0.745455
macro avg      0.689868  0.689773  0.688259  110.000000
weighted avg   0.734544  0.745455  0.738361  110.000000
</code></pre>
<h3 id="training-and-validation-losses-1">Training and Validation Losses</h3>
<p>The plot of training loss shows a consistent decrease, indicating that the model is effectively learning from the data. The vlaidation loss decreases alongside the training loss but begins to plateau, suggesting that the model might be nearing its learning capacity with the current configuration and dataset.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># take the tracked losses from thet training loop</span>
</span></span><span class="line"><span class="cl"><span class="n">plot_losses</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_64_0.png" type="" alt="png"  /></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">visualize_predictions</span><span class="p">(</span><span class="n">model_vit</span><span class="p">,</span> <span class="n">vis_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">num_images</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_65_0.png" type="" alt="png"  /></p>
<h2 id="using-the-model-for-inference-on-new-data">Using the Model for Inference on New Data</h2>
<p>Let&rsquo;s try out the model on new, unseen data. This photo did not come from a dataset, but rather from a friend of mine who wants to know her cat&rsquo;s mood.</p>
<p>For inference, we first need to apply some simple transformation (not augmentation). In this case, we can use our previously defined <code>val_transforms</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">val_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                         <span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span></code></pre></div><p>Then, we can define a simple function to open, transform and add a batch dimension to the file we want to pass.</p>
<p>Then, the function will pass the image to the model to make inference. This latter step is done in a way so that the gradients are not computed, and the image data is passed as a forward pass only.</p>
<p>Finally, we will include a de-normalization to the transformed image, so that we can display it along with the predicted label.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Function to classify a single image and display it with the predicted label</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">classify_and_display_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">val_transforms</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">image_tensor</span> <span class="o">=</span> <span class="n">transformed_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># Disable gradient calculation</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">[</span><span class="n">predicted</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Convert the transformed image tensor back to a PIL image for display</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transformed_image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Change from (C, H, W) to (H, W, C)</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transformed_image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Denormalize the image</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">std</span> <span class="o">*</span> <span class="n">transformed_image</span> <span class="o">+</span> <span class="n">mean</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Display the image with the predicted label</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Predicted Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Example usage</span>
</span></span><span class="line"><span class="cl"><span class="n">classify_and_display_image</span><span class="p">(</span><span class="s1">&#39;gato.jpg&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="/images/20240515_cat_mood_classification/output_67_0.png" type="" alt="png"  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>This tutorial guides you through creating a cat expression classifier using convolutional neural networks with ResNet18 and later with a Vision Transformer (ViT). It demonstrates how to apply transfer learning to improve efficiency and accuracy with limited data.</p>
<p>The guide is structured to provide clear steps and practical examples for each phase of the project, from data preprocessing and model training to evaluation. By breaking down complex concepts and processes into manageable parts, it ensures that readers can easily follow along and apply these techniques to their projects.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
